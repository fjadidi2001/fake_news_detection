{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVEgSXTeWr0P9jcInVMl3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/DansE_Mar31.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "oA7Z8kvmccRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive in Colab"
      ],
      "metadata": {
        "id": "PxIpn64y8_JC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Mzv5ciD71hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501a6d66-e806-4ce2-efd8-2f40a8b98510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "9J1bk_HnDc7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Replace with your actual file path\n",
        "file_path = '/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path, encoding='latin-1')"
      ],
      "metadata": {
        "id": "bhyykHvM89kI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Data Inspection"
      ],
      "metadata": {
        "id": "TDzZ-uobD38a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(2))  # See first 2 rows\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSYdMJU3DiFH",
        "outputId": "6c85eb38-5362-49c6-bd02-01c5301fae52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     account_id       post_id    Category               Page  \\\n",
            "0  1.840000e+14  1.040000e+15  mainstream  ABC News Politics   \n",
            "1  1.840000e+14  1.040000e+15  mainstream  ABC News Politics   \n",
            "\n",
            "                                            Post URL Date Published Post Type  \\\n",
            "0  https://www.facebook.com/ABCNewsPolitics/posts...      9/19/2016     video   \n",
            "1  https://www.facebook.com/ABCNewsPolitics/posts...      9/19/2016      link   \n",
            "\n",
            "               Rating Debate  share_count  reaction_count  comment_count  \\\n",
            "0  no factual content    NaN          NaN           146.0           15.0   \n",
            "1         mostly true    NaN          1.0            33.0           34.0   \n",
            "\n",
            "                                        Context Post  \n",
            "0  WATCH: &quot;JEB EXCLAMATION POINT!&quot; - Je...  \n",
            "1  Can either candidate move the needle in the de...  \n",
            "\n",
            "Missing values:\n",
            " account_id           0\n",
            "post_id              0\n",
            "Category             0\n",
            "Page                 0\n",
            "Post URL             0\n",
            "Date Published       0\n",
            "Post Type            0\n",
            "Rating               0\n",
            "Debate            1984\n",
            "share_count         70\n",
            "reaction_count       2\n",
            "comment_count        2\n",
            "Context Post         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Values"
      ],
      "metadata": {
        "id": "_hVZN6j7FnqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 1: Fill categorical columns\n",
        "df['Rating'] = df['Rating'].fillna('Unknown')\n",
        "df['Debate'] = df['Debate'].fillna('Not Specified')\n",
        "\n",
        "# Strategy 2: Fill numerical columns with median\n",
        "numeric_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Alternative: Drop rows with critical missing values\n",
        "# df = df.dropna(subset=['important_column'])"
      ],
      "metadata": {
        "id": "CCesok3vEbWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Preprocessing"
      ],
      "metadata": {
        "id": "GFWw2cFnGKbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date to datetime format\n",
        "df['Date Published'] = pd.to_datetime(df['Date Published'], format='%m/%d/%Y')\n",
        "\n",
        "# Clean text columns\n",
        "df['Context Post'] = df['Context Post'].str.replace('\"', '')"
      ],
      "metadata": {
        "id": "-jLm9vpHGC-h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['account_id'] = df['account_id'].astype(str)\n",
        "df['post_id'] = df['post_id'].astype(str)"
      ],
      "metadata": {
        "id": "o2_ZzlhgGlq1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['Category', 'Page', 'Post Type']\n",
        "df[categorical_cols] = df[categorical_cols].fillna('Unknown')"
      ],
      "metadata": {
        "id": "DuV4oIehGoMR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgVsdZkBG2KU",
        "outputId": "5472e94c-4691-4b22-cb71-18f44ad414bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2282 entries, 0 to 2281\n",
            "Data columns (total 13 columns):\n",
            " #   Column          Non-Null Count  Dtype         \n",
            "---  ------          --------------  -----         \n",
            " 0   account_id      2282 non-null   object        \n",
            " 1   post_id         2282 non-null   object        \n",
            " 2   Category        2282 non-null   object        \n",
            " 3   Page            2282 non-null   object        \n",
            " 4   Post URL        2282 non-null   object        \n",
            " 5   Date Published  2282 non-null   datetime64[ns]\n",
            " 6   Post Type       2282 non-null   object        \n",
            " 7   Rating          2282 non-null   object        \n",
            " 8   Debate          2282 non-null   object        \n",
            " 9   share_count     2282 non-null   float64       \n",
            " 10  reaction_count  2282 non-null   float64       \n",
            " 11  comment_count   2282 non-null   float64       \n",
            " 12  Context Post    2282 non-null   object        \n",
            "dtypes: datetime64[ns](1), float64(3), object(9)\n",
            "memory usage: 231.9+ KB\n",
            "None\n",
            "account_id        0\n",
            "post_id           0\n",
            "Category          0\n",
            "Page              0\n",
            "Post URL          0\n",
            "Date Published    0\n",
            "Post Type         0\n",
            "Rating            0\n",
            "Debate            0\n",
            "share_count       0\n",
            "reaction_count    0\n",
            "comment_count     0\n",
            "Context Post      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main source"
      ],
      "metadata": {
        "id": "LVGdPq9osmqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wordembeddings"
      ],
      "metadata": {
        "id": "kzfAQRwVzfGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean up the environment\n",
        "!pip uninstall -y numpy mittens gensim scipy smart-open wrapt tensorflow tensorflow-datasets dm-tree numba\n",
        "\n",
        "# Step 2: Install compatible versions\n",
        "!pip install numpy==1.26.4 mittens==0.2 gensim==4.3.3 scipy==1.13.1 smart-open==7.1.0 wrapt==1.17.2\n",
        "\n",
        "# Step 3: Restart runtime (run this once, then comment out)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# Step 4: After restart, run the code\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-iINw1nwrYN",
        "outputId": "b490b631-b029-49f5-d1fd-a74236705ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping mittens as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: scipy 1.14.1\n",
            "Uninstalling scipy-1.14.1:\n",
            "  Successfully uninstalled scipy-1.14.1\n",
            "Found existing installation: smart-open 7.1.0\n",
            "Uninstalling smart-open-7.1.0:\n",
            "  Successfully uninstalled smart-open-7.1.0\n",
            "Found existing installation: wrapt 1.17.2\n",
            "Uninstalling wrapt-1.17.2:\n",
            "  Successfully uninstalled wrapt-1.17.2\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: tensorflow-datasets 4.9.8\n",
            "Uninstalling tensorflow-datasets-4.9.8:\n",
            "  Successfully uninstalled tensorflow-datasets-4.9.8\n",
            "Found existing installation: dm-tree 0.1.9\n",
            "Uninstalling dm-tree-0.1.9:\n",
            "  Successfully uninstalled dm-tree-0.1.9\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mittens==0.2\n",
            "  Downloading mittens-0.2-py3-none-any.whl.metadata (377 bytes)\n",
            "Collecting gensim==4.3.3\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scipy==1.13.1\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open==7.1.0\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt==1.17.2\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, mittens, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "tensorflow-probability 0.25.0 requires dm-tree, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "shap 0.47.0 requires numba>=0.54, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 mittens-0.2 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import GloVe\n",
        "import mittens\n",
        "print(\"Mittens version (GloVe only):\", mittens.__version__)\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])"
      ],
      "metadata": {
        "id": "9fuNa6-WuR4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenization"
      ],
      "metadata": {
        "id": "Vomn4xUHzZEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages and download NLTK data\n",
        "!pip install numpy==1.26.4 gensim==4.3.3 mittens==0.2 spacy==3.7.2 stop-words==2018.7.23 -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from mittens import GloVe\n",
        "import os\n",
        "\n",
        "# Special characters dictionary\n",
        "specialchar_dic = {\n",
        "    \"’\": \"'\", \"„\": \"\\\"\", \"“\": \"\\\"\", \"”\": \"\\\"\", \"«\": \"<<\", \"»\": \">>\",\n",
        "    \"…\": \"...\", \"—\": \"--\", \"¡\": \"!\", \"¿\": \"?\", \"©\": \" \", \"–\": \" \"\n",
        "}\n",
        "\n",
        "# Stop words function (cached globally)\n",
        "def stopWordsEN():\n",
        "    sw_stop_words = get_stop_words('en')\n",
        "    sw_nltk = stopwords.words('english')\n",
        "    sw_spacy = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
        "    sw_mallet = ['a', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', 'different', 'do', 'does', 'doing', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', 'way', 'we', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'x', 'y', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n",
        "    return list(set(sw_stop_words + sw_nltk + sw_mallet + sw_spacy))\n",
        "\n",
        "# Precompile regex and load Spacy model\n",
        "punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "specialchar_re = re.compile('(%s)' % '|'.join(specialchar_dic.keys()))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "cachedStopWords_en = stopWordsEN()\n",
        "\n",
        "class Tokenization:\n",
        "    def applyFE(self, text):\n",
        "        \"\"\"Combine negation with words to reduce bias.\"\"\"\n",
        "        final_text = text.replace('cannot', 'can not').replace('can\\'t', 'can not')\n",
        "        final_text = final_text.replace('won\\'t', 'will not').replace('n\\'t', ' not').replace(' not ', ' not')\n",
        "        return final_text\n",
        "\n",
        "    def removeStopWords(self, text):\n",
        "        return ' '.join([word for word in text.split() if word not in cachedStopWords_en])\n",
        "\n",
        "    def removePunctuation(self, text, punctuation=punctuation):\n",
        "        for c in punctuation:\n",
        "            text = text.replace(c, ' ')\n",
        "        return text\n",
        "\n",
        "    def replaceUTF8Char(self, text, specialchars=specialchar_dic):\n",
        "        def replace(match):\n",
        "            return specialchars[match.group(0)]\n",
        "        return specialchar_re.sub(replace, text)\n",
        "\n",
        "    def createCorpus(self, text, remove_punctuation=True, remove_stopwords=True, apply_FE=True):\n",
        "        corpus = []\n",
        "        try:\n",
        "            text = self.replaceUTF8Char(text).replace(\"\\n\", \" \")\n",
        "            doc = nlp(text)\n",
        "            processed_text = ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text if not t.ent_type_ else t.text for t in doc])\n",
        "            processed_text = processed_text.replace(\"\\s\\s+\", ' ')\n",
        "\n",
        "            doc = nlp(processed_text.lower())\n",
        "            rawText = not (remove_punctuation or remove_stopwords or apply_FE)\n",
        "\n",
        "            for sentence in doc.sents:\n",
        "                sent = str(sentence.text)\n",
        "                if len(sent) == 0:\n",
        "                    continue\n",
        "                if not rawText:\n",
        "                    if apply_FE:\n",
        "                        sent = self.applyFE(text=sent)\n",
        "                    if remove_punctuation:\n",
        "                        sent = self.removePunctuation(text=sent)\n",
        "                    if remove_stopwords:\n",
        "                        sent = self.removeStopWords(text=sent)\n",
        "                sent = sent.lower().split()\n",
        "                if sent:\n",
        "                    corpus.append(sent)\n",
        "        except Exception as exp:\n",
        "            print('exception=', str(exp))\n",
        "            print('text=', text)\n",
        "        return corpus\n",
        "\n",
        "    def __del__(self):\n",
        "        print(\"Destructor Tokenization\")\n",
        "\n",
        "class WordEmbeddings:\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Tokenization example\n",
        "    tkn = Tokenization()\n",
        "    text1 = \"Apple data-intensive is looking at buying U.K. startup for $1 billion. This is great! The new D.P. model is funcitonal and ready\"\n",
        "    corpus1 = tkn.createCorpus(text1)\n",
        "    print(\"Corpus 1:\", corpus1)\n",
        "\n",
        "    # Larger text example\n",
        "    text2 = \"\"\"The lion may be known as the king of the jungle, but lions do not live in jungles. They’re the rulers of the African savannahs that are covered in brown grasses and speckled with sparse trees. Lions’ coloring helps them blend in perfectly with the tall grass so they can ambush their prey as best as possible. And lions are ferocious. Although they’re one of the most powerful predators on land, lions are in danger. Hunters and poachers target lions to prove to the world their machismo.\\n\\nAnd while hunters seek to wipe lions off the face of the earth to bolster their egos, the Kevin Richardson Wildlife Sanctuary hopes to stop them and protect the big African cat at all cost.\\n\\nRichardson has earned the nickname the “Lion Whisperer” for a reason. He aims to educate the world about lions. And for those lucky enough to volunteer alongside Richardson, he encourages them to learn more about lions and help protect the wild species.\\n\\n“To raise awareness, Kevin has now set up his YouTube Channel ‘LionWhispererTV’. The channel is all about raising awareness about not only the declining numbers of lions but also how this rapid decrease is happening. By watching these videos, you are directly contributing to our scheme of land acquisition,” he writes in his bio.\\n\\nAs part of the volunteer program, Richardson hosts a “volunteer enrichment and lion enrichment” walk. As the name suggests, Richardson takes his group of volunteers out into the savannah of South Africa to hang out with two lions. There, the volunteers meet a male lion, Bobcat, and a female lioness, Gabby. Both lions look ferocious, but are truly “affectionate,” at least that’s what Richardson says. And remember, he’s the lion whisperer, so he’s got an advantage with these deadly big cats.\\n\\nAs Richardson showers the pair of lions with love, the volunteers stay locked in the truck, unwilling to put their lives in danger. And while they are in the vehicle, the lions are just feet from them – and if something goes wrong, they could wind up injured anyway.\\n\\nRichardson shared the video on his “The Lion Whisperer” YouTube channel. With more than one million hits, this video has proven to be one of his most famous.\\n\\nThe video describes the moment caught on tape as follows:\\n\\n“It’s an enrichment walk for both the volunteers and the lions as Kevin shows off his lovely lions as well as giving some amazing lion facts to the volunteers.”\\n\\nViewers like you are overwhelmed with the magnificent footage. The following are a few comments shared on the video.\\n\\n“I hope to someday volunteer there with Kevin. I believe in the work and his perspective about conservation. This video makes me want to all the more! Bobcat and Gabby are lovely lions.” “Every time I watch a one of your videos I somehow end up smiling from ear to ear!” “That was so beautiful, wish I could rub my head against a lion.”\\n\\nTake a moment to watch this video. Would you ever want to volunteer with Kevin Richardson and his lions?\"\"\"\n",
        "    corpus2 = tkn.createCorpus(text2, remove_stopwords=False)\n",
        "    print(\"Corpus 2:\", corpus2)\n",
        "\n",
        "    # Generate embeddings from Corpus 2\n",
        "    we = WordEmbeddings(corpus2)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(\"\\nDocuments shape:\", np.array(docs, dtype=object).shape)\n",
        "    print(\"Documents:\", docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec shape:\", w2v.shape)\n",
        "    print(\"Word2Vec embeddings:\", w2v[:5])  # Print first 5 for brevity\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText shape:\", w2f.shape)\n",
        "    print(\"FastText embeddings:\", w2f[:5])\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe shape:\", w2g.shape)\n",
        "    print(\"GloVe embeddings:\", w2g[:5])\n",
        "\n",
        "    print(\"\\nComparison for word 'lion' (ID varies):\")\n",
        "    lion_id = we.word2id.get('lion', -1)\n",
        "    if lion_id != -1:\n",
        "        print(\"Word2Vec:\", w2v[lion_id])\n",
        "        print(\"FastText:\", w2f[lion_id])\n",
        "        print(\"GloVe:\", w2g[lion_id])\n",
        "    else:\n",
        "        print(\"'lion' not found in vocabulary\")"
      ],
      "metadata": {
        "id": "HAIG9UunzNiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "S6BE_eCL3gmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install numpy==1.26.4 gensim==4.3.3 spacy==3.7.2 stop-words==2018.7.23 pandas scipy -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import io as sio\n",
        "import re\n",
        "import spacy\n",
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from mittens import GloVe\n",
        "from multiprocessing import cpu_count\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define stop words (simplified for brevity; use your full list)\n",
        "def stopWordsEN():\n",
        "    sw_stop_words = get_stop_words('en')\n",
        "    sw_nltk = stopwords.words('english')\n",
        "    sw_spacy = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
        "    return list(set(sw_stop_words + sw_nltk + sw_spacy))\n",
        "\n",
        "# Tokenization setup\n",
        "specialchar_dic = {\n",
        "    \"’\": \"'\", \"„\": \"\\\"\", \"“\": \"\\\"\", \"”\": \"\\\"\", \"«\": \"<<\", \"»\": \">>\",\n",
        "    \"…\": \"...\", \"—\": \"--\", \"¡\": \"!\", \"¿\": \"?\", \"©\": \" \", \"–\": \" \"\n",
        "}\n",
        "punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "specialchar_re = re.compile('(%s)' % '|'.join(specialchar_dic.keys()))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "cachedStopWords_en = stopWordsEN()\n",
        "\n",
        "class Tokenization:\n",
        "    def applyFE(self, text):\n",
        "        final_text = text.replace('cannot', 'can not').replace('can\\'t', 'can not')\n",
        "        final_text = final_text.replace('won\\'t', 'will not').replace('n\\'t', ' not').replace(' not ', ' not')\n",
        "        return final_text\n",
        "\n",
        "    def removeStopWords(self, text):\n",
        "        return ' '.join([word for word in text.split() if word not in cachedStopWords_en])\n",
        "\n",
        "    def removePunctuation(self, text, punctuation=punctuation):\n",
        "        for c in punctuation:\n",
        "            text = text.replace(c, ' ')\n",
        "        return text\n",
        "\n",
        "    def replaceUTF8Char(self, text, specialchars=specialchar_dic):\n",
        "        def replace(match):\n",
        "            return specialchars[match.group(0)]\n",
        "        return specialchar_re.sub(replace, text)\n",
        "\n",
        "    def createCorpus(self, text, remove_punctuation=True, remove_stopwords=True, apply_FE=True):\n",
        "        if pd.isna(text):\n",
        "            text = \"\"\n",
        "        corpus = []\n",
        "        try:\n",
        "            text = self.replaceUTF8Char(text).replace(\"\\n\", \" \")\n",
        "            doc = nlp(text)\n",
        "            processed_text = ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text if not t.ent_type_ else t.text for t in doc])\n",
        "            processed_text = processed_text.replace(\"\\s\\s+\", ' ')\n",
        "            doc = nlp(processed_text.lower())\n",
        "            rawText = not (remove_punctuation or remove_stopwords or apply_FE)\n",
        "            for sentence in doc.sents:\n",
        "                sent = str(sentence.text)\n",
        "                if len(sent) == 0:\n",
        "                    continue\n",
        "                if not rawText:\n",
        "                    if apply_FE:\n",
        "                        sent = self.applyFE(text=sent)\n",
        "                    if remove_punctuation:\n",
        "                        sent = self.removePunctuation(text=sent)\n",
        "                    if remove_stopwords:\n",
        "                        sent = self.removeStopWords(text=sent)\n",
        "                sent = sent.lower().split()\n",
        "                if sent:\n",
        "                    corpus.append(sent)\n",
        "        except Exception as exp:\n",
        "            print('exception=', str(exp))\n",
        "            print('text=', text)\n",
        "        return corpus\n",
        "\n",
        "class WordEmbeddings:\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus = corpus\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=cpu_count(), sg=0):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components)\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=cpu_count(), sg=0):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "def processElement(elem):\n",
        "    idx, text = elem  # Unpack as (index, text)\n",
        "    tkn = Tokenization()\n",
        "    text = tkn.createCorpus(text, remove_stopwords=False)\n",
        "    return idx, text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load dataset\n",
        "    file_path = '/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv'\n",
        "    df = pd.read_csv(file_path, encoding='latin-1')\n",
        "    print(\"Dataset Head:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Label mapping\n",
        "    label2id = {\n",
        "        'mostly true': 0,\n",
        "        'mixture of true and false': 1,\n",
        "        'no factual content': 1,\n",
        "        'mostly false': 1\n",
        "    }\n",
        "    df['Rating'] = df['Rating'].map(label2id)\n",
        "    y = df['Rating'].astype(int).to_numpy()\n",
        "    sio.savemat('labels.mat', {'y': y})\n",
        "\n",
        "    # Network features\n",
        "    network_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "    X_network = df[network_cols].fillna(0).to_numpy()\n",
        "    scaler_std = StandardScaler()\n",
        "    X_net_std = scaler_std.fit_transform(X_network)\n",
        "    X_net_std = X_net_std.reshape((X_net_std.shape[0], 1, X_net_std.shape[1]))\n",
        "    print(\"\\nX_network shape:\", X_network.shape)\n",
        "    print(\"X_net_std shape:\", X_net_std.shape)\n",
        "    sio.savemat('network.mat', {'X_net_std': X_net_std})\n",
        "\n",
        "    # Tokenization\n",
        "    print(\"\\nStart Tokenization\")\n",
        "    # Use row indices (0 to 2281) paired with Context Post\n",
        "    texts = list(enumerate(df['Context Post'].tolist()))\n",
        "    corpus = [None] * len(texts)\n",
        "    no_threads = cpu_count() - 1\n",
        "    with ProcessPoolExecutor(max_workers=no_threads) as worker:\n",
        "        for result in worker.map(processElement, texts):\n",
        "            if result:\n",
        "                corpus[result[0]] = result[1]\n",
        "\n",
        "    print(\"Corpus sample (first 5):\")\n",
        "    for idx, doc in enumerate(corpus[:5]):\n",
        "        print(idx, doc)\n",
        "\n",
        "    # Word Embeddings\n",
        "    print(\"\\nStart Document Tokenization\")\n",
        "    we = WordEmbeddings(corpus)\n",
        "    documents = we.prepareDocuments()\n",
        "    vocabulary_size = we.no_words\n",
        "    max_size = we.max_size\n",
        "    print(\"Vocabulary size:\", vocabulary_size)\n",
        "    print(\"Max Document size:\", max_size)\n",
        "\n",
        "    X_docs = []\n",
        "    for document in documents:\n",
        "        doc_size = len(document)\n",
        "        X_docs.append(document + [0] * (max_size - doc_size))\n",
        "    X_docs = np.array(X_docs)\n",
        "    sio.savemat('corpus.mat', {'X': X_docs})\n",
        "\n",
        "    print(\"Start W2V CBOW\")\n",
        "    w2v_cbow = we.word2vecEmbedding(sg=0)\n",
        "    sio.savemat('w2v_cbow.mat', {'w2v_cbow': w2v_cbow})\n",
        "\n",
        "    print(\"Start W2V SG\")\n",
        "    w2v_sg = we.word2vecEmbedding(sg=1)\n",
        "    sio.savemat('w2v_sg.mat', {'w2v_sg': w2v_sg})\n",
        "\n",
        "    print(\"Start FT CBOW\")\n",
        "    ft_cbow = we.word2FastTextEmbeddings(sg=0)\n",
        "    sio.savemat('ft_cbow.mat', {'ft_cbow': ft_cbow})\n",
        "\n",
        "    print(\"Start FT SG\")\n",
        "    ft_sg = we.word2FastTextEmbeddings(sg=1)\n",
        "    sio.savemat('ft_sg.mat', {'ft_sg': ft_sg})\n",
        "\n",
        "    print(\"Start GLOVE\")\n",
        "    glove = we.word2GloVeEmbedding()\n",
        "    sio.savemat('glove.mat', {'glove': glove})"
      ],
      "metadata": {
        "id": "s8obA6Gw5sU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset Size: 2282 samples.\n",
        "\n",
        "- Network Features: X_net_std has shape (2282, 1, 3) (from share_count, reaction_count, comment_count).\n",
        "\n",
        "- Text Data: X_docs will have shape (2282, 117) (max document size is 117).\n",
        "\n",
        "- Vocabulary Size: 4023 unique words (including padding token 0).\n",
        "\n",
        "- Embeddings: Successfully generated w2v_cbow, w2v_sg, ft_cbow, ft_sg, and glove, each with 128 dimensions (default no_components).\n",
        "\n"
      ],
      "metadata": {
        "id": "2BoiXHOL7ll_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Script"
      ],
      "metadata": {
        "id": "Au4jSj444VEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "wlJaMAFn8tkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import io as sio\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Input, Concatenate, Conv1D, Flatten, MaxPooling1D, Reshape\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 2\n",
        "batch_size = 256\n",
        "epochs_n = 5\n",
        "units = 128\n",
        "filters = int(units / 2)\n",
        "no_attributes_lstm = units\n",
        "kernel_size_lstm = int(no_attributes_lstm / 2)\n",
        "no_attributes_bilstm = int(units * 2)\n",
        "kernel_size_bilstm = int(no_attributes_bilstm / 2)\n",
        "\n",
        "execution = {}\n",
        "accuracies = {}\n",
        "precisions = {}\n",
        "recalls = {}\n",
        "\n",
        "def evaluate(y_test, y_pred, modelName='LSTM', wordemb='w2v_sg', iters=0):\n",
        "    y_pred_norm = []\n",
        "    for elem in y_pred:\n",
        "        line = [0] * len(elem)\n",
        "        try:\n",
        "            elem[np.isnan(elem)] = 0\n",
        "            line[elem.tolist().index(max(elem.tolist()))] = 1\n",
        "        except:\n",
        "            print(\"Error for getting predicted class\")\n",
        "            print(elem.tolist())\n",
        "            line[random.randint(0, len(elem)-1)] = 1\n",
        "        y_pred_norm.append(line)\n",
        "    y_p = np.argmax(np.array(y_pred_norm), 1)\n",
        "    y_t = np.argmax(np.array(y_test), 1)\n",
        "    accuracy = accuracy_score(y_t, y_p)\n",
        "    accuracies[wordemb][modelName].append(accuracy)\n",
        "    precision = precision_score(y_t, y_p, average='weighted')\n",
        "    precisions[wordemb][modelName].append(precision)\n",
        "    recall = recall_score(y_t, y_p, average='weighted')\n",
        "    recalls[wordemb][modelName].append(recall)\n",
        "    print(f\"{modelName} {wordemb} Accuracy {accuracy:.4f}\")\n",
        "    print(f\"{modelName} {wordemb} Precision {precision:.4f}\")\n",
        "    print(f\"{modelName} {wordemb} Recall {recall:.4f}\")\n",
        "    return y_p, y_t\n",
        "\n",
        "# Model definitions with corrected shapes\n",
        "def modelContentNetworkLSTM_00CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')  # Tuple (117,)\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"LSTM-00CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkLSTM_01CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_net)\n",
        "    model_net = Reshape((no_attributes_lstm, 1))(model_net)\n",
        "    model_net = Conv1D(filters=filters, kernel_size=kernel_size_lstm, activation='relu')(model_net)\n",
        "    model_net = MaxPooling1D()(model_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"LSTM-01CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkLSTM_10CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(model_docs)\n",
        "    model_docs = Conv1D(filters=int(filters/2), kernel_size=int(kernel_size_lstm/2), activation='relu')(model_docs)\n",
        "    model_docs = MaxPooling1D()(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"LSTM-10CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkLSTM_11CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(model_docs)\n",
        "    model_docs = Conv1D(filters=int(filters/2), kernel_size=int(kernel_size_lstm/2), activation='relu')(model_docs)\n",
        "    model_docs = MaxPooling1D()(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(input_net)\n",
        "    model_net = Reshape((no_attributes_lstm, 1))(model_net)\n",
        "    model_net = Conv1D(filters=filters, kernel_size=kernel_size_lstm, activation='relu')(model_net)\n",
        "    model_net = MaxPooling1D()(model_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"LSTM-11CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentLSTM(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    output = Dense(units=num_classes, activation='softmax')(model_docs)\n",
        "    model = Model(inputs=input_docs, outputs=output, name=\"LSTM-Content\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=X_train_docs, y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=(X_val_docs, y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict(X_test_docs, verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentLSTMCNN(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(model_docs)\n",
        "    model_docs = Conv1D(filters=int(filters/2), kernel_size=int(kernel_size_lstm/2), activation='relu')(model_docs)\n",
        "    model_docs = MaxPooling1D()(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    output = Dense(units=num_classes, activation='softmax')(model_docs)\n",
        "    model = Model(inputs=input_docs, outputs=output, name=\"LSTM-CNN-Content\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=X_train_docs, y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=(X_val_docs, y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict(X_test_docs, verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkBiLSTM_00CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(input_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"BiLSTM-00CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkBiLSTM_01CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(input_net)\n",
        "    model_net = Reshape((no_attributes_bilstm, 1))(model_net)\n",
        "    model_net = Conv1D(filters=filters, kernel_size=kernel_size_bilstm, activation='relu')(model_net)\n",
        "    model_net = MaxPooling1D()(model_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"BiLSTM-01CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkBiLSTM_10CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(model_docs)\n",
        "    model_docs = Conv1D(filters=int(filters/2), kernel_size=int(kernel_size_bilstm/2), activation='relu')(model_docs)\n",
        "    model_docs = MaxPooling1D()(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(input_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"BiLSTM-10CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentNetworkBiLSTM_11CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(model_docs)\n",
        "    model_docs = Conv1D(filters=int(filters/2), kernel_size=int(kernel_size_bilstm/2), activation='relu')(model_docs)\n",
        "    model_docs = MaxPooling1D()(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    input_net = Input(shape=(1, 3), name='NETS_INPUT')\n",
        "    model_net = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(input_net)\n",
        "    model_net = Reshape((no_attributes_bilstm, 1))(model_net)\n",
        "    model_net = Conv1D(filters=filters, kernel_size=kernel_size_bilstm, activation='relu')(model_net)\n",
        "    model_net = MaxPooling1D()(model_net)\n",
        "    model_net = Flatten()(model_net)\n",
        "    combined = Concatenate()([model_docs, model_net])\n",
        "    output = Dense(units=num_classes, activation='softmax')(combined)\n",
        "    model = Model(inputs=[input_docs, input_net], outputs=output, name=\"BiLSTM-11CNN-ContentNets\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=[X_train_docs, X_train_net], y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=([X_val_docs, X_val_net], y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict([X_test_docs, X_test_net], verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentBiLSTM(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    output = Dense(units=num_classes, activation='softmax')(model_docs)\n",
        "    model = Model(inputs=input_docs, outputs=output, name=\"BiLSTM-Content\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=X_train_docs, y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=(X_val_docs, y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict(X_test_docs, verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "def modelContentBiLSTMCNN(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx):\n",
        "    input_docs = Input(shape=(X_train_docs.shape[1],), name='DOCS_INPUT')\n",
        "    model_docs = Embedding(input_dim=4023, output_dim=units, weights=[w2v], trainable=False)(input_docs)\n",
        "    model_docs = Bidirectional(LSTM(units=units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(model_docs)\n",
        "    model_docs = Conv1D(filters=int(filters/2), kernel_size=int(kernel_size_bilstm/2), activation='relu')(model_docs)\n",
        "    model_docs = MaxPooling1D()(model_docs)\n",
        "    model_docs = Flatten()(model_docs)\n",
        "    output = Dense(units=num_classes, activation='softmax')(model_docs)\n",
        "    model = Model(inputs=input_docs, outputs=output, name=\"BiLSTM-CNN-Content\")\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "    start_time = time.time()\n",
        "    model.fit(x=X_train_docs, y=y_train, epochs=epochs_n, verbose=1,\n",
        "              validation_data=(X_val_docs, y_val), batch_size=batch_size, callbacks=[es])\n",
        "    end_time = time.time()\n",
        "    y_pred = model.predict(X_test_docs, verbose=0)\n",
        "    evaluate(y_test, y_pred, modelName=model.name, wordemb=wordemb, iters=idx)\n",
        "    exc_time = end_time - start_time\n",
        "    execution[wordemb][model.name].append(exc_time)\n",
        "    print(f\"Time taken to train: {exc_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    y = sio.loadmat('labels.mat')['y'][0]\n",
        "    X_net_std = sio.loadmat('network.mat')['X_net_std']\n",
        "    X_docs = sio.loadmat('corpus.mat')['X']\n",
        "    print(\"y shape:\", y.shape)\n",
        "    print(\"X_net_std shape:\", X_net_std.shape)\n",
        "    print(\"X_docs shape:\", X_docs.shape)\n",
        "\n",
        "    # Verify vocabulary size\n",
        "    vocabulary_size = 4023\n",
        "    max_size = 117\n",
        "    print(f\"Vocabulary size: {vocabulary_size}, Max size: {max_size}\")\n",
        "\n",
        "    embedding_types = ['w2v_cbow', 'w2v_sg', 'ft_cbow', 'ft_sg', 'glove']\n",
        "    models = [\n",
        "        \"LSTM-00CNN-ContentNets\", \"LSTM-01CNN-ContentNets\", \"LSTM-10CNN-ContentNets\", \"LSTM-11CNN-ContentNets\",\n",
        "        \"LSTM-Content\", \"LSTM-CNN-Content\",\n",
        "        \"BiLSTM-00CNN-ContentNets\", \"BiLSTM-01CNN-ContentNets\", \"BiLSTM-10CNN-ContentNets\", \"BiLSTM-11CNN-ContentNets\",\n",
        "        \"BiLSTM-Content\", \"BiLSTM-CNN-Content\"\n",
        "    ]\n",
        "\n",
        "    for wordemb in embedding_types:\n",
        "        accuracies[wordemb] = {model: [] for model in models}\n",
        "        precisions[wordemb] = {model: [] for model in models}\n",
        "        recalls[wordemb] = {model: [] for model in models}\n",
        "        execution[wordemb] = {model: [] for model in models}\n",
        "\n",
        "        w2v = sio.loadmat(f'{wordemb}.mat')[wordemb]\n",
        "        print(f\"Loaded {wordemb} shape: {w2v.shape}\")\n",
        "\n",
        "        for idx in range(5):\n",
        "            X_train_docs, X_test_docs, X_train_net, X_test_net, y_train, y_test = train_test_split(\n",
        "                X_docs, X_net_std, y, test_size=0.30, shuffle=True, stratify=y)\n",
        "            X_train_docs, X_val_docs, X_train_net, X_val_net, y_train, y_val = train_test_split(\n",
        "                X_train_docs, X_train_net, y_train, test_size=0.20, shuffle=True, stratify=y_train)\n",
        "            y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "            y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "            y_val = to_categorical(y_val, num_classes=num_classes)\n",
        "\n",
        "            print(f\"\\nIteration {idx+1} - Split shapes:\")\n",
        "            print(\"X_train_docs:\", X_train_docs.shape, \"X_val_docs:\", X_val_docs.shape, \"X_test_docs:\", X_test_docs.shape)\n",
        "            print(\"X_train_net:\", X_train_net.shape, \"X_val_net:\", X_val_net.shape, \"X_test_net:\", X_test_net.shape)\n",
        "            print(\"y_train:\", y_train.shape, \"y_val:\", y_val.shape, \"y_test:\", y_test.shape)\n",
        "\n",
        "            print(f\"\\nRunning models with {wordemb} embedding:\")\n",
        "            modelContentLSTM(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentLSTMCNN(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkLSTM_00CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkLSTM_01CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkLSTM_10CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkLSTM_11CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentBiLSTM(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentBiLSTMCNN(X_train_docs, X_val_docs, X_test_docs, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkBiLSTM_00CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkBiLSTM_01CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkBiLSTM_10CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "            modelContentNetworkBiLSTM_11CNN(X_train_docs, X_val_docs, X_test_docs, X_train_net, X_val_net, X_test_net, y_train, y_val, y_test, w2v, num_classes, wordemb, idx)\n",
        "\n",
        "        print(f\"\\nSummary for {wordemb}:\")\n",
        "        for model in models:\n",
        "            print(f\"{model} {wordemb} ACCURACY {np.mean(accuracies[wordemb][model]):.4f} ± {np.std(accuracies[wordemb][model]):.4f}\")\n",
        "            print(f\"{model} {wordemb} PRECISION {np.mean(precisions[wordemb][model]):.4f} ± {np.std(precisions[wordemb][model]):.4f}\")\n",
        "            print(f\"{model} {wordemb} RECALL {np.mean(recalls[wordemb][model]):.4f} ± {np.std(recalls[wordemb][model]):.4f}\")\n",
        "            print(f\"{model} {wordemb} EXECUTION TIME {np.mean(execution[wordemb][model]):.2f} ± {np.std(execution[wordemb][model]):.2f}\")"
      ],
      "metadata": {
        "id": "pQ9z0Kf_8qSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWF6lpFj88_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}