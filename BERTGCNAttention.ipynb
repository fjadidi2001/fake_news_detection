{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzW8iUPNrQXBA5I573gW6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/BERTGCNAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup and Data Loading\n",
        "\n"
      ],
      "metadata": {
        "id": "K-4U7Y5EqCOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvTEVxbhnXsW",
        "outputId": "1003d12a-1905-4815-b6a7-3d4d16443e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torch-geometric transformers imbalanced-learn -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "from scipy import io as sio\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "\n",
        "# Extract network features and handle NaN\n",
        "network_features = df[['share_count', 'reaction_count', 'comment_count']].fillna(0).values  # Replace NaN with 0\n",
        "print(\"NaN in network features before filling:\", np.isnan(network_features).any())  # Should be True if NaN exists\n",
        "print(\"NaN in network features after filling:\", np.isnan(network_features).any())  # Should be False\n",
        "\n",
        "# Standardize network features\n",
        "scaler = StandardScaler()\n",
        "X_net_std = scaler.fit_transform(network_features)  # (2282, 3)\n",
        "print(\"NaN in X_net_std:\", np.isnan(X_net_std).any())  # Should be False\n",
        "\n",
        "# Save standardized features\n",
        "sio.savemat('network.mat', {'X_net_std': X_net_std})\n",
        "print(\"Network features shape:\", X_net_std.shape)\n",
        "\n",
        "# Prepare labels (binary classification)\n",
        "labels = df['Rating'].apply(lambda x: 0 if x == 'mostly true' else 1).values  # 0: mostly true, 1: others\n",
        "y = np.array(labels)\n",
        "print(\"Label distribution:\", np.bincount(y))  # [1669, 613]\n",
        "\n",
        "# Move files to Google Drive\n",
        "!mv /content/network.mat /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"Network features saved to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Social Branch (Graph Construction and GCN Embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "IRbmJC6fq2Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "\n",
        "# Load standardized network features\n",
        "X_net_std = sio.loadmat('/content/drive/MyDrive/Projects/Hayat/network.mat')['X_net_std']\n",
        "print(\"NaN in X_net_std (loaded):\", np.isnan(X_net_std).any())  # Should be False\n",
        "\n",
        "# Construct graph\n",
        "G = nx.Graph()\n",
        "for idx in range(len(df)):\n",
        "    G.add_node(idx, features=X_net_std[idx])\n",
        "account_groups = df.groupby('account_id').indices\n",
        "for account_id, indices in account_groups.items():\n",
        "    indices = list(indices)\n",
        "    for i in range(len(indices)):\n",
        "        for j in range(i + 1, len(indices)):\n",
        "            G.add_edge(indices[i], indices[j])\n",
        "print(\"Graph nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n",
        "\n",
        "# Prepare GNN data\n",
        "edges = list(G.edges)\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "x = torch.tensor(X_net_std, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "print(\"GNN Data:\", data)\n",
        "\n",
        "# Define GCN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, out_channels=128):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Compute GCN embeddings\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gcn_model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "gcn_model.eval()\n",
        "with torch.no_grad():\n",
        "    gcn_embeddings = gcn_model(data)  # (2282, 128)\n",
        "print(\"GCN Embeddings shape:\", gcn_embeddings.shape)\n",
        "print(\"NaN in gcn_embeddings:\", torch.isnan(gcn_embeddings).any().item())  # Should be False\n",
        "\n",
        "# Save GCN model and embeddings\n",
        "torch.save(gcn_model.state_dict(), 'gcn_model.pth')\n",
        "torch.save(gcn_embeddings.cpu(), 'gcn_embeddings.pt')\n",
        "print(\"GCN model and embeddings saved\")\n",
        "\n",
        "# Move to Google Drive\n",
        "!mv /content/gcn_model.pth /content/drive/MyDrive/Projects/Hayat/\n",
        "!mv /content/gcn_embeddings.pt /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"GCN model and embeddings moved to Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7cKaUjSqojW",
        "outputId": "7ec64291-4980-425c-8058-1bf51e5a4df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN in X_net_std (loaded): False\n",
            "Graph nodes: 2282 Edges: 368312\n",
            "GNN Data: Data(x=[2282, 3], edge_index=[2, 370594])\n",
            "GCN Embeddings shape: torch.Size([2282, 128])\n",
            "NaN in gcn_embeddings: False\n",
            "GCN model and embeddings saved\n",
            "GCN model and embeddings moved to Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Text Branch (BERT + Attention Embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "B1BECSj7rB6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Define Attention Layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        scores = self.attention(embeddings)  # (batch_size, seq_len, 1)\n",
        "        scores = torch.softmax(scores, dim=1)  # (batch_size, seq_len, 1)\n",
        "        context = torch.sum(embeddings * scores, dim=1)  # (batch_size, hidden_dim)\n",
        "        return context\n",
        "\n",
        "# BERT Setup\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "attention_layer = Attention(hidden_dim=768).to(device)\n",
        "\n",
        "# Process texts with BERT + Attention\n",
        "batch_size = 32\n",
        "bert_embeddings = []\n",
        "texts = df['Context Post'].fillna(\"\").tolist()\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i + batch_size]\n",
        "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=117)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        token_embeddings = outputs.last_hidden_state  # (batch_size, seq_len, 768)\n",
        "        context_vector = attention_layer(token_embeddings)  # (batch_size, 768)\n",
        "        bert_embeddings.append(context_vector.cpu())\n",
        "\n",
        "# Concatenate all batches\n",
        "bert_embeddings = torch.cat(bert_embeddings, dim=0)  # (2282, 768)\n",
        "print(\"BERT Embeddings with Attention shape:\", bert_embeddings.shape)\n",
        "print(\"NaN in bert_embeddings:\", torch.isnan(bert_embeddings).any().item())  # Should be False\n",
        "\n",
        "# Save embeddings\n",
        "torch.save(bert_embeddings, 'bert_embeddings_with_attention.pt')\n",
        "!mv /content/bert_embeddings_with_attention.pt /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"BERT embeddings with Attention saved to Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9chfy1z1rJt5",
        "outputId": "6d72ce77-b969-4bf2-c5b5-23169964e73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Embeddings with Attention shape: torch.Size([2282, 768])\n",
            "NaN in bert_embeddings: False\n",
            "BERT embeddings with Attention saved to Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Combine Embeddings and Address Class Imbalance\n",
        "\n"
      ],
      "metadata": {
        "id": "VZ6VDcnUstD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load embeddings\n",
        "gcn_embeddings = torch.load('/content/drive/MyDrive/Projects/Hayat/gcn_embeddings.pt')  # (2282, 128)\n",
        "bert_embeddings = torch.load('/content/drive/MyDrive/Projects/Hayat/bert_embeddings_with_attention.pt')  # (2282, 768)\n",
        "\n",
        "# Check for NaN in embeddings\n",
        "print(\"NaN in gcn_embeddings:\", torch.isnan(gcn_embeddings).any().item())\n",
        "print(\"NaN in bert_embeddings:\", torch.isnan(bert_embeddings).any().item())\n",
        "\n",
        "# Combine embeddings\n",
        "combined_embeddings = torch.cat((gcn_embeddings, bert_embeddings), dim=1)  # (2282, 896)\n",
        "print(\"Combined embeddings shape:\", combined_embeddings.shape)\n",
        "print(\"NaN in combined_embeddings:\", torch.isnan(combined_embeddings).any().item())\n",
        "\n",
        "# Address class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(combined_embeddings.numpy(), y)\n",
        "X_resampled = torch.tensor(X_resampled, dtype=torch.float)\n",
        "y_resampled = torch.tensor(y_resampled, dtype=torch.long)\n",
        "print(\"Resampled data shape:\", X_resampled.shape, \"Label distribution:\", np.bincount(y_resampled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZgIdhmsq8f",
        "outputId": "c4b12ae5-949b-4150-a53f-022d19f1e8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN in gcn_embeddings: False\n",
            "NaN in bert_embeddings: False\n",
            "Combined embeddings shape: torch.Size([2282, 896])\n",
            "NaN in combined_embeddings: False\n",
            "Resampled data shape: torch.Size([3338, 896]) Label distribution: [1669 1669]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN in embeddings\n",
        "print(\"NaN in gcn_embeddings:\", torch.isnan(gcn_embeddings).any().item())\n",
        "print(\"NaN in bert_embeddings:\", torch.isnan(bert_embeddings).any().item())\n",
        "print(\"NaN in combined_embeddings:\", torch.isnan(combined_embeddings).any().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IvmGDB22h98",
        "outputId": "517090a1-9d6c-422c-a902-da6a2e3f7a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN in gcn_embeddings: True\n",
            "NaN in bert_embeddings: False\n",
            "NaN in combined_embeddings: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Ensemble and Train the Classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "7A4dVTzQ5Jvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ensemble classifier\n",
        "class EnsembleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=896, hidden_dim1=512, hidden_dim2=256, num_classes=2):\n",
        "        super(EnsembleClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare data for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Split data\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, stratify=y_resampled, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42)\n",
        "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val), \"Test size:\", len(X_test))\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "ensemble_classifier = EnsembleClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(ensemble_classifier.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "counter = 0\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    ensemble_classifier.train()\n",
        "    train_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ensemble_classifier(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    ensemble_classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = ensemble_classifier(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(ensemble_classifier.state_dict(), 'best_ensemble_classifier.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Evaluate on test set\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "ensemble_classifier.load_state_dict(torch.load('best_ensemble_classifier.pth'))\n",
        "ensemble_classifier.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = ensemble_classifier(batch_X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(batch_y.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Compute metrics\n",
        "print(\"\\nEnsemble Test Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
        "print(\"\\nEnsemble Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['mostly true (0)', 'others (1)']))\n",
        "\n",
        "# Save the model\n",
        "torch.save(ensemble_classifier.state_dict(), 'final_ensemble_classifier.pth')\n",
        "!mv /content/final_ensemble_classifier.pth /content/drive/MyDrive/Projects/Hayat/\n",
        "!mv /content/best_ensemble_classifier.pth /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"Ensemble model saved to Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyy8faqt4020",
        "outputId": "5c0d9673-5759-44cc-8bf3-78824f2bbbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1868 Val size: 468 Test size: 1002\n",
            "Epoch 1/50, Train Loss: 0.5520, Val Loss: 0.5015\n",
            "Epoch 2/50, Train Loss: 0.4721, Val Loss: 0.4955\n",
            "Epoch 3/50, Train Loss: 0.4552, Val Loss: 0.4766\n",
            "Epoch 4/50, Train Loss: 0.4378, Val Loss: 0.4724\n",
            "Epoch 5/50, Train Loss: 0.4100, Val Loss: 0.4581\n",
            "Epoch 6/50, Train Loss: 0.4064, Val Loss: 0.4591\n",
            "Epoch 7/50, Train Loss: 0.3995, Val Loss: 0.4405\n",
            "Epoch 8/50, Train Loss: 0.3827, Val Loss: 0.4418\n",
            "Epoch 9/50, Train Loss: 0.3822, Val Loss: 0.4366\n",
            "Epoch 10/50, Train Loss: 0.3603, Val Loss: 0.4386\n",
            "Epoch 11/50, Train Loss: 0.3381, Val Loss: 0.4332\n",
            "Epoch 12/50, Train Loss: 0.3329, Val Loss: 0.4451\n",
            "Epoch 13/50, Train Loss: 0.3297, Val Loss: 0.4490\n",
            "Epoch 14/50, Train Loss: 0.3116, Val Loss: 0.4380\n",
            "Epoch 15/50, Train Loss: 0.2937, Val Loss: 0.4527\n",
            "Epoch 16/50, Train Loss: 0.2792, Val Loss: 0.4333\n",
            "Epoch 17/50, Train Loss: 0.2845, Val Loss: 0.4571\n",
            "Epoch 18/50, Train Loss: 0.2826, Val Loss: 0.4541\n",
            "Epoch 19/50, Train Loss: 0.2784, Val Loss: 0.4571\n",
            "Epoch 20/50, Train Loss: 0.2745, Val Loss: 0.4713\n",
            "Epoch 21/50, Train Loss: 0.2728, Val Loss: 0.4596\n",
            "Early stopping triggered\n",
            "\n",
            "Ensemble Test Results:\n",
            "Accuracy: 0.8163672654690619\n",
            "Precision: 0.8440024779389786\n",
            "Recall: 0.8163672654690619\n",
            "\n",
            "Ensemble Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "mostly true (0)       0.94      0.67      0.79       501\n",
            "     others (1)       0.75      0.96      0.84       501\n",
            "\n",
            "       accuracy                           0.82      1002\n",
            "      macro avg       0.84      0.82      0.81      1002\n",
            "   weighted avg       0.84      0.82      0.81      1002\n",
            "\n",
            "Ensemble model saved to Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Deployment\n",
        "\n"
      ],
      "metadata": {
        "id": "yWiaJZ-v5Ymz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models for inference\n",
        "gcn_model = GCN().to(device)\n",
        "gcn_model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/gcn_model.pth', map_location=device))\n",
        "gcn_model.eval()\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "attention_layer = Attention(hidden_dim=768).to(device)\n",
        "\n",
        "ensemble_classifier = EnsembleClassifier().to(device)\n",
        "ensemble_classifier.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/final_ensemble_classifier.pth', map_location=device))\n",
        "ensemble_classifier.eval()\n",
        "\n",
        "# Load scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(sio.loadmat('/content/drive/MyDrive/Projects/Hayat/network.mat')['X_net_std'])\n",
        "\n",
        "# Inference function\n",
        "def predict_veracity(post_data):\n",
        "    \"\"\"\n",
        "    post_data: dict with 'account_id', 'share_count', 'reaction_count', 'comment_count', 'Context Post'\n",
        "    Returns: dict with predicted class and probabilities\n",
        "    \"\"\"\n",
        "    # Social branch: GCN\n",
        "    network_features = np.array([[post_data['share_count'],\n",
        "                                  post_data['reaction_count'],\n",
        "                                  post_data['comment_count']]])\n",
        "    X_net_std = scaler.transform(network_features)  # (1, 3)\n",
        "    x = torch.tensor(X_net_std, dtype=torch.float).to(device)\n",
        "    edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)  # Self-loop\n",
        "    data = Data(x=x, edge_index=edge_index).to(device)\n",
        "    with torch.no_grad():\n",
        "        gcn_emb = gcn_model(data)  # (1, 128)\n",
        "\n",
        "    # Text branch: BERT + Attention\n",
        "    text = post_data['Context Post'] or \"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=117)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        bert_out = bert_model(**inputs)\n",
        "        token_embeddings = bert_out.last_hidden_state  # (1, seq_len, 768)\n",
        "        bert_emb = attention_layer(token_embeddings)  # (1, 768)\n",
        "\n",
        "    # Combine embeddings\n",
        "    combined_emb = torch.cat((gcn_emb, bert_emb), dim=1)  # (1, 896)\n",
        "\n",
        "    # Predict with ensemble classifier\n",
        "    with torch.no_grad():\n",
        "        logits = ensemble_classifier(combined_emb)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]  # [P(0), P(1)]\n",
        "        pred = np.argmax(probs)\n",
        "\n",
        "    return {\n",
        "        'prediction': 'mostly true' if pred == 0 else 'others',\n",
        "        'probabilities': {'mostly true': probs[0], 'others': probs[1]}\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "new_post = {\n",
        "    'account_id': '123',\n",
        "    'share_count': 10,\n",
        "    'reaction_count': 50,\n",
        "    'comment_count': 20,\n",
        "    'Context Post': 'This is a sample post about a news event.'\n",
        "}\n",
        "result = predict_veracity(new_post)\n",
        "print(\"Prediction:\", result['prediction'])\n",
        "print(\"Probabilities:\", result['probabilities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P72ktdPt5VIX",
        "outputId": "0ce8707e-f625-4566-cb9e-00bfadb394f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: others\n",
            "Probabilities: {'mostly true': np.float32(0.34932137), 'others': np.float32(0.6506787)}\n"
          ]
        }
      ]
    }
  ]
}