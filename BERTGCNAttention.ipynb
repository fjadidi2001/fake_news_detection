{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4MPWezOPxelGEVuuWjFnO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/BERTGCNAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup and Data Loading\n",
        "\n"
      ],
      "metadata": {
        "id": "K-4U7Y5EqCOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvTEVxbhnXsW",
        "outputId": "d3035c9f-1fdf-4b89-9ff5-ba16078b41d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/363.4 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torch-geometric transformers imbalanced-learn -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "from scipy import io as sio\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "\n",
        "# Extract network features and handle NaN\n",
        "network_features = df[['share_count', 'reaction_count', 'comment_count']].fillna(0).values  # Replace NaN with 0\n",
        "print(\"NaN in network features before filling:\", np.isnan(network_features).any())  # Should be True if NaN exists\n",
        "print(\"NaN in network features after filling:\", np.isnan(network_features).any())  # Should be False\n",
        "\n",
        "# Standardize network features\n",
        "scaler = StandardScaler()\n",
        "X_net_std = scaler.fit_transform(network_features)  # (2282, 3)\n",
        "print(\"NaN in X_net_std:\", np.isnan(X_net_std).any())  # Should be False\n",
        "\n",
        "# Save standardized features\n",
        "sio.savemat('network.mat', {'X_net_std': X_net_std})\n",
        "print(\"Network features shape:\", X_net_std.shape)\n",
        "\n",
        "# Prepare labels (binary classification)\n",
        "labels = df['Rating'].apply(lambda x: 0 if x == 'mostly true' else 1).values  # 0: mostly true, 1: others\n",
        "y = np.array(labels)\n",
        "print(\"Label distribution:\", np.bincount(y))  # [1669, 613]\n",
        "\n",
        "# Move files to Google Drive\n",
        "!mv /content/network.mat /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"Network features saved to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bar Chart (Class Distribution):\n",
        "\n"
      ],
      "metadata": {
        "id": "2nl4hCyU0aUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=y)\n",
        "plt.title(\"Class Distribution (0: Mostly True, 1: Others)\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jo1ZR7400DuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pie Chart (Class Proportions):\n"
      ],
      "metadata": {
        "id": "5WR4saYh0Uh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(np.bincount(y), labels=['Mostly True (0)', 'Others (1)'], autopct='%1.1f%%')\n",
        "plt.title(\"Class Proportions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tdqGEd3z0N5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box Plot (Feature Spread):\n",
        "\n"
      ],
      "metadata": {
        "id": "-XONpvst04ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(data=df[['share_count', 'reaction_count', 'comment_count']].fillna(0))\n",
        "plt.title(\"Box Plot of Network Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bvCp1vzz097W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table (Summary Statistics):\n",
        "\n"
      ],
      "metadata": {
        "id": "c-KAxzGv1F4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary Statistics of Network Features:\")\n",
        "print(pd.DataFrame(network_features, columns=['share_count', 'reaction_count', 'comment_count']).describe())"
      ],
      "metadata": {
        "id": "mY_R-J8W1NQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Social Branch (Graph Construction and GCN Embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "IRbmJC6fq2Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "\n",
        "# Load standardized network features\n",
        "X_net_std = sio.loadmat('/content/drive/MyDrive/Projects/Hayat/network.mat')['X_net_std']\n",
        "print(\"NaN in X_net_std (loaded):\", np.isnan(X_net_std).any())  # Should be False\n",
        "\n",
        "# Construct graph\n",
        "G = nx.Graph()\n",
        "for idx in range(len(df)):\n",
        "    G.add_node(idx, features=X_net_std[idx])\n",
        "account_groups = df.groupby('account_id').indices\n",
        "for account_id, indices in account_groups.items():\n",
        "    indices = list(indices)\n",
        "    for i in range(len(indices)):\n",
        "        for j in range(i + 1, len(indices)):\n",
        "            G.add_edge(indices[i], indices[j])\n",
        "print(\"Graph nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n",
        "\n",
        "# Prepare GNN data\n",
        "edges = list(G.edges)\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "x = torch.tensor(X_net_std, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "print(\"GNN Data:\", data)\n",
        "\n",
        "# Define GCN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, out_channels=128):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Compute GCN embeddings\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gcn_model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "gcn_model.eval()\n",
        "with torch.no_grad():\n",
        "    gcn_embeddings = gcn_model(data)  # (2282, 128)\n",
        "print(\"GCN Embeddings shape:\", gcn_embeddings.shape)\n",
        "print(\"NaN in gcn_embeddings:\", torch.isnan(gcn_embeddings).any().item())  # Should be False\n",
        "\n",
        "# Save GCN model and embeddings\n",
        "torch.save(gcn_model.state_dict(), 'gcn_model.pth')\n",
        "torch.save(gcn_embeddings.cpu(), 'gcn_embeddings.pt')\n",
        "print(\"GCN model and embeddings saved\")\n",
        "\n",
        "# Move to Google Drive\n",
        "!mv /content/gcn_model.pth /content/drive/MyDrive/Projects/Hayat/\n",
        "!mv /content/gcn_embeddings.pt /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"GCN model and embeddings moved to Google Drive\")"
      ],
      "metadata": {
        "id": "l7cKaUjSqojW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram (Node Degree Distribution):\n",
        "\n"
      ],
      "metadata": {
        "id": "3eG4uqvA1w_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = [d for _, d in G.degree()]\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(degrees, bins=30)\n",
        "plt.title(\"Node Degree Distribution\")\n",
        "plt.xlabel(\"Degree\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3dqPodDl1t2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heatmap (Feature Correlation):\n",
        "\n"
      ],
      "metadata": {
        "id": "xudBZKGF18JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(pd.DataFrame(X_net_std, columns=['share_count', 'reaction_count', 'comment_count']).corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap of Network Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-aW0hXqB15d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table (Graph Statistics):\n",
        "\n"
      ],
      "metadata": {
        "id": "O7b-7yT_2Bzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Graph Statistics:\")\n",
        "print(pd.DataFrame({\n",
        "    'Nodes': [G.number_of_nodes()],\n",
        "    'Edges': [G.number_of_edges()],\n",
        "    'Avg Degree': [np.mean([d for _, d in G.degree()])]\n",
        "}))"
      ],
      "metadata": {
        "id": "eKunVh642GSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Text Branch (BERT + Attention Embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "B1BECSj7rB6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Define Attention Layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        scores = self.attention(embeddings)  # (batch_size, seq_len, 1)\n",
        "        scores = torch.softmax(scores, dim=1)  # (batch_size, seq_len, 1)\n",
        "        context = torch.sum(embeddings * scores, dim=1)  # (batch_size, hidden_dim)\n",
        "        return context\n",
        "\n",
        "# BERT Setup\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "attention_layer = Attention(hidden_dim=768).to(device)\n",
        "\n",
        "# Process texts with BERT + Attention\n",
        "batch_size = 32\n",
        "bert_embeddings = []\n",
        "texts = df['Context Post'].fillna(\"\").tolist()\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i + batch_size]\n",
        "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=117)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        token_embeddings = outputs.last_hidden_state  # (batch_size, seq_len, 768)\n",
        "        context_vector = attention_layer(token_embeddings)  # (batch_size, 768)\n",
        "        bert_embeddings.append(context_vector.cpu())\n",
        "\n",
        "# Concatenate all batches\n",
        "bert_embeddings = torch.cat(bert_embeddings, dim=0)  # (2282, 768)\n",
        "print(\"BERT Embeddings with Attention shape:\", bert_embeddings.shape)\n",
        "print(\"NaN in bert_embeddings:\", torch.isnan(bert_embeddings).any().item())  # Should be False\n",
        "\n",
        "# Save embeddings\n",
        "torch.save(bert_embeddings, 'bert_embeddings_with_attention.pt')\n",
        "!mv /content/bert_embeddings_with_attention.pt /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"BERT embeddings with Attention saved to Google Drive\")"
      ],
      "metadata": {
        "id": "9chfy1z1rJt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram (Text Length Distribution):\n",
        "\n"
      ],
      "metadata": {
        "id": "1QrfXZ_92evp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_lengths = [len(tokenizer.encode(text, max_length=512)) for text in texts]\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(text_lengths, bins=30)\n",
        "plt.title(\"Distribution of Text Lengths (Tokens)\")\n",
        "plt.xlabel(\"Token Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gJ-rvevK2hMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box Plot (Embedding Norms):\n",
        "\n"
      ],
      "metadata": {
        "id": "hS64MH3j2p_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norms = torch.norm(bert_embeddings, dim=1).numpy()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(y=norms)\n",
        "plt.title(\"Box Plot of BERT Embedding Norms\")\n",
        "plt.ylabel(\"L2 Norm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o0Rzw5Ze2sUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Combine Embeddings and Address Class Imbalance\n",
        "\n"
      ],
      "metadata": {
        "id": "VZ6VDcnUstD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load embeddings\n",
        "gcn_embeddings = torch.load('/content/drive/MyDrive/Projects/Hayat/gcn_embeddings.pt')  # (2282, 128)\n",
        "bert_embeddings = torch.load('/content/drive/MyDrive/Projects/Hayat/bert_embeddings_with_attention.pt')  # (2282, 768)\n",
        "\n",
        "# Check for NaN in embeddings\n",
        "print(\"NaN in gcn_embeddings:\", torch.isnan(gcn_embeddings).any().item())\n",
        "print(\"NaN in bert_embeddings:\", torch.isnan(bert_embeddings).any().item())\n",
        "\n",
        "# Combine embeddings\n",
        "combined_embeddings = torch.cat((gcn_embeddings, bert_embeddings), dim=1)  # (2282, 896)\n",
        "print(\"Combined embeddings shape:\", combined_embeddings.shape)\n",
        "print(\"NaN in combined_embeddings:\", torch.isnan(combined_embeddings).any().item())\n",
        "\n",
        "# Address class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(combined_embeddings.numpy(), y)\n",
        "X_resampled = torch.tensor(X_resampled, dtype=torch.float)\n",
        "y_resampled = torch.tensor(y_resampled, dtype=torch.long)\n",
        "print(\"Resampled data shape:\", X_resampled.shape, \"Label distribution:\", np.bincount(y_resampled))"
      ],
      "metadata": {
        "id": "U1ZgIdhmsq8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bar Chart (Class Distribution After SMOTE):\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CTndhJkl8au3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")  # or \"whitegrid\", \"dark\", \"white\", \"ticks\", etc.\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "sns.countplot(x=y, ax=axes[0])\n",
        "axes[0].set_title(\"Before SMOTE\")\n",
        "sns.countplot(x=y_resampled, ax=axes[1])\n",
        "axes[1].set_title(\"After SMOTE\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cyA6bZSa8xNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''"
      ],
      "metadata": {
        "id": "Li1fytAAa6A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE Plot (Embedding Distribution):\n",
        "\n"
      ],
      "metadata": {
        "id": "kq-Zh_Ax83OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(combined_embeddings.numpy())\n",
        "X_resampled_tsne = tsne.fit_transform(X_resampled.numpy())\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "for ax, data, title in [(axes[0], X_tsne, \"Before SMOTE\"), (axes[1], X_resampled_tsne, \"After SMOTE\")]:\n",
        "    scatter = ax.scatter(data[:, 0], data[:, 1], c=y_resampled[:len(data)], cmap='viridis')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LU4YUh7H87ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heatmap (Correlation of Combined Embeddings):\n",
        "\n"
      ],
      "metadata": {
        "id": "6GZphaeu9EE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(pd.DataFrame(combined_embeddings.numpy()[:, :10]).corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap of Combined Embeddings (First 10 Dimensions)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EPIuLoaY9JTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN in embeddings\n",
        "print(\"NaN in gcn_embeddings:\", torch.isnan(gcn_embeddings).any().item())\n",
        "print(\"NaN in bert_embeddings:\", torch.isnan(bert_embeddings).any().item())\n",
        "print(\"NaN in combined_embeddings:\", torch.isnan(combined_embeddings).any().item())"
      ],
      "metadata": {
        "id": "8IvmGDB22h98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NaN Summary:\")\n",
        "print(pd.DataFrame({\n",
        "    'Embedding': ['GCN', 'BERT', 'Combined'],\n",
        "    'Contains NaN': [torch.isnan(gcn_embeddings).any().item(),\n",
        "                     torch.isnan(bert_embeddings).any().item(),\n",
        "                     torch.isnan(combined_embeddings).any().item()]\n",
        "}))"
      ],
      "metadata": {
        "id": "I295gGtV9ccw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Ensemble and Train the Classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "7A4dVTzQ5Jvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ensemble classifier\n",
        "class EnsembleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=896, hidden_dim1=512, hidden_dim2=256, num_classes=2):\n",
        "        super(EnsembleClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare data for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Split data\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, stratify=y_resampled, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42)\n",
        "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val), \"Test size:\", len(X_test))\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "ensemble_classifier = EnsembleClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(ensemble_classifier.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "counter = 0\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    ensemble_classifier.train()\n",
        "    train_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ensemble_classifier(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    ensemble_classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = ensemble_classifier(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(ensemble_classifier.state_dict(), 'best_ensemble_classifier.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Evaluate on test set\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "ensemble_classifier.load_state_dict(torch.load('best_ensemble_classifier.pth'))\n",
        "ensemble_classifier.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = ensemble_classifier(batch_X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(batch_y.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Compute metrics\n",
        "print(\"\\nEnsemble Test Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
        "print(\"\\nEnsemble Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['mostly true (0)', 'others (1)']))\n",
        "\n",
        "# Save the model\n",
        "torch.save(ensemble_classifier.state_dict(), 'final_ensemble_classifier.pth')\n",
        "!mv /content/final_ensemble_classifier.pth /content/drive/MyDrive/Projects/Hayat/\n",
        "!mv /content/best_ensemble_classifier.pth /content/drive/MyDrive/Projects/Hayat/\n",
        "print(\"Ensemble model saved to Google Drive\")"
      ],
      "metadata": {
        "id": "nyy8faqt4020"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Deployment\n",
        "\n"
      ],
      "metadata": {
        "id": "yWiaJZ-v5Ymz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models for inference\n",
        "gcn_model = GCN().to(device)\n",
        "gcn_model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/gcn_model.pth', map_location=device))\n",
        "gcn_model.eval()\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "attention_layer = Attention(hidden_dim=768).to(device)\n",
        "\n",
        "ensemble_classifier = EnsembleClassifier().to(device)\n",
        "ensemble_classifier.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/final_ensemble_classifier.pth', map_location=device))\n",
        "ensemble_classifier.eval()\n",
        "\n",
        "# Load scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(sio.loadmat('/content/drive/MyDrive/Projects/Hayat/network.mat')['X_net_std'])\n",
        "\n",
        "# Inference function\n",
        "def predict_veracity(post_data):\n",
        "    \"\"\"\n",
        "    post_data: dict with 'account_id', 'share_count', 'reaction_count', 'comment_count', 'Context Post'\n",
        "    Returns: dict with predicted class and probabilities\n",
        "    \"\"\"\n",
        "    # Social branch: GCN\n",
        "    network_features = np.array([[post_data['share_count'],\n",
        "                                  post_data['reaction_count'],\n",
        "                                  post_data['comment_count']]])\n",
        "    X_net_std = scaler.transform(network_features)  # (1, 3)\n",
        "    x = torch.tensor(X_net_std, dtype=torch.float).to(device)\n",
        "    edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)  # Self-loop\n",
        "    data = Data(x=x, edge_index=edge_index).to(device)\n",
        "    with torch.no_grad():\n",
        "        gcn_emb = gcn_model(data)  # (1, 128)\n",
        "\n",
        "    # Text branch: BERT + Attention\n",
        "    text = post_data['Context Post'] or \"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=117)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        bert_out = bert_model(**inputs)\n",
        "        token_embeddings = bert_out.last_hidden_state  # (1, seq_len, 768)\n",
        "        bert_emb = attention_layer(token_embeddings)  # (1, 768)\n",
        "\n",
        "    # Combine embeddings\n",
        "    combined_emb = torch.cat((gcn_emb, bert_emb), dim=1)  # (1, 896)\n",
        "\n",
        "    # Predict with ensemble classifier\n",
        "    with torch.no_grad():\n",
        "        logits = ensemble_classifier(combined_emb)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]  # [P(0), P(1)]\n",
        "        pred = np.argmax(probs)\n",
        "\n",
        "    return {\n",
        "        'prediction': 'mostly true' if pred == 0 else 'others',\n",
        "        'probabilities': {'mostly true': probs[0], 'others': probs[1]}\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "new_post = {\n",
        "    'account_id': '123',\n",
        "    'share_count': 10,\n",
        "    'reaction_count': 50,\n",
        "    'comment_count': 20,\n",
        "    'Context Post': 'This is a sample post about a news event.'\n",
        "}\n",
        "result = predict_veracity(new_post)\n",
        "print(\"Prediction:\", result['prediction'])\n",
        "print(\"Probabilities:\", result['probabilities'])"
      ],
      "metadata": {
        "id": "P72ktdPt5VIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}