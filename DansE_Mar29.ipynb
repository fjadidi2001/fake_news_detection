{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOETmZEQjhUexcjdxT3CAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/DansE_Mar29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "oA7Z8kvmccRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive in Colab"
      ],
      "metadata": {
        "id": "PxIpn64y8_JC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Mzv5ciD71hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c34f4ef-4ab2-48c2-b771-eed627578666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "9J1bk_HnDc7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Replace with your actual file path\n",
        "file_path = '/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path, encoding='latin-1')"
      ],
      "metadata": {
        "id": "bhyykHvM89kI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Data Inspection"
      ],
      "metadata": {
        "id": "TDzZ-uobD38a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(2))  # See first 2 rows\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSYdMJU3DiFH",
        "outputId": "1b8339e9-e749-4b69-c2d3-5d5e40c697d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     account_id       post_id    Category               Page  \\\n",
            "0  1.840000e+14  1.040000e+15  mainstream  ABC News Politics   \n",
            "1  1.840000e+14  1.040000e+15  mainstream  ABC News Politics   \n",
            "\n",
            "                                            Post URL Date Published Post Type  \\\n",
            "0  https://www.facebook.com/ABCNewsPolitics/posts...      9/19/2016     video   \n",
            "1  https://www.facebook.com/ABCNewsPolitics/posts...      9/19/2016      link   \n",
            "\n",
            "               Rating Debate  share_count  reaction_count  comment_count  \\\n",
            "0  no factual content    NaN          NaN           146.0           15.0   \n",
            "1         mostly true    NaN          1.0            33.0           34.0   \n",
            "\n",
            "                                        Context Post  \n",
            "0  WATCH: &quot;JEB EXCLAMATION POINT!&quot; - Je...  \n",
            "1  Can either candidate move the needle in the de...  \n",
            "\n",
            "Missing values:\n",
            " account_id           0\n",
            "post_id              0\n",
            "Category             0\n",
            "Page                 0\n",
            "Post URL             0\n",
            "Date Published       0\n",
            "Post Type            0\n",
            "Rating               0\n",
            "Debate            1984\n",
            "share_count         70\n",
            "reaction_count       2\n",
            "comment_count        2\n",
            "Context Post         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Values"
      ],
      "metadata": {
        "id": "_hVZN6j7FnqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 1: Fill categorical columns\n",
        "df['Rating'] = df['Rating'].fillna('Unknown')\n",
        "df['Debate'] = df['Debate'].fillna('Not Specified')\n",
        "\n",
        "# Strategy 2: Fill numerical columns with median\n",
        "numeric_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Alternative: Drop rows with critical missing values\n",
        "# df = df.dropna(subset=['important_column'])"
      ],
      "metadata": {
        "id": "CCesok3vEbWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Preprocessing"
      ],
      "metadata": {
        "id": "GFWw2cFnGKbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date to datetime format\n",
        "df['Date Published'] = pd.to_datetime(df['Date Published'], format='%m/%d/%Y')\n",
        "\n",
        "# Clean text columns\n",
        "df['Context Post'] = df['Context Post'].str.replace('\"', '')"
      ],
      "metadata": {
        "id": "-jLm9vpHGC-h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['account_id'] = df['account_id'].astype(str)\n",
        "df['post_id'] = df['post_id'].astype(str)"
      ],
      "metadata": {
        "id": "o2_ZzlhgGlq1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['Category', 'Page', 'Post Type']\n",
        "df[categorical_cols] = df[categorical_cols].fillna('Unknown')"
      ],
      "metadata": {
        "id": "DuV4oIehGoMR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgVsdZkBG2KU",
        "outputId": "5d16b487-68eb-47d9-fcff-c88889ad8418"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2282 entries, 0 to 2281\n",
            "Data columns (total 13 columns):\n",
            " #   Column          Non-Null Count  Dtype         \n",
            "---  ------          --------------  -----         \n",
            " 0   account_id      2282 non-null   object        \n",
            " 1   post_id         2282 non-null   object        \n",
            " 2   Category        2282 non-null   object        \n",
            " 3   Page            2282 non-null   object        \n",
            " 4   Post URL        2282 non-null   object        \n",
            " 5   Date Published  2282 non-null   datetime64[ns]\n",
            " 6   Post Type       2282 non-null   object        \n",
            " 7   Rating          2282 non-null   object        \n",
            " 8   Debate          2282 non-null   object        \n",
            " 9   share_count     2282 non-null   float64       \n",
            " 10  reaction_count  2282 non-null   float64       \n",
            " 11  comment_count   2282 non-null   float64       \n",
            " 12  Context Post    2282 non-null   object        \n",
            "dtypes: datetime64[ns](1), float64(3), object(9)\n",
            "memory usage: 231.9+ KB\n",
            "None\n",
            "account_id        0\n",
            "post_id           0\n",
            "Category          0\n",
            "Page              0\n",
            "Post URL          0\n",
            "Date Published    0\n",
            "Post Type         0\n",
            "Rating            0\n",
            "Debate            0\n",
            "share_count       0\n",
            "reaction_count    0\n",
            "comment_count     0\n",
            "Context Post      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processElement(elem):\n",
        "    id_line = elem[0]\n",
        "    text = elem[1]\n",
        "    # Use 'Context Post' instead of 'content' if needed\n",
        "    text = tkn.createCorpus(text, remove_stopwords=False)\n",
        "    return id_line, text"
      ],
      "metadata": {
        "id": "hGiF99i4JzJb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main source"
      ],
      "metadata": {
        "id": "LVGdPq9osmqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download and save to Drive (run once)\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip -O /content/drive/MyDrive/glove.6B.zip\n",
        "!unzip /content/drive/MyDrive/glove.6B.zip -d /content/drive/MyDrive/glove\n",
        "\n",
        "# Load from Drive in future sessions\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/glove/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K5rbfZArlgD",
        "outputId": "b4013e1c-35b5-4486-c05b-3c22230a7495"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/glove.6B.zip\n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.50d.txt  \n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.100d.txt  \n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.200d.txt  \n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.300d.txt  \n",
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wordembeddings"
      ],
      "metadata": {
        "id": "kzfAQRwVzfGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean up the environment\n",
        "!pip uninstall -y numpy mittens gensim scipy smart-open wrapt tensorflow tensorflow-datasets dm-tree numba\n",
        "\n",
        "# Step 2: Install compatible versions\n",
        "!pip install numpy==1.26.4 mittens==0.2 gensim==4.3.3 scipy==1.13.1 smart-open==7.1.0 wrapt==1.17.2\n",
        "\n",
        "# Step 3: Restart runtime (run this once, then comment out)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# Step 4: After restart, run the code\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-iINw1nwrYN",
        "outputId": "d1acb732-f0b4-4cfb-a026-a2af9beaa851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping mittens as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: scipy 1.14.1\n",
            "Uninstalling scipy-1.14.1:\n",
            "  Successfully uninstalled scipy-1.14.1\n",
            "Found existing installation: smart-open 7.1.0\n",
            "Uninstalling smart-open-7.1.0:\n",
            "  Successfully uninstalled smart-open-7.1.0\n",
            "Found existing installation: wrapt 1.17.2\n",
            "Uninstalling wrapt-1.17.2:\n",
            "  Successfully uninstalled wrapt-1.17.2\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: tensorflow-datasets 4.9.8\n",
            "Uninstalling tensorflow-datasets-4.9.8:\n",
            "  Successfully uninstalled tensorflow-datasets-4.9.8\n",
            "Found existing installation: dm-tree 0.1.9\n",
            "Uninstalling dm-tree-0.1.9:\n",
            "  Successfully uninstalled dm-tree-0.1.9\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mittens==0.2\n",
            "  Downloading mittens-0.2-py3-none-any.whl.metadata (377 bytes)\n",
            "Collecting gensim==4.3.3\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scipy==1.13.1\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open==7.1.0\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt==1.17.2\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, mittens, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "tensorflow-probability 0.25.0 requires dm-tree, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "shap 0.47.0 requires numba>=0.54, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 mittens-0.2 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)\n",
        "import osfit(cooccurrence, vocab=None, initial_embedding_dict=None)\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "    def word2MittensEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2mittens = np.empty(shape=(self.no_words, no_components))\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        model = Mittens(n=no_components, max_iter=epochs, mittens=learning_rate)\n",
        "        embeddings = model.fit(cooc_matrix, vocab=vocab)\n",
        "        self.word2mittens[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2mittens[idx] = embeddings[idx - 1]\n",
        "        return self.word2mittens\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    w2m = we.word2MittensEmbedding()\n",
        "    print(\"Mittens:\", w2m.shape)\n",
        "    print(w2m)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])\n",
        "    print(\"Mittens:\", w2m[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tJ-FKOOovXUR",
        "outputId": "2e02fed7-7d80-488e-b76e-531ea717c3a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version: 0.2\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.35759968 -0.41002173 -0.11296739 ... -0.13695379 -0.08978107\n",
            "  -0.02064198]\n",
            " [ 0.03458208 -0.26300714  0.31167143 ... -0.42121385  0.17037075\n",
            "   0.06615572]\n",
            " ...\n",
            " [ 0.00209569  0.26936345  0.07195031 ...  0.01113254  0.02607798\n",
            "   0.16050002]\n",
            " [ 0.00497766  0.02229993  0.00638193 ...  0.05898192 -0.15140509\n",
            "  -0.05942085]\n",
            " [ 0.08744059  0.12552654  0.06976608 ... -0.14350026 -0.07537804\n",
            "   0.00438942]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Mittens' object has no attribute 'has_embedding'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5e2d75135b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mw2m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2MittensEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mittens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a5e2d75135b9>\u001b[0m in \u001b[0;36mword2MittensEmbedding\u001b[0;34m(self, window_size, no_components, epochs, workers, learning_rate)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMittens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmittens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcooc_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2mittens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mittens/mittens_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_coincidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         return self._fit(X, weights, log_coincidence,\n\u001b[0m\u001b[1;32m     82\u001b[0m                          \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                          \u001b[0minitial_embedding_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_embedding_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mittens/np_mittens.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, coincidence, weights, log_coincidence, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             gradients, error = self._get_gradients_and_error(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 pred, log_coincidence, weights)\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mittens/np_mittens.py\u001b[0m in \u001b[0;36m_get_gradients_and_error\u001b[0;34m(self, predictions, log_coincidence, weights)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmittens\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mcurr_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mwgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmittens\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Mittens' object has no attribute 'has_embedding'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)\n",
        "\n",
        "# Minimal test for Mittens\n",
        "vocab = ['hello', 'world']\n",
        "cooc_matrix = np.array([[0, 1], [1, 0]])  # Simple co-occurrence matrix\n",
        "mittens_model = Mittens(n=10, max_iter=10)\n",
        "try:\n",
        "    embeddings = mittens_model.fit(cooc_matrix, vocab=vocab)\n",
        "    print(\"Mittens test successful:\", embeddings.shape)\n",
        "except Exception as e:\n",
        "    print(\"Mittens test failed:\", str(e))\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "    def word2MittensEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2mittens = np.empty(shape=(self.no_words, no_components))\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        model = Mittens(n=no_components, max_iter=epochs, mittens=learning_rate)\n",
        "        try:\n",
        "            embeddings = model.fit(cooc_matrix, vocab=vocab)  # Explicitly test this\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for word, idx in self.word2id.items():\n",
        "                self.word2mittens[idx] = embeddings[idx - 1]\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error in Mittens.fit: {e}\")\n",
        "            embeddings = np.zeros((len(vocab), no_components))  # Fallback\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for i, word in enumerate(vocab, 1):\n",
        "                self.word2mittens[i] = embeddings[i - 1]\n",
        "        return self.word2mittens\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    w2m = we.word2MittensEmbedding()\n",
        "    print(\"Mittens:\", w2m.shape)\n",
        "    print(w2m)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])\n",
        "    print(\"Mittens:\", w2m[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzniUIiExhUt",
        "outputId": "976c519e-ad97-46f8-df34-92413a655258"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version: 0.2\n",
            "Mittens test failed: 'Mittens' object has no attribute 'has_embedding'\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.24156722  0.19728762  0.28115077 ...  0.25290197 -0.28166244\n",
            "   0.03103951]\n",
            " [-0.20704135 -0.10191444  0.2596386  ...  0.16890988  0.03980409\n",
            "   0.13393758]\n",
            " ...\n",
            " [-0.10637504 -0.02374604 -0.0016289  ...  0.03591276  0.28356568\n",
            "   0.06366436]\n",
            " [-0.05972623  0.04286632  0.09204827 ...  0.1193088   0.33685324\n",
            "  -0.31354328]\n",
            " [-0.12742595 -0.09153549  0.28084373 ... -0.32305798  0.05837046\n",
            "  -0.34258802]]\n",
            "Error in Mittens.fit: 'Mittens' object has no attribute 'has_embedding'\n",
            "Mittens: (15, 128)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "Comparison for word ID 1:\n",
            "Word2Vec: [-6.21626532e-05  3.19088437e-03 -7.12745590e-03  6.01633405e-03\n",
            "  4.82634921e-03  3.97735788e-03  5.58952475e-03  6.64290274e-03\n",
            "  6.49750407e-04 -1.30172889e-03  4.28103493e-04 -7.31625035e-03\n",
            "  6.59953337e-03 -5.00487117e-03  6.55153207e-03 -3.32944584e-03\n",
            "  5.09716512e-04 -7.12373713e-03 -7.48809706e-03 -6.06476609e-03\n",
            " -6.01806538e-03  2.62050075e-04 -5.66331577e-03 -3.86920548e-03\n",
            " -4.16113203e-03 -3.37171252e-03  5.41871926e-03  3.72564164e-03\n",
            "  6.80003501e-03  5.55603579e-03 -4.45879949e-03  5.70788607e-03\n",
            " -7.26995803e-03 -2.01427122e-03 -6.02612738e-03  3.31634958e-03\n",
            "  1.43352617e-03  5.52080385e-03  2.32943310e-03 -5.44838468e-03\n",
            "  5.98756177e-03 -4.64449916e-03  7.02972896e-03  2.28456082e-03\n",
            " -3.11649730e-03 -3.70101258e-03 -3.46256793e-03 -4.79195034e-03\n",
            "  7.32414331e-03 -2.00591167e-03  6.06871955e-03 -7.54190609e-03\n",
            "  1.68094877e-03 -9.79775796e-04  5.90900565e-03 -7.03375321e-03\n",
            "  5.83272101e-03 -3.95497913e-03 -4.70296340e-03 -4.40138392e-03\n",
            " -2.66821543e-03 -2.65624467e-03 -2.50413013e-03 -5.86187281e-03\n",
            "  6.00978325e-04 -4.53708722e-04 -1.28188857e-03  2.92269792e-03\n",
            " -5.91868116e-03 -2.48495163e-03  4.01353044e-03  6.70103775e-03\n",
            " -7.66575430e-03  5.61965723e-03  4.15526563e-03 -3.09771299e-03\n",
            "  6.73769321e-03 -7.15261744e-03  5.66246081e-03  4.26008087e-03\n",
            "  9.85841965e-04 -4.10628133e-03 -3.25019588e-03 -2.61222548e-03\n",
            "  1.28108205e-03  1.24865468e-03  5.82506089e-03  7.80742289e-03\n",
            "  6.93350285e-03 -3.08938394e-03  7.56247900e-03 -5.23670344e-04\n",
            "  3.73711600e-03  1.97167136e-03 -4.63132892e-04  2.85691721e-03\n",
            " -4.20561666e-03 -4.55231313e-03 -5.94439125e-03  1.47928542e-03\n",
            "  5.05184289e-03  6.73822535e-04  9.28732043e-04  2.46044085e-03\n",
            "  6.36986131e-03 -6.00371370e-03  1.73082878e-03 -5.86407073e-03\n",
            "  2.87816045e-03  7.43294507e-03  5.87216392e-03  5.02722338e-03\n",
            "  6.27435837e-03  5.13774622e-03  5.31336619e-03  6.73875213e-03\n",
            " -3.89733445e-03  7.20354123e-03  3.95858521e-03 -1.59854104e-03\n",
            "  6.62540551e-03  3.92472045e-03  7.54022226e-03  2.18087039e-03\n",
            "  7.74372742e-03  9.92520479e-04  7.14840600e-03  2.83964048e-03]\n",
            "FastText: [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04  1.07396161e-03\n",
            "  7.47827522e-04  2.43436918e-03  1.25494599e-03  8.32708203e-04\n",
            "  1.39498618e-04 -1.04375544e-03  1.66534202e-03  5.92023076e-04\n",
            " -1.01467571e-03  4.17726085e-04  1.15030364e-03 -8.53901089e-04\n",
            "  3.93663533e-04  1.66030577e-03  8.04261887e-04 -1.37844379e-03\n",
            " -1.00217399e-03 -1.71076367e-03 -6.08318776e-04 -3.13780096e-04\n",
            "  7.09959713e-04  6.52309740e-04 -1.03251881e-03  6.82479527e-04\n",
            "  1.76711939e-04 -1.86265504e-03  6.39586418e-04  1.27322774e-03\n",
            " -2.63598538e-03  1.30318059e-03  1.28268311e-03  7.31006585e-05\n",
            " -8.57610779e-04 -1.77271932e-03  4.76834684e-05  7.52914173e-04\n",
            "  6.25460816e-04 -3.61822196e-04 -2.49825674e-03  6.06271380e-04\n",
            "  2.61506008e-04 -1.91198278e-03 -7.39933399e-04 -4.00340045e-03\n",
            "  3.98896955e-04 -6.40470942e-04 -9.73124232e-04 -4.94266103e-04\n",
            "  9.33958218e-04  3.16818740e-04 -1.39790284e-03  8.03306350e-04\n",
            "  4.68277285e-04 -7.05544895e-04  8.22283386e-04 -1.07828462e-04\n",
            "  3.76482669e-04  2.11784281e-04 -4.11284942e-04  9.16630845e-04\n",
            " -3.64862324e-04 -6.43940584e-04 -1.22504728e-03 -4.09014639e-04\n",
            "  2.33947922e-05 -4.68008395e-04 -1.46471866e-04  1.28898071e-03\n",
            " -1.87438796e-03 -1.11290443e-04 -2.82968773e-04  1.62297476e-03\n",
            " -3.61244201e-05 -1.74025889e-03  2.76980782e-03 -8.65978887e-04\n",
            "  1.14296249e-03 -1.82072268e-04 -1.75701268e-03  7.34397385e-04\n",
            " -1.71435822e-03  3.74486670e-04 -2.81040557e-05 -2.88294628e-04\n",
            "  2.09705951e-03 -7.59013637e-04  1.51762320e-03 -4.17047850e-04\n",
            "  4.31056396e-04 -4.13735892e-04 -1.18768564e-03 -2.78128078e-03\n",
            " -3.55213997e-04 -2.31009349e-03  5.31876693e-04  1.93082809e-03\n",
            "  2.91791657e-04  5.18289220e-04 -7.66348530e-05 -5.61056775e-04\n",
            "  2.11288570e-03  1.20306201e-03  1.37995798e-04  7.44218880e-04\n",
            "  6.21078652e-04  2.57000350e-03  2.15380220e-03  1.53649191e-03\n",
            " -5.03137708e-04 -5.04195632e-04  1.45434562e-04  1.80694833e-03\n",
            "  3.40078317e-04 -9.81442863e-04 -7.77254521e-04 -1.38233358e-04\n",
            " -6.73915958e-04 -6.80455332e-06 -7.82594958e-04  1.34958001e-03\n",
            " -1.20654621e-03  4.79545008e-04  1.65466184e-03 -2.04305412e-04]\n",
            "GloVe: [-0.24156722  0.19728762  0.28115077 -0.01267708 -0.15987858  0.21574398\n",
            " -0.1480353  -0.17230529  0.18451083  0.30139633 -0.01786616 -0.10788706\n",
            " -0.02538971 -0.17632268  0.17546725  0.23223604 -0.04676747 -0.32730955\n",
            " -0.17357518  0.28444662  0.22395164 -0.15287594  0.02411032  0.16742667\n",
            " -0.30741247 -0.08741171 -0.17499335 -0.23875255 -0.10647899 -0.43408586\n",
            "  0.03226503 -0.02690753 -0.03439709 -0.06679865 -0.41822772  0.00441864\n",
            " -0.33425292  0.33217133 -0.02931358 -0.3106526   0.13540257  0.08195526\n",
            " -0.28979497 -0.00119301 -0.14958068 -0.11357246 -0.33342334  0.1647589\n",
            "  0.00192798  0.12798077  0.35912439  0.47988574  0.29816269  0.02062277\n",
            "  0.341725   -0.09348523  0.00970518  0.09629346 -0.13512328  0.03271592\n",
            " -0.22528884  0.12753717 -0.00965846  0.05195969 -0.09110937 -0.03771555\n",
            "  0.11739977 -0.04516209 -0.02465634 -0.05721831 -0.11449855 -0.16619301\n",
            "  0.05787712  0.03165011 -0.10521808 -0.23580368 -0.32708301  0.56562534\n",
            "  0.05927055  0.00094551  0.31607433 -0.35772211 -0.27208042 -0.19823527\n",
            "  0.04679147  0.07974814  0.15175472 -0.03920601  0.17394178  0.27804593\n",
            " -0.00697557 -0.43042942  0.23622731 -0.13813303 -0.2330843  -0.25773563\n",
            "  0.29068773 -0.32492003  0.02964249 -0.24167678  0.12750401 -0.00378335\n",
            " -0.28357644  0.20254182 -0.03352816  0.08491384  0.15584422  0.15268531\n",
            " -0.26024194  0.17879857 -0.18562622 -0.24850885 -0.13399434  0.43196456\n",
            "  0.26941041  0.27321471  0.08527431  0.17110002  0.12516943 -0.36612575\n",
            " -0.07978609 -0.01374283  0.14129145  0.29812763  0.42890394  0.25290197\n",
            " -0.28166244  0.03103951]\n",
            "Mittens: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)\n",
        "\n",
        "from scipy.sparse import csr_matrix  # Add sparse matrix support\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "    def word2MittensEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2mittens = np.empty(shape=(self.no_words, no_components))\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        # Convert to sparse matrix\n",
        "        cooc_matrix_sparse = csr_matrix(cooc_matrix)\n",
        "        model = Mittens(n=no_components, max_iter=epochs, mittens=learning_rate)\n",
        "        try:\n",
        "            embeddings = model.fit(cooc_matrix_sparse, vocab=vocab)\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for word, idx in self.word2id.items():\n",
        "                self.word2mittens[idx] = embeddings[idx - 1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Mittens.fit: {e}\")\n",
        "            embeddings = np.zeros((len(vocab), no_components))  # Fallback\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for i, word in enumerate(vocab, 1):\n",
        "                self.word2mittens[i] = embeddings[i - 1]\n",
        "        return self.word2mittens\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    w2m = we.word2MittensEmbedding()\n",
        "    print(\"Mittens:\", w2m.shape)\n",
        "    print(w2m)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])\n",
        "    print(\"Mittens:\", w2m[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHvZNe7qyhTU",
        "outputId": "9a141780-d10a-4947-c293-d2fb5d271c78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version: 0.2\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0001"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.24973279 -0.47211752 -0.3471582  ... -0.01941465  0.43606771\n",
            "  -0.0892403 ]\n",
            " [-0.2158313  -0.12271662 -0.21352648 ... -0.07537541  0.31608701\n",
            "   0.02839591]\n",
            " ...\n",
            " [ 0.16518111  0.128661   -0.0561627  ... -0.03275083  0.00815867\n",
            "  -0.02345717]\n",
            " [-0.09888651 -0.27747496  0.19922241 ... -0.0432742   0.0807169\n",
            "   0.25686838]\n",
            " [ 0.07700253 -0.14335636  0.38736752 ...  0.15984586  0.09920613\n",
            "   0.03748307]]\n",
            "Error in Mittens.fit: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().\n",
            "Mittens: (15, 128)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "Comparison for word ID 1:\n",
            "Word2Vec: [-6.21626532e-05  3.19088437e-03 -7.12745590e-03  6.01633405e-03\n",
            "  4.82634921e-03  3.97735788e-03  5.58952475e-03  6.64290274e-03\n",
            "  6.49750407e-04 -1.30172889e-03  4.28103493e-04 -7.31625035e-03\n",
            "  6.59953337e-03 -5.00487117e-03  6.55153207e-03 -3.32944584e-03\n",
            "  5.09716512e-04 -7.12373713e-03 -7.48809706e-03 -6.06476609e-03\n",
            " -6.01806538e-03  2.62050075e-04 -5.66331577e-03 -3.86920548e-03\n",
            " -4.16113203e-03 -3.37171252e-03  5.41871926e-03  3.72564164e-03\n",
            "  6.80003501e-03  5.55603579e-03 -4.45879949e-03  5.70788607e-03\n",
            " -7.26995803e-03 -2.01427122e-03 -6.02612738e-03  3.31634958e-03\n",
            "  1.43352617e-03  5.52080385e-03  2.32943310e-03 -5.44838468e-03\n",
            "  5.98756177e-03 -4.64449916e-03  7.02972896e-03  2.28456082e-03\n",
            " -3.11649730e-03 -3.70101258e-03 -3.46256793e-03 -4.79195034e-03\n",
            "  7.32414331e-03 -2.00591167e-03  6.06871955e-03 -7.54190609e-03\n",
            "  1.68094877e-03 -9.79775796e-04  5.90900565e-03 -7.03375321e-03\n",
            "  5.83272101e-03 -3.95497913e-03 -4.70296340e-03 -4.40138392e-03\n",
            " -2.66821543e-03 -2.65624467e-03 -2.50413013e-03 -5.86187281e-03\n",
            "  6.00978325e-04 -4.53708722e-04 -1.28188857e-03  2.92269792e-03\n",
            " -5.91868116e-03 -2.48495163e-03  4.01353044e-03  6.70103775e-03\n",
            " -7.66575430e-03  5.61965723e-03  4.15526563e-03 -3.09771299e-03\n",
            "  6.73769321e-03 -7.15261744e-03  5.66246081e-03  4.26008087e-03\n",
            "  9.85841965e-04 -4.10628133e-03 -3.25019588e-03 -2.61222548e-03\n",
            "  1.28108205e-03  1.24865468e-03  5.82506089e-03  7.80742289e-03\n",
            "  6.93350285e-03 -3.08938394e-03  7.56247900e-03 -5.23670344e-04\n",
            "  3.73711600e-03  1.97167136e-03 -4.63132892e-04  2.85691721e-03\n",
            " -4.20561666e-03 -4.55231313e-03 -5.94439125e-03  1.47928542e-03\n",
            "  5.05184289e-03  6.73822535e-04  9.28732043e-04  2.46044085e-03\n",
            "  6.36986131e-03 -6.00371370e-03  1.73082878e-03 -5.86407073e-03\n",
            "  2.87816045e-03  7.43294507e-03  5.87216392e-03  5.02722338e-03\n",
            "  6.27435837e-03  5.13774622e-03  5.31336619e-03  6.73875213e-03\n",
            " -3.89733445e-03  7.20354123e-03  3.95858521e-03 -1.59854104e-03\n",
            "  6.62540551e-03  3.92472045e-03  7.54022226e-03  2.18087039e-03\n",
            "  7.74372742e-03  9.92520479e-04  7.14840600e-03  2.83964048e-03]\n",
            "FastText: [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04  1.07396161e-03\n",
            "  7.47827522e-04  2.43436918e-03  1.25494599e-03  8.32708203e-04\n",
            "  1.39498618e-04 -1.04375544e-03  1.66534202e-03  5.92023076e-04\n",
            " -1.01467571e-03  4.17726085e-04  1.15030364e-03 -8.53901089e-04\n",
            "  3.93663533e-04  1.66030577e-03  8.04261887e-04 -1.37844379e-03\n",
            " -1.00217399e-03 -1.71076367e-03 -6.08318776e-04 -3.13780096e-04\n",
            "  7.09959713e-04  6.52309740e-04 -1.03251881e-03  6.82479527e-04\n",
            "  1.76711939e-04 -1.86265504e-03  6.39586418e-04  1.27322774e-03\n",
            " -2.63598538e-03  1.30318059e-03  1.28268311e-03  7.31006585e-05\n",
            " -8.57610779e-04 -1.77271932e-03  4.76834684e-05  7.52914173e-04\n",
            "  6.25460816e-04 -3.61822196e-04 -2.49825674e-03  6.06271380e-04\n",
            "  2.61506008e-04 -1.91198278e-03 -7.39933399e-04 -4.00340045e-03\n",
            "  3.98896955e-04 -6.40470942e-04 -9.73124232e-04 -4.94266103e-04\n",
            "  9.33958218e-04  3.16818740e-04 -1.39790284e-03  8.03306350e-04\n",
            "  4.68277285e-04 -7.05544895e-04  8.22283386e-04 -1.07828462e-04\n",
            "  3.76482669e-04  2.11784281e-04 -4.11284942e-04  9.16630845e-04\n",
            " -3.64862324e-04 -6.43940584e-04 -1.22504728e-03 -4.09014639e-04\n",
            "  2.33947922e-05 -4.68008395e-04 -1.46471866e-04  1.28898071e-03\n",
            " -1.87438796e-03 -1.11290443e-04 -2.82968773e-04  1.62297476e-03\n",
            " -3.61244201e-05 -1.74025889e-03  2.76980782e-03 -8.65978887e-04\n",
            "  1.14296249e-03 -1.82072268e-04 -1.75701268e-03  7.34397385e-04\n",
            " -1.71435822e-03  3.74486670e-04 -2.81040557e-05 -2.88294628e-04\n",
            "  2.09705951e-03 -7.59013637e-04  1.51762320e-03 -4.17047850e-04\n",
            "  4.31056396e-04 -4.13735892e-04 -1.18768564e-03 -2.78128078e-03\n",
            " -3.55213997e-04 -2.31009349e-03  5.31876693e-04  1.93082809e-03\n",
            "  2.91791657e-04  5.18289220e-04 -7.66348530e-05 -5.61056775e-04\n",
            "  2.11288570e-03  1.20306201e-03  1.37995798e-04  7.44218880e-04\n",
            "  6.21078652e-04  2.57000350e-03  2.15380220e-03  1.53649191e-03\n",
            " -5.03137708e-04 -5.04195632e-04  1.45434562e-04  1.80694833e-03\n",
            "  3.40078317e-04 -9.81442863e-04 -7.77254521e-04 -1.38233358e-04\n",
            " -6.73915958e-04 -6.80455332e-06 -7.82594958e-04  1.34958001e-03\n",
            " -1.20654621e-03  4.79545008e-04  1.65466184e-03 -2.04305412e-04]\n",
            "GloVe: [-0.24973279 -0.47211752 -0.3471582   0.09206807 -0.13866986  0.15487264\n",
            "  0.10668666 -0.1446956  -0.37882732 -0.31554635 -0.12138892  0.0336272\n",
            " -0.00930984  0.03009921 -0.13521719 -0.08467253 -0.19571198 -0.13949303\n",
            " -0.35111745  0.027482    0.11905168  0.18794745  0.11306527  0.12954\n",
            "  0.05691607  0.13720159  0.22953001  0.10246665  0.20592919 -0.21759341\n",
            " -0.23108193  0.21138186 -0.26241941  0.04116371 -0.21286277 -0.11801152\n",
            "  0.31120586 -0.29845012 -0.05790725 -0.18902083  0.22907882 -0.12611849\n",
            "  0.40500281 -0.22766443  0.04437517 -0.03016994 -0.00181209  0.24601004\n",
            " -0.1288323   0.14103511  0.25497694 -0.09054451  0.16266434  0.11801647\n",
            " -0.16773795 -0.12649524  0.21566807  0.01240493  0.01311322  0.2398281\n",
            "  0.24605117 -0.48681179  0.17264584  0.11088587 -0.30783911  0.25423832\n",
            "  0.05147336 -0.12579411  0.15714605 -0.10685451  0.31093266 -0.17077477\n",
            " -0.03805187 -0.31860494 -0.23026643 -0.17669139  0.26581533 -0.40427327\n",
            "  0.03385626  0.34479212 -0.06676299  0.09452871 -0.0890423  -0.02015828\n",
            " -0.25413745 -0.20022885  0.31813517 -0.0295259  -0.39070183 -0.29587508\n",
            " -0.04877057  0.56845457 -0.27300501  0.20099248  0.56908743  0.04327456\n",
            " -0.28636521  0.52484852  0.06302865 -0.00368409  0.30760488 -0.1834542\n",
            "  0.0405381  -0.01405919 -0.37281245  0.10027112  0.00985007 -0.19940571\n",
            " -0.19570702  0.02242911 -0.33625058 -0.22106823  0.44265247 -0.15831874\n",
            " -0.50237319  0.218131    0.34700928 -0.14113709  0.17201621  0.20608975\n",
            " -0.35104507 -0.23447789 -0.19862362  0.01224652 -0.30237286 -0.01941465\n",
            "  0.43606771 -0.0892403 ]\n",
            "Mittens: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### slimmed-down version without Mittens\n",
        "\n"
      ],
      "metadata": {
        "id": "UlYgCEY3y8LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import GloVe\n",
        "import mittens\n",
        "print(\"Mittens version (GloVe only):\", mittens.__version__)\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fuNa6-WuR4a",
        "outputId": "96a23ddc-0209-42e2-8fd2-ec5579559d96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version (GloVe only): 0.2\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.14491981 -0.09447325  0.12144629 ... -0.2624261  -0.18998113\n",
            "  -0.09980709]\n",
            " [ 0.32938427 -0.05994402  0.05077884 ...  0.01185149  0.20612515\n",
            "   0.28760553]\n",
            " ...\n",
            " [-0.01241104 -0.08506157  0.29257517 ... -0.17227083 -0.19028408\n",
            "  -0.19335479]\n",
            " [ 0.37489702 -0.22289123  0.05223132 ... -0.01077101 -0.10361216\n",
            "   0.00802199]\n",
            " [ 0.04938111  0.12498871  0.37583299 ...  0.16933881  0.26277163\n",
            "  -0.23265269]]\n",
            "\n",
            "\n",
            "Comparison for word ID 1:\n",
            "Word2Vec: [-6.21626532e-05  3.19088437e-03 -7.12745590e-03  6.01633405e-03\n",
            "  4.82634921e-03  3.97735788e-03  5.58952475e-03  6.64290274e-03\n",
            "  6.49750407e-04 -1.30172889e-03  4.28103493e-04 -7.31625035e-03\n",
            "  6.59953337e-03 -5.00487117e-03  6.55153207e-03 -3.32944584e-03\n",
            "  5.09716512e-04 -7.12373713e-03 -7.48809706e-03 -6.06476609e-03\n",
            " -6.01806538e-03  2.62050075e-04 -5.66331577e-03 -3.86920548e-03\n",
            " -4.16113203e-03 -3.37171252e-03  5.41871926e-03  3.72564164e-03\n",
            "  6.80003501e-03  5.55603579e-03 -4.45879949e-03  5.70788607e-03\n",
            " -7.26995803e-03 -2.01427122e-03 -6.02612738e-03  3.31634958e-03\n",
            "  1.43352617e-03  5.52080385e-03  2.32943310e-03 -5.44838468e-03\n",
            "  5.98756177e-03 -4.64449916e-03  7.02972896e-03  2.28456082e-03\n",
            " -3.11649730e-03 -3.70101258e-03 -3.46256793e-03 -4.79195034e-03\n",
            "  7.32414331e-03 -2.00591167e-03  6.06871955e-03 -7.54190609e-03\n",
            "  1.68094877e-03 -9.79775796e-04  5.90900565e-03 -7.03375321e-03\n",
            "  5.83272101e-03 -3.95497913e-03 -4.70296340e-03 -4.40138392e-03\n",
            " -2.66821543e-03 -2.65624467e-03 -2.50413013e-03 -5.86187281e-03\n",
            "  6.00978325e-04 -4.53708722e-04 -1.28188857e-03  2.92269792e-03\n",
            " -5.91868116e-03 -2.48495163e-03  4.01353044e-03  6.70103775e-03\n",
            " -7.66575430e-03  5.61965723e-03  4.15526563e-03 -3.09771299e-03\n",
            "  6.73769321e-03 -7.15261744e-03  5.66246081e-03  4.26008087e-03\n",
            "  9.85841965e-04 -4.10628133e-03 -3.25019588e-03 -2.61222548e-03\n",
            "  1.28108205e-03  1.24865468e-03  5.82506089e-03  7.80742289e-03\n",
            "  6.93350285e-03 -3.08938394e-03  7.56247900e-03 -5.23670344e-04\n",
            "  3.73711600e-03  1.97167136e-03 -4.63132892e-04  2.85691721e-03\n",
            " -4.20561666e-03 -4.55231313e-03 -5.94439125e-03  1.47928542e-03\n",
            "  5.05184289e-03  6.73822535e-04  9.28732043e-04  2.46044085e-03\n",
            "  6.36986131e-03 -6.00371370e-03  1.73082878e-03 -5.86407073e-03\n",
            "  2.87816045e-03  7.43294507e-03  5.87216392e-03  5.02722338e-03\n",
            "  6.27435837e-03  5.13774622e-03  5.31336619e-03  6.73875213e-03\n",
            " -3.89733445e-03  7.20354123e-03  3.95858521e-03 -1.59854104e-03\n",
            "  6.62540551e-03  3.92472045e-03  7.54022226e-03  2.18087039e-03\n",
            "  7.74372742e-03  9.92520479e-04  7.14840600e-03  2.83964048e-03]\n",
            "FastText: [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04  1.07396161e-03\n",
            "  7.47827522e-04  2.43436918e-03  1.25494599e-03  8.32708203e-04\n",
            "  1.39498618e-04 -1.04375544e-03  1.66534202e-03  5.92023076e-04\n",
            " -1.01467571e-03  4.17726085e-04  1.15030364e-03 -8.53901089e-04\n",
            "  3.93663533e-04  1.66030577e-03  8.04261887e-04 -1.37844379e-03\n",
            " -1.00217399e-03 -1.71076367e-03 -6.08318776e-04 -3.13780096e-04\n",
            "  7.09959713e-04  6.52309740e-04 -1.03251881e-03  6.82479527e-04\n",
            "  1.76711939e-04 -1.86265504e-03  6.39586418e-04  1.27322774e-03\n",
            " -2.63598538e-03  1.30318059e-03  1.28268311e-03  7.31006585e-05\n",
            " -8.57610779e-04 -1.77271932e-03  4.76834684e-05  7.52914173e-04\n",
            "  6.25460816e-04 -3.61822196e-04 -2.49825674e-03  6.06271380e-04\n",
            "  2.61506008e-04 -1.91198278e-03 -7.39933399e-04 -4.00340045e-03\n",
            "  3.98896955e-04 -6.40470942e-04 -9.73124232e-04 -4.94266103e-04\n",
            "  9.33958218e-04  3.16818740e-04 -1.39790284e-03  8.03306350e-04\n",
            "  4.68277285e-04 -7.05544895e-04  8.22283386e-04 -1.07828462e-04\n",
            "  3.76482669e-04  2.11784281e-04 -4.11284942e-04  9.16630845e-04\n",
            " -3.64862324e-04 -6.43940584e-04 -1.22504728e-03 -4.09014639e-04\n",
            "  2.33947922e-05 -4.68008395e-04 -1.46471866e-04  1.28898071e-03\n",
            " -1.87438796e-03 -1.11290443e-04 -2.82968773e-04  1.62297476e-03\n",
            " -3.61244201e-05 -1.74025889e-03  2.76980782e-03 -8.65978887e-04\n",
            "  1.14296249e-03 -1.82072268e-04 -1.75701268e-03  7.34397385e-04\n",
            " -1.71435822e-03  3.74486670e-04 -2.81040557e-05 -2.88294628e-04\n",
            "  2.09705951e-03 -7.59013637e-04  1.51762320e-03 -4.17047850e-04\n",
            "  4.31056396e-04 -4.13735892e-04 -1.18768564e-03 -2.78128078e-03\n",
            " -3.55213997e-04 -2.31009349e-03  5.31876693e-04  1.93082809e-03\n",
            "  2.91791657e-04  5.18289220e-04 -7.66348530e-05 -5.61056775e-04\n",
            "  2.11288570e-03  1.20306201e-03  1.37995798e-04  7.44218880e-04\n",
            "  6.21078652e-04  2.57000350e-03  2.15380220e-03  1.53649191e-03\n",
            " -5.03137708e-04 -5.04195632e-04  1.45434562e-04  1.80694833e-03\n",
            "  3.40078317e-04 -9.81442863e-04 -7.77254521e-04 -1.38233358e-04\n",
            " -6.73915958e-04 -6.80455332e-06 -7.82594958e-04  1.34958001e-03\n",
            " -1.20654621e-03  4.79545008e-04  1.65466184e-03 -2.04305412e-04]\n",
            "GloVe: [-0.14491981 -0.09447325  0.12144629  0.33654058 -0.26823103 -0.00851899\n",
            " -0.01720095  0.00417736  0.07016572  0.15440059 -0.05897453 -0.03388286\n",
            "  0.00166583  0.02728988 -0.02364711  0.20430687 -0.27785534  0.17192418\n",
            " -0.4214381  -0.31993437  0.37089122  0.31378692 -0.16434697  0.06254594\n",
            " -0.37126779  0.02074445  0.02755287 -0.03419243 -0.04800101  0.23402676\n",
            " -0.03905685  0.11388236 -0.09545866 -0.06881654  0.46783014 -0.1684731\n",
            " -0.30359971 -0.18722352 -0.31340323 -0.04368029 -0.0997356   0.20585347\n",
            " -0.12803517 -0.03103505 -0.22988654 -0.20921109  0.13892224 -0.25125431\n",
            " -0.34561802 -0.02544955 -0.11478891  0.2110641  -0.10692891  0.14797026\n",
            "  0.21640493  0.0203965   0.02068296 -0.02507762 -0.06478238 -0.21582785\n",
            " -0.18612787  0.33198676 -0.23596947 -0.1166765   0.08838274 -0.11613951\n",
            "  0.17417221  0.04276799 -0.29862126  0.33014894  0.04703465 -0.2152572\n",
            " -0.17628108  0.08623899 -0.0541081  -0.12096651 -0.07759373  0.12235248\n",
            "  0.01088267 -0.03379119  0.01154564 -0.0255124  -0.11684421 -0.08885345\n",
            "  0.17115128  0.17162357  0.25984815 -0.23705988 -0.06767285  0.23263982\n",
            "  0.17997825 -0.36049717 -0.27579434 -0.04070212  0.20846426 -0.25062646\n",
            " -0.22516775 -0.03231266  0.21774265  0.23756354  0.01277131  0.23864298\n",
            " -0.20071755  0.1087316  -0.3951387   0.05903905 -0.21734861  0.11410184\n",
            "  0.02475853  0.34337829 -0.08874847  0.25982878 -0.00071343  0.04635114\n",
            " -0.17474179 -0.08504665 -0.14526755  0.01800153 -0.04054321 -0.16867252\n",
            "  0.0332301   0.03932167 -0.35854248  0.15627902  0.40057809 -0.2624261\n",
            " -0.18998113 -0.09980709]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenization"
      ],
      "metadata": {
        "id": "Vomn4xUHzZEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages and download NLTK data\n",
        "!pip install numpy==1.26.4 gensim==4.3.3 mittens==0.2 spacy==3.7.2 stop-words==2018.7.23 -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from mittens import GloVe\n",
        "import os\n",
        "\n",
        "# Special characters dictionary\n",
        "specialchar_dic = {\n",
        "    \"’\": \"'\", \"„\": \"\\\"\", \"“\": \"\\\"\", \"”\": \"\\\"\", \"«\": \"<<\", \"»\": \">>\",\n",
        "    \"…\": \"...\", \"—\": \"--\", \"¡\": \"!\", \"¿\": \"?\", \"©\": \" \", \"–\": \" \"\n",
        "}\n",
        "\n",
        "# Stop words function (cached globally)\n",
        "def stopWordsEN():\n",
        "    sw_stop_words = get_stop_words('en')\n",
        "    sw_nltk = stopwords.words('english')\n",
        "    sw_spacy = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
        "    sw_mallet = ['a', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', 'different', 'do', 'does', 'doing', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', 'way', 'we', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'x', 'y', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n",
        "    return list(set(sw_stop_words + sw_nltk + sw_mallet + sw_spacy))\n",
        "\n",
        "# Precompile regex and load Spacy model\n",
        "punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "specialchar_re = re.compile('(%s)' % '|'.join(specialchar_dic.keys()))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "cachedStopWords_en = stopWordsEN()\n",
        "\n",
        "class Tokenization:\n",
        "    def applyFE(self, text):\n",
        "        \"\"\"Combine negation with words to reduce bias.\"\"\"\n",
        "        final_text = text.replace('cannot', 'can not').replace('can\\'t', 'can not')\n",
        "        final_text = final_text.replace('won\\'t', 'will not').replace('n\\'t', ' not').replace(' not ', ' not')\n",
        "        return final_text\n",
        "\n",
        "    def removeStopWords(self, text):\n",
        "        return ' '.join([word for word in text.split() if word not in cachedStopWords_en])\n",
        "\n",
        "    def removePunctuation(self, text, punctuation=punctuation):\n",
        "        for c in punctuation:\n",
        "            text = text.replace(c, ' ')\n",
        "        return text\n",
        "\n",
        "    def replaceUTF8Char(self, text, specialchars=specialchar_dic):\n",
        "        def replace(match):\n",
        "            return specialchars[match.group(0)]\n",
        "        return specialchar_re.sub(replace, text)\n",
        "\n",
        "    def createCorpus(self, text, remove_punctuation=True, remove_stopwords=True, apply_FE=True):\n",
        "        corpus = []\n",
        "        try:\n",
        "            text = self.replaceUTF8Char(text).replace(\"\\n\", \" \")\n",
        "            doc = nlp(text)\n",
        "            processed_text = ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text if not t.ent_type_ else t.text for t in doc])\n",
        "            processed_text = processed_text.replace(\"\\s\\s+\", ' ')\n",
        "\n",
        "            doc = nlp(processed_text.lower())\n",
        "            rawText = not (remove_punctuation or remove_stopwords or apply_FE)\n",
        "\n",
        "            for sentence in doc.sents:\n",
        "                sent = str(sentence.text)\n",
        "                if len(sent) == 0:\n",
        "                    continue\n",
        "                if not rawText:\n",
        "                    if apply_FE:\n",
        "                        sent = self.applyFE(text=sent)\n",
        "                    if remove_punctuation:\n",
        "                        sent = self.removePunctuation(text=sent)\n",
        "                    if remove_stopwords:\n",
        "                        sent = self.removeStopWords(text=sent)\n",
        "                sent = sent.lower().split()\n",
        "                if sent:\n",
        "                    corpus.append(sent)\n",
        "        except Exception as exp:\n",
        "            print('exception=', str(exp))\n",
        "            print('text=', text)\n",
        "        return corpus\n",
        "\n",
        "    def __del__(self):\n",
        "        print(\"Destructor Tokenization\")\n",
        "\n",
        "class WordEmbeddings:\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Tokenization example\n",
        "    tkn = Tokenization()\n",
        "    text1 = \"Apple data-intensive is looking at buying U.K. startup for $1 billion. This is great! The new D.P. model is funcitonal and ready\"\n",
        "    corpus1 = tkn.createCorpus(text1)\n",
        "    print(\"Corpus 1:\", corpus1)\n",
        "\n",
        "    # Larger text example\n",
        "    text2 = \"\"\"The lion may be known as the king of the jungle, but lions do not live in jungles. They’re the rulers of the African savannahs that are covered in brown grasses and speckled with sparse trees. Lions’ coloring helps them blend in perfectly with the tall grass so they can ambush their prey as best as possible. And lions are ferocious. Although they’re one of the most powerful predators on land, lions are in danger. Hunters and poachers target lions to prove to the world their machismo.\\n\\nAnd while hunters seek to wipe lions off the face of the earth to bolster their egos, the Kevin Richardson Wildlife Sanctuary hopes to stop them and protect the big African cat at all cost.\\n\\nRichardson has earned the nickname the “Lion Whisperer” for a reason. He aims to educate the world about lions. And for those lucky enough to volunteer alongside Richardson, he encourages them to learn more about lions and help protect the wild species.\\n\\n“To raise awareness, Kevin has now set up his YouTube Channel ‘LionWhispererTV’. The channel is all about raising awareness about not only the declining numbers of lions but also how this rapid decrease is happening. By watching these videos, you are directly contributing to our scheme of land acquisition,” he writes in his bio.\\n\\nAs part of the volunteer program, Richardson hosts a “volunteer enrichment and lion enrichment” walk. As the name suggests, Richardson takes his group of volunteers out into the savannah of South Africa to hang out with two lions. There, the volunteers meet a male lion, Bobcat, and a female lioness, Gabby. Both lions look ferocious, but are truly “affectionate,” at least that’s what Richardson says. And remember, he’s the lion whisperer, so he’s got an advantage with these deadly big cats.\\n\\nAs Richardson showers the pair of lions with love, the volunteers stay locked in the truck, unwilling to put their lives in danger. And while they are in the vehicle, the lions are just feet from them – and if something goes wrong, they could wind up injured anyway.\\n\\nRichardson shared the video on his “The Lion Whisperer” YouTube channel. With more than one million hits, this video has proven to be one of his most famous.\\n\\nThe video describes the moment caught on tape as follows:\\n\\n“It’s an enrichment walk for both the volunteers and the lions as Kevin shows off his lovely lions as well as giving some amazing lion facts to the volunteers.”\\n\\nViewers like you are overwhelmed with the magnificent footage. The following are a few comments shared on the video.\\n\\n“I hope to someday volunteer there with Kevin. I believe in the work and his perspective about conservation. This video makes me want to all the more! Bobcat and Gabby are lovely lions.” “Every time I watch a one of your videos I somehow end up smiling from ear to ear!” “That was so beautiful, wish I could rub my head against a lion.”\\n\\nTake a moment to watch this video. Would you ever want to volunteer with Kevin Richardson and his lions?\"\"\"\n",
        "    corpus2 = tkn.createCorpus(text2, remove_stopwords=False)\n",
        "    print(\"Corpus 2:\", corpus2)\n",
        "\n",
        "    # Generate embeddings from Corpus 2\n",
        "    we = WordEmbeddings(corpus2)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(\"\\nDocuments shape:\", np.array(docs, dtype=object).shape)\n",
        "    print(\"Documents:\", docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec shape:\", w2v.shape)\n",
        "    print(\"Word2Vec embeddings:\", w2v[:5])  # Print first 5 for brevity\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText shape:\", w2f.shape)\n",
        "    print(\"FastText embeddings:\", w2f[:5])\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe shape:\", w2g.shape)\n",
        "    print(\"GloVe embeddings:\", w2g[:5])\n",
        "\n",
        "    print(\"\\nComparison for word 'lion' (ID varies):\")\n",
        "    lion_id = we.word2id.get('lion', -1)\n",
        "    if lion_id != -1:\n",
        "        print(\"Word2Vec:\", w2v[lion_id])\n",
        "        print(\"FastText:\", w2f[lion_id])\n",
        "        print(\"GloVe:\", w2g[lion_id])\n",
        "    else:\n",
        "        print(\"'lion' not found in vocabulary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAIG9UunzNiF",
        "outputId": "71cf2f66-15e8-4815-bf95-cdb320b2998a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus 1: [['apple', 'data', 'intensive', 'buy', 'startup', '1', 'billion'], ['great'], ['model', 'funcitonal', 'ready']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus 2: [['the', 'lion', 'may', 'be', 'know', 'as', 'the', 'king', 'of', 'the', 'jungle', 'but', 'lion', 'do', 'notlive', 'in', 'jungle'], ['they', 'be', 'the', 'ruler', 'of', 'the', 'african', 'savannah', 'that', 'be', 'cover', 'in', 'brown', 'grass', 'and', 'speckle', 'with', 'sparse', 'tree'], ['lion', 'coloring', 'help', 'they', 'blend', 'in', 'perfectly', 'with', 'the', 'tall', 'grass', 'so', 'they', 'can', 'ambush', 'their', 'prey', 'as', 'well', 'as', 'possible'], ['and', 'lion', 'be', 'ferocious'], ['although', 'they', 'be', 'one', 'of', 'the', 'most', 'powerful', 'predator', 'on', 'land', 'lion', 'be', 'in', 'danger'], ['hunter', 'and', 'poacher', 'target', 'lion', 'to', 'prove', 'to', 'the', 'world', 'their', 'machismo'], ['and', 'while', 'hunter', 'seek', 'to', 'wipe', 'lion', 'off', 'the', 'face', 'of', 'the', 'earth', 'to', 'bolster', 'their', 'ego', 'the', 'kevin', 'richardson', 'wildlife', 'sanctuary', 'hope', 'to', 'stop', 'they', 'and', 'protect', 'the', 'big', 'african', 'cat', 'at', 'all', 'cost'], ['richardson', 'have', 'earn', 'the', 'nickname', 'the', 'lion', 'whisperer', 'for', 'a', 'reason'], ['he', 'aim', 'to', 'educate', 'the', 'world', 'about', 'lion'], ['and', 'for', 'those', 'lucky', 'enough', 'to', 'volunteer', 'alongside', 'richardson', 'he', 'encourage', 'they', 'to', 'learn', 'more', 'about', 'lion', 'and', 'help', 'protect', 'the', 'wild', 'specie'], ['to', 'raise', 'awareness', 'kevin', 'have', 'now', 'set', 'up', 'his', 'youtube', 'channel', 'lionwhisperertv'], ['the', 'channel', 'be', 'all', 'about', 'raise', 'awareness', 'about', 'notonly', 'the', 'decline', 'number', 'of', 'lion', 'but', 'also', 'how', 'this', 'rapid', 'decrease', 'be', 'happen'], ['by', 'watch', 'these', 'video', 'you', 'be', 'directly', 'contribute', 'to', 'our', 'scheme', 'of', 'land', 'acquisition', 'he', 'write', 'in', 'his', 'bio'], ['as', 'part', 'of', 'the', 'volunteer', 'program', 'richardson', 'host', 'a', 'volunteer', 'enrichment', 'and', 'lion', 'enrichment', 'walk'], ['as', 'the', 'name', 'suggest', 'richardson', 'take', 'his', 'group', 'of', 'volunteer', 'out', 'into', 'the', 'savannah', 'of', 'south', 'africa', 'to', 'hang', 'out', 'with', 'two', 'lion'], ['there', 'the', 'volunteer', 'meet', 'a', 'male', 'lion', 'bobcat', 'and', 'a', 'female', 'lioness', 'gabby'], ['both', 'lion', 'look', 'ferocious', 'but', 'be', 'truly', 'affectionate', 'at', 'least', 'that', 'be', 'what', 'richardson', 'say'], ['and', 'remember', 'he', 'be', 'the', 'lion', 'whisperer', 'so', 'he', 'be', 'get', 'an', 'advantage', 'with', 'these', 'deadly', 'big', 'cat'], ['as', 'richardson', 'shower', 'the', 'pair', 'of', 'lion', 'with', 'love', 'the', 'volunteer', 'stay', 'locked', 'in', 'the', 'truck', 'unwilling', 'to', 'put', 'their', 'life', 'in', 'danger'], ['and', 'while', 'they', 'be', 'in', 'the', 'vehicle', 'the', 'lion', 'be', 'just', 'foot', 'from', 'they', 'and', 'if', 'something', 'go', 'wrong', 'they', 'could', 'wind', 'up', 'injure', 'anyway'], ['richardson', 'share', 'the', 'video', 'on', 'his', 'the', 'lion', 'whisperer', 'youtube', 'channel'], ['with', 'more', 'than', 'one', 'million', 'hit', 'this', 'video', 'have', 'prove', 'to', 'be', 'one', 'of', 'his', 'most', 'famous'], ['the', 'video', 'describe', 'the', 'moment', 'catch', 'on', 'tape', 'as', 'follow', 'it', 'be', 'an', 'enrichment', 'walk', 'for', 'both', 'the', 'volunteer', 'and', 'the', 'lion', 'as', 'kevin', 'show', 'off', 'his', 'lovely', 'lion', 'as', 'well', 'as', 'give', 'some', 'amazing', 'lion', 'fact', 'to', 'the', 'volunteer'], ['viewer', 'like', 'you', 'be', 'overwhelmed', 'with', 'the', 'magnificent', 'footage'], ['the', 'follow', 'be', 'a', 'few', 'comment', 'share', 'on', 'the', 'video'], ['i', 'hope', 'to', 'someday', 'volunteer', 'there', 'with', 'kevin'], ['i', 'believe', 'in', 'the', 'work', 'and', 'his', 'perspective', 'about', 'conservation'], ['this', 'video', 'make', 'i', 'want', 'to', 'all', 'the', 'more'], ['bobcat', 'and', 'gabby', 'be', 'lovely', 'lion'], ['every', 'time', 'i', 'watch', 'a', 'one', 'of', 'your', 'video', 'i', 'somehow', 'end', 'up', 'smile', 'from', 'ear', 'to', 'ear'], ['that', 'be', 'so', 'beautiful', 'wish', 'i', 'could', 'rub', 'my', 'head', 'against', 'a', 'lion'], ['take', 'a', 'moment', 'to', 'watch', 'this', 'video'], ['would', 'you', 'ever', 'want', 'to', 'volunteer', 'with', 'kevin', 'richardson', 'and', 'his', 'lion']]\n",
            "\n",
            "Documents shape: (33,)\n",
            "Documents: [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 3, 12, 7, 6, 13, 9, 14, 1, 2, 3, 12, 5, 7, 15, 6, 16, 1, 2, 3, 17, 18, 7, 15, 4, 3, 11, 18, 1, 4, 5, 6, 7, 19, 6, 7, 6, 1, 4, 5, 20, 3, 5, 7, 17, 18, 7, 15, 4, 3], [1, 2, 3, 10, 11, 3, 1, 2, 3, 21, 18, 4, 3, 21, 6, 16, 1, 2, 3, 9, 16, 21, 5, 22, 9, 7, 14, 9, 20, 9, 7, 7, 9, 2, 1, 2, 9, 1, 11, 3, 22, 6, 20, 3, 21, 5, 7, 11, 21, 6, 13, 7, 15, 21, 9, 14, 14, 9, 7, 19, 14, 23, 3, 22, 12, 4, 3, 13, 5, 1, 2, 14, 23, 9, 21, 14, 3, 1, 21, 3, 3], [4, 5, 6, 7, 22, 6, 4, 6, 21, 5, 7, 15, 2, 3, 4, 23, 1, 2, 3, 10, 11, 4, 3, 7, 19, 5, 7, 23, 3, 21, 16, 3, 22, 1, 4, 10, 13, 5, 1, 2, 1, 2, 3, 1, 9, 4, 4, 15, 21, 9, 14, 14, 14, 6, 1, 2, 3, 10, 22, 9, 7, 9, 8, 11, 18, 14, 2, 1, 2, 3, 5, 21, 23, 21, 3, 10, 9, 14, 13, 3, 4, 4, 9, 14, 23, 6, 14, 14, 5, 11, 4, 3], [9, 7, 19, 4, 5, 6, 7, 11, 3, 16, 3, 21, 6, 22, 5, 6, 18, 14], [9, 4, 1, 2, 6, 18, 15, 2, 1, 2, 3, 10, 11, 3, 6, 7, 3, 6, 16, 1, 2, 3, 8, 6, 14, 1, 23, 6, 13, 3, 21, 16, 18, 4, 23, 21, 3, 19, 9, 1, 6, 21, 6, 7, 4, 9, 7, 19, 4, 5, 6, 7, 11, 3, 5, 7, 19, 9, 7, 15, 3, 21], [2, 18, 7, 1, 3, 21, 9, 7, 19, 23, 6, 9, 22, 2, 3, 21, 1, 9, 21, 15, 3, 1, 4, 5, 6, 7, 1, 6, 23, 21, 6, 20, 3, 1, 6, 1, 2, 3, 13, 6, 21, 4, 19, 1, 2, 3, 5, 21, 8, 9, 22, 2, 5, 14, 8, 6], [9, 7, 19, 13, 2, 5, 4, 3, 2, 18, 7, 1, 3, 21, 14, 3, 3, 12, 1, 6, 13, 5, 23, 3, 4, 5, 6, 7, 6, 16, 16, 1, 2, 3, 16, 9, 22, 3, 6, 16, 1, 2, 3, 3, 9, 21, 1, 2, 1, 6, 11, 6, 4, 14, 1, 3, 21, 1, 2, 3, 5, 21, 3, 15, 6, 1, 2, 3, 12, 3, 20, 5, 7, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 13, 5, 4, 19, 4, 5, 16, 3, 14, 9, 7, 22, 1, 18, 9, 21, 10, 2, 6, 23, 3, 1, 6, 14, 1, 6, 23, 1, 2, 3, 10, 9, 7, 19, 23, 21, 6, 1, 3, 22, 1, 1, 2, 3, 11, 5, 15, 9, 16, 21, 5, 22, 9, 7, 22, 9, 1, 9, 1, 9, 4, 4, 22, 6, 14, 1], [21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 2, 9, 20, 3, 3, 9, 21, 7, 1, 2, 3, 7, 5, 22, 12, 7, 9, 8, 3, 1, 2, 3, 4, 5, 6, 7, 13, 2, 5, 14, 23, 3, 21, 3, 21, 16, 6, 21, 9, 21, 3, 9, 14, 6, 7], [2, 3, 9, 5, 8, 1, 6, 3, 19, 18, 22, 9, 1, 3, 1, 2, 3, 13, 6, 21, 4, 19, 9, 11, 6, 18, 1, 4, 5, 6, 7], [9, 7, 19, 16, 6, 21, 1, 2, 6, 14, 3, 4, 18, 22, 12, 10, 3, 7, 6, 18, 15, 2, 1, 6, 20, 6, 4, 18, 7, 1, 3, 3, 21, 9, 4, 6, 7, 15, 14, 5, 19, 3, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 2, 3, 3, 7, 22, 6, 18, 21, 9, 15, 3, 1, 2, 3, 10, 1, 6, 4, 3, 9, 21, 7, 8, 6, 21, 3, 9, 11, 6, 18, 1, 4, 5, 6, 7, 9, 7, 19, 2, 3, 4, 23, 23, 21, 6, 1, 3, 22, 1, 1, 2, 3, 13, 5, 4, 19, 14, 23, 3, 22, 5, 3], [1, 6, 21, 9, 5, 14, 3, 9, 13, 9, 21, 3, 7, 3, 14, 14, 12, 3, 20, 5, 7, 2, 9, 20, 3, 7, 6, 13, 14, 3, 1, 18, 23, 2, 5, 14, 10, 6, 18, 1, 18, 11, 3, 22, 2, 9, 7, 7, 3, 4, 4, 5, 6, 7, 13, 2, 5, 14, 23, 3, 21, 3, 21, 1, 20], [1, 2, 3, 22, 2, 9, 7, 7, 3, 4, 11, 3, 9, 4, 4, 9, 11, 6, 18, 1, 21, 9, 5, 14, 3, 9, 13, 9, 21, 3, 7, 3, 14, 14, 9, 11, 6, 18, 1, 7, 6, 1, 6, 7, 4, 10, 1, 2, 3, 19, 3, 22, 4, 5, 7, 3, 7, 18, 8, 11, 3, 21, 6, 16, 4, 5, 6, 7, 11, 18, 1, 9, 4, 14, 6, 2, 6, 13, 1, 2, 5, 14, 21, 9, 23, 5, 19, 19, 3, 22, 21, 3, 9, 14, 3, 11, 3, 2, 9, 23, 23, 3, 7], [11, 10, 13, 9, 1, 22, 2, 1, 2, 3, 14, 3, 20, 5, 19, 3, 6, 10, 6, 18, 11, 3, 19, 5, 21, 3, 22, 1, 4, 10, 22, 6, 7, 1, 21, 5, 11, 18, 1, 3, 1, 6, 6, 18, 21, 14, 22, 2, 3, 8, 3, 6, 16, 4, 9, 7, 19, 9, 22, 24, 18, 5, 14, 5, 1, 5, 6, 7, 2, 3, 13, 21, 5, 1, 3, 5, 7, 2, 5, 14, 11, 5, 6], [9, 14, 23, 9, 21, 1, 6, 16, 1, 2, 3, 20, 6, 4, 18, 7, 1, 3, 3, 21, 23, 21, 6, 15, 21, 9, 8, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 2, 6, 14, 1, 9, 20, 6, 4, 18, 7, 1, 3, 3, 21, 3, 7, 21, 5, 22, 2, 8, 3, 7, 1, 9, 7, 19, 4, 5, 6, 7, 3, 7, 21, 5, 22, 2, 8, 3, 7, 1, 13, 9, 4, 12], [9, 14, 1, 2, 3, 7, 9, 8, 3, 14, 18, 15, 15, 3, 14, 1, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 1, 9, 12, 3, 2, 5, 14, 15, 21, 6, 18, 23, 6, 16, 20, 6, 4, 18, 7, 1, 3, 3, 21, 6, 18, 1, 5, 7, 1, 6, 1, 2, 3, 14, 9, 20, 9, 7, 7, 9, 2, 6, 16, 14, 6, 18, 1, 2, 9, 16, 21, 5, 22, 9, 1, 6, 2, 9, 7, 15, 6, 18, 1, 13, 5, 1, 2, 1, 13, 6, 4, 5, 6, 7], [1, 2, 3, 21, 3, 1, 2, 3, 20, 6, 4, 18, 7, 1, 3, 3, 21, 8, 3, 3, 1, 9, 8, 9, 4, 3, 4, 5, 6, 7, 11, 6, 11, 22, 9, 1, 9, 7, 19, 9, 16, 3, 8, 9, 4, 3, 4, 5, 6, 7, 3, 14, 14, 15, 9, 11, 11, 10], [11, 6, 1, 2, 4, 5, 6, 7, 4, 6, 6, 12, 16, 3, 21, 6, 22, 5, 6, 18, 14, 11, 18, 1, 11, 3, 1, 21, 18, 4, 10, 9, 16, 16, 3, 22, 1, 5, 6, 7, 9, 1, 3, 9, 1, 4, 3, 9, 14, 1, 1, 2, 9, 1, 11, 3, 13, 2, 9, 1, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 14, 9, 10], [9, 7, 19, 21, 3, 8, 3, 8, 11, 3, 21, 2, 3, 11, 3, 1, 2, 3, 4, 5, 6, 7, 13, 2, 5, 14, 23, 3, 21, 3, 21, 14, 6, 2, 3, 11, 3, 15, 3, 1, 9, 7, 9, 19, 20, 9, 7, 1, 9, 15, 3, 13, 5, 1, 2, 1, 2, 3, 14, 3, 19, 3, 9, 19, 4, 10, 11, 5, 15, 22, 9, 1], [9, 14, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 14, 2, 6, 13, 3, 21, 1, 2, 3, 23, 9, 5, 21, 6, 16, 4, 5, 6, 7, 13, 5, 1, 2, 4, 6, 20, 3, 1, 2, 3, 20, 6, 4, 18, 7, 1, 3, 3, 21, 14, 1, 9, 10, 4, 6, 22, 12, 3, 19, 5, 7, 1, 2, 3, 1, 21, 18, 22, 12, 18, 7, 13, 5, 4, 4, 5, 7, 15, 1, 6, 23, 18, 1, 1, 2, 3, 5, 21, 4, 5, 16, 3, 5, 7, 19, 9, 7, 15, 3, 21], [9, 7, 19, 13, 2, 5, 4, 3, 1, 2, 3, 10, 11, 3, 5, 7, 1, 2, 3, 20, 3, 2, 5, 22, 4, 3, 1, 2, 3, 4, 5, 6, 7, 11, 3, 17, 18, 14, 1, 16, 6, 6, 1, 16, 21, 6, 8, 1, 2, 3, 10, 9, 7, 19, 5, 16, 14, 6, 8, 3, 1, 2, 5, 7, 15, 15, 6, 13, 21, 6, 7, 15, 1, 2, 3, 10, 22, 6, 18, 4, 19, 13, 5, 7, 19, 18, 23, 5, 7, 17, 18, 21, 3, 9, 7, 10, 13, 9, 10], [21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 14, 2, 9, 21, 3, 1, 2, 3, 20, 5, 19, 3, 6, 6, 7, 2, 5, 14, 1, 2, 3, 4, 5, 6, 7, 13, 2, 5, 14, 23, 3, 21, 3, 21, 10, 6, 18, 1, 18, 11, 3, 22, 2, 9, 7, 7, 3, 4], [13, 5, 1, 2, 8, 6, 21, 3, 1, 2, 9, 7, 6, 7, 3, 8, 5, 4, 4, 5, 6, 7, 2, 5, 1, 1, 2, 5, 14, 20, 5, 19, 3, 6, 2, 9, 20, 3, 23, 21, 6, 20, 3, 1, 6, 11, 3, 6, 7, 3, 6, 16, 2, 5, 14, 8, 6, 14, 1, 16, 9, 8, 6, 18, 14], [1, 2, 3, 20, 5, 19, 3, 6, 19, 3, 14, 22, 21, 5, 11, 3, 1, 2, 3, 8, 6, 8, 3, 7, 1, 22, 9, 1, 22, 2, 6, 7, 1, 9, 23, 3, 9, 14, 16, 6, 4, 4, 6, 13, 5, 1, 11, 3, 9, 7, 3, 7, 21, 5, 22, 2, 8, 3, 7, 1, 13, 9, 4, 12, 16, 6, 21, 11, 6, 1, 2, 1, 2, 3, 20, 6, 4, 18, 7, 1, 3, 3, 21, 9, 7, 19, 1, 2, 3, 4, 5, 6, 7, 9, 14, 12, 3, 20, 5, 7, 14, 2, 6, 13, 6, 16, 16, 2, 5, 14, 4, 6, 20, 3, 4, 10, 4, 5, 6, 7, 9, 14, 13, 3, 4, 4, 9, 14, 15, 5, 20, 3, 14, 6, 8, 3, 9, 8, 9, 25, 5, 7, 15, 4, 5, 6, 7, 16, 9, 22, 1, 1, 6, 1, 2, 3, 20, 6, 4, 18, 7, 1, 3, 3, 21], [20, 5, 3, 13, 3, 21, 4, 5, 12, 3, 10, 6, 18, 11, 3, 6, 20, 3, 21, 13, 2, 3, 4, 8, 3, 19, 13, 5, 1, 2, 1, 2, 3, 8, 9, 15, 7, 5, 16, 5, 22, 3, 7, 1, 16, 6, 6, 1, 9, 15, 3], [1, 2, 3, 16, 6, 4, 4, 6, 13, 11, 3, 9, 16, 3, 13, 22, 6, 8, 8, 3, 7, 1, 14, 2, 9, 21, 3, 6, 7, 1, 2, 3, 20, 5, 19, 3, 6], [5, 2, 6, 23, 3, 1, 6, 14, 6, 8, 3, 19, 9, 10, 20, 6, 4, 18, 7, 1, 3, 3, 21, 1, 2, 3, 21, 3, 13, 5, 1, 2, 12, 3, 20, 5, 7], [5, 11, 3, 4, 5, 3, 20, 3, 5, 7, 1, 2, 3, 13, 6, 21, 12, 9, 7, 19, 2, 5, 14, 23, 3, 21, 14, 23, 3, 22, 1, 5, 20, 3, 9, 11, 6, 18, 1, 22, 6, 7, 14, 3, 21, 20, 9, 1, 5, 6, 7], [1, 2, 5, 14, 20, 5, 19, 3, 6, 8, 9, 12, 3, 5, 13, 9, 7, 1, 1, 6, 9, 4, 4, 1, 2, 3, 8, 6, 21, 3], [11, 6, 11, 22, 9, 1, 9, 7, 19, 15, 9, 11, 11, 10, 11, 3, 4, 6, 20, 3, 4, 10, 4, 5, 6, 7], [3, 20, 3, 21, 10, 1, 5, 8, 3, 5, 13, 9, 1, 22, 2, 9, 6, 7, 3, 6, 16, 10, 6, 18, 21, 20, 5, 19, 3, 6, 5, 14, 6, 8, 3, 2, 6, 13, 3, 7, 19, 18, 23, 14, 8, 5, 4, 3, 16, 21, 6, 8, 3, 9, 21, 1, 6, 3, 9, 21], [1, 2, 9, 1, 11, 3, 14, 6, 11, 3, 9, 18, 1, 5, 16, 18, 4, 13, 5, 14, 2, 5, 22, 6, 18, 4, 19, 21, 18, 11, 8, 10, 2, 3, 9, 19, 9, 15, 9, 5, 7, 14, 1, 9, 4, 5, 6, 7], [1, 9, 12, 3, 9, 8, 6, 8, 3, 7, 1, 1, 6, 13, 9, 1, 22, 2, 1, 2, 5, 14, 20, 5, 19, 3, 6], [13, 6, 18, 4, 19, 10, 6, 18, 3, 20, 3, 21, 13, 9, 7, 1, 1, 6, 20, 6, 4, 18, 7, 1, 3, 3, 21, 13, 5, 1, 2, 12, 3, 20, 5, 7, 21, 5, 22, 2, 9, 21, 19, 14, 6, 7, 9, 7, 19, 2, 5, 14, 4, 5, 6, 7]]\n",
            "Word2Vec shape: (26, 128)\n",
            "Word2Vec embeddings: [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.        ]\n",
            " [ 0.01499402 -0.01157733  0.17544267  0.02072983 -0.01279209 -0.22414222\n",
            "   0.02005932  0.05753706 -0.01944208 -0.01496397  0.26419944 -0.05098562\n",
            "   0.00068617 -0.10155127 -0.0083288  -0.10500033 -0.02105615  0.12449843\n",
            "  -0.09189223  0.02844688  0.17492576  0.15495333  0.1234379   0.01979536\n",
            "  -0.08433766  0.14595085 -0.09517133  0.08291405  0.01341272 -0.07428417\n",
            "  -0.32279915  0.03707847 -0.01850857 -0.00256643 -0.03621771  0.03751522\n",
            "   0.25613242  0.15841964  0.22752108 -0.06902725 -0.07913905  0.14114203\n",
            "  -0.07540777 -0.05079257  0.10607625  0.11041861 -0.02181652 -0.07784761\n",
            "   0.00213869  0.14830884 -0.00059169  0.08966215  0.1156004   0.12130155\n",
            "   0.16061838  0.0336818   0.13044322 -0.10215595 -0.09064406 -0.02845797\n",
            "   0.12307254 -0.12313317  0.0758945  -0.05884093  0.09977613 -0.08700339\n",
            "   0.11170813  0.10546141  0.05461083 -0.0199768   0.08025031  0.06022271\n",
            "  -0.0984007  -0.07962459 -0.06485014  0.02855733  0.04233344  0.09179606\n",
            "  -0.03789996  0.1922449  -0.02950055 -0.05775835  0.07863863  0.20354739\n",
            "  -0.01033415  0.03245977  0.21939985 -0.12307432  0.09601112  0.12998174\n",
            "  -0.07754666 -0.07079767 -0.14696963 -0.00380163  0.17493714  0.00878621\n",
            "  -0.27671295 -0.2373458   0.05133488 -0.07435016 -0.27829465 -0.02052513\n",
            "   0.15102841  0.07837185  0.01408205 -0.06637745  0.08462285 -0.14705178\n",
            "  -0.0670146  -0.11807993  0.01047196 -0.04148376 -0.00957589  0.0374644\n",
            "  -0.05080111 -0.02199528 -0.03624016  0.18639028  0.03645542  0.12346306\n",
            "  -0.0484759  -0.02065499  0.05408379 -0.04601029  0.0070316  -0.16495083\n",
            "  -0.09677251  0.11445434]\n",
            " [ 0.01120927 -0.01229759  0.14619313  0.01820905 -0.00984941 -0.17653432\n",
            "   0.00625947  0.0445849  -0.00302503 -0.02235435  0.20662643 -0.04820666\n",
            "  -0.0003899  -0.08616165 -0.01862833 -0.08342921 -0.01871815  0.09652351\n",
            "  -0.06410894  0.02295668  0.138972    0.11943815  0.1020916   0.01117401\n",
            "  -0.07332649  0.12683004 -0.07058313  0.06515531  0.00873678 -0.0585236\n",
            "  -0.24777117  0.03044221 -0.02774958  0.00622973 -0.02271798  0.03362234\n",
            "   0.20179845  0.12956372  0.18266998 -0.0628015  -0.07178313  0.11402784\n",
            "  -0.06304166 -0.04129346  0.0980479   0.07707357 -0.0087734  -0.06073997\n",
            "  -0.00864474  0.11786789 -0.00688951  0.06209255  0.08560757  0.11011614\n",
            "   0.12724286  0.01544299  0.10301307 -0.07654072 -0.07999831 -0.02951059\n",
            "   0.09318781 -0.10079204  0.07130464 -0.05790024  0.08042991 -0.05926042\n",
            "   0.08084351  0.08263571  0.04880581 -0.01118087  0.06234659  0.0372063\n",
            "  -0.08074088 -0.05885597 -0.05293849  0.02506431  0.03634458  0.07233185\n",
            "  -0.03139332  0.14835022 -0.03495574 -0.04410892  0.06820326  0.15439025\n",
            "  -0.01060585  0.01798056  0.17270879 -0.09880178  0.07301574  0.1021336\n",
            "  -0.05871237 -0.05843655 -0.11893436 -0.00554401  0.14368182  0.00530237\n",
            "  -0.21737449 -0.18138793  0.04190405 -0.05320113 -0.22867918 -0.01915473\n",
            "   0.10776629  0.06341786  0.01441992 -0.05051031  0.05467576 -0.11880143\n",
            "  -0.06228844 -0.09512389  0.0007074  -0.04084899 -0.00740577  0.03142025\n",
            "  -0.03817569 -0.01392264 -0.02176478  0.14214489  0.03033371  0.09394103\n",
            "  -0.03704738 -0.01754271  0.04157491 -0.03768833  0.01093687 -0.13729051\n",
            "  -0.07281277  0.09601191]\n",
            " [ 0.01730257 -0.00953603  0.21957414  0.03580673 -0.02896699 -0.26889575\n",
            "   0.02110887  0.07295913 -0.0222119  -0.0304713   0.31444907 -0.06436796\n",
            "   0.00233396 -0.121361   -0.01844882 -0.13131161 -0.02625451  0.14783923\n",
            "  -0.11116016  0.01567253  0.2039125   0.18597323  0.1554957   0.02267443\n",
            "  -0.0950366   0.17961089 -0.10558522  0.10365453  0.00747619 -0.08672766\n",
            "  -0.38545167  0.04988506 -0.02197753 -0.00878327 -0.0406637   0.03750611\n",
            "   0.30338243  0.18681757  0.26237568 -0.09238958 -0.10685302  0.17111628\n",
            "  -0.105108   -0.05755792  0.13442914  0.12161613 -0.03288093 -0.08361109\n",
            "   0.00141274  0.18726119 -0.00868135  0.10198653  0.12616743  0.15383993\n",
            "   0.18938276  0.02897169  0.15365106 -0.12337323 -0.11901077 -0.02392855\n",
            "   0.13783234 -0.14201102  0.09482872 -0.08462695  0.1131926  -0.09839271\n",
            "   0.12155097  0.12312762  0.07103656 -0.02572436  0.09411974  0.07343425\n",
            "  -0.11980262 -0.1030589  -0.07468288  0.0403131   0.06259272  0.10857429\n",
            "  -0.05095596  0.21047047 -0.04627442 -0.05926947  0.10722244  0.23876509\n",
            "  -0.02149254  0.03391412  0.26014698 -0.14812325  0.10499465  0.15755513\n",
            "  -0.09484807 -0.08019046 -0.16803972 -0.00205919  0.21280335  0.01558243\n",
            "  -0.32513154 -0.27744862  0.06896555 -0.08233567 -0.33947471 -0.02970529\n",
            "   0.18049055  0.10088883  0.02223437 -0.08937211  0.09491598 -0.16674116\n",
            "  -0.08689647 -0.14038013  0.01296616 -0.06396686 -0.01390059  0.05992101\n",
            "  -0.06197552 -0.01514385 -0.03496974  0.21849996  0.04159915  0.14959928\n",
            "  -0.05028775 -0.03420506  0.07657877 -0.06605278  0.0185842  -0.20176147\n",
            "  -0.12363981  0.13757817]\n",
            " [ 0.01160929 -0.00571138  0.19115035  0.02154689 -0.01153172 -0.23134071\n",
            "   0.0104657   0.05938489 -0.00768537 -0.01794801  0.27871984 -0.05453704\n",
            "   0.00754639 -0.11534719 -0.01852744 -0.11607613 -0.02726089  0.13066071\n",
            "  -0.09849925  0.02861637  0.17301539  0.15534428  0.1320194   0.01913837\n",
            "  -0.09695223  0.16313168 -0.09817943  0.08676554  0.00783102 -0.07279933\n",
            "  -0.33661956  0.04538644 -0.03189864 -0.00351229 -0.02848195  0.03881381\n",
            "   0.25633109  0.169254    0.23618399 -0.0765278  -0.09600569  0.15249002\n",
            "  -0.09479634 -0.05376277  0.12323415  0.10634656 -0.01959261 -0.07507385\n",
            "  -0.00617921  0.16814321 -0.00139284  0.09605507  0.12198368  0.12899549\n",
            "   0.1598122   0.03550463  0.13918534 -0.09671399 -0.09806382 -0.02321306\n",
            "   0.11616217 -0.12290662  0.08495273 -0.0665621   0.10799092 -0.08686738\n",
            "   0.11613295  0.10651687  0.0651307  -0.01926764  0.07544614  0.05948778\n",
            "  -0.09853823 -0.08870437 -0.06749688  0.02787233  0.05734394  0.10453244\n",
            "  -0.04215761  0.20043699 -0.03848394 -0.06149787  0.09026626  0.21355872\n",
            "  -0.01692095  0.02583307  0.231975   -0.12670907  0.09550221  0.13244164\n",
            "  -0.07956137 -0.07756685 -0.15527655 -0.00701684  0.17900249  0.00271225\n",
            "  -0.2881248  -0.24991532  0.06076964 -0.0861194  -0.30475989 -0.03581985\n",
            "   0.14989203  0.0913241   0.01765605 -0.07481111  0.08090401 -0.16109481\n",
            "  -0.07503198 -0.12031668  0.01088947 -0.05096997 -0.00738508  0.04757883\n",
            "  -0.06537563 -0.02475809 -0.04235476  0.18904702  0.04237592  0.14104275\n",
            "  -0.04873987 -0.02882081  0.06889497 -0.05037365  0.00941983 -0.1746801\n",
            "  -0.10425587  0.11571732]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.1536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText shape: (26, 128)\n",
            "FastText embeddings: [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.        ]\n",
            " [-0.03521762 -0.03479476  0.02845298  0.03818076 -0.10347812 -0.24461722\n",
            "  -0.0398801   0.00829738  0.07086217  0.03210483  0.14250544 -0.05988738\n",
            "   0.043044   -0.21647644  0.12729996 -0.12470272 -0.01031252  0.17025226\n",
            "  -0.00640404 -0.01018168  0.1128727   0.24142292  0.07770351  0.01904791\n",
            "  -0.16076791  0.07717168 -0.06782666  0.00315943  0.04416052 -0.20612578\n",
            "  -0.22635052 -0.02949098  0.1662809   0.0055493  -0.07213143  0.08606412\n",
            "   0.27374834 -0.01323915  0.0514768  -0.07055693  0.01257666 -0.11110409\n",
            "   0.01348197 -0.05160948  0.13984257  0.15162733 -0.03907603  0.04631592\n",
            "  -0.07320632  0.08950317  0.14034824  0.02085699  0.09221118  0.02751477\n",
            "   0.05860198 -0.0096325   0.08359747 -0.08662512 -0.02877791  0.10509843\n",
            "   0.01697085 -0.06064966  0.01473941  0.01839161 -0.04832568 -0.03552043\n",
            "  -0.01407829 -0.08453895  0.02666492 -0.0491309   0.05176568 -0.03498224\n",
            "  -0.04261473 -0.05162619 -0.21276352  0.05485211  0.05130785  0.32911539\n",
            "  -0.11335763  0.16664857 -0.09866799 -0.00948143  0.13258909  0.22322884\n",
            "  -0.07449373  0.09767507  0.1009602  -0.04988967  0.13869271  0.09700397\n",
            "  -0.06731243  0.03723895 -0.07815526  0.13919882  0.21837261 -0.10972118\n",
            "  -0.14000946 -0.19459903  0.09720778 -0.09949904 -0.07235846  0.04634211\n",
            "   0.12396379  0.2277104  -0.00532926  0.05510124  0.06808233 -0.17318082\n",
            "   0.06054887 -0.07227612  0.038886   -0.07327665  0.09935898 -0.00697105\n",
            "  -0.09061994  0.00271671 -0.13481113  0.12069831 -0.06475468  0.02834334\n",
            "  -0.10791166  0.05857265  0.07574338 -0.10499662  0.15036327 -0.04937436\n",
            "  -0.08445308  0.0817403 ]\n",
            " [-0.02508084 -0.02698006  0.03112009  0.03011565 -0.07832682 -0.18023914\n",
            "  -0.04129011  0.01095258  0.05981293  0.01368426  0.10345447 -0.05337849\n",
            "   0.03306868 -0.16989928  0.08780941 -0.09880283 -0.00947401  0.12439243\n",
            "  -0.00274204 -0.00839408  0.08693956  0.18482663  0.06443585  0.0102375\n",
            "  -0.12505949  0.06143058 -0.04761124 -0.00164278  0.03510197 -0.15347147\n",
            "  -0.17043817 -0.02605797  0.1218322   0.01111209 -0.05053085  0.06736852\n",
            "   0.21189308 -0.01226938  0.04315957 -0.05591968  0.00567977 -0.07998382\n",
            "   0.01055386 -0.04238204  0.1140881   0.11069563 -0.02111855  0.03447913\n",
            "  -0.05761512  0.06635045  0.10533063  0.01022345  0.06586535  0.02375779\n",
            "   0.04372685 -0.01300844  0.06832409 -0.06374122 -0.02778277  0.07677603\n",
            "   0.00904306 -0.04649606  0.01260574  0.00940854 -0.03825759 -0.01513013\n",
            "  -0.00991239 -0.06488805  0.0222335  -0.03208063  0.04097136 -0.03373407\n",
            "  -0.0375471  -0.03273152 -0.16583432  0.04299524  0.03778704  0.25794134\n",
            "  -0.08815387  0.12467932 -0.07930787 -0.00977296  0.09981501  0.1659812\n",
            "  -0.05536789  0.07076938  0.07406545 -0.04176927  0.10832403  0.07659382\n",
            "  -0.04688633  0.02613531 -0.05844467  0.10293429  0.16442236 -0.089151\n",
            "  -0.10703923 -0.14960019  0.07527824 -0.07506679 -0.06096547  0.03586984\n",
            "   0.08481038  0.17213634 -0.00267179  0.04163826  0.04619286 -0.129576\n",
            "   0.04034287 -0.05602095  0.02622984 -0.06027877  0.07537665 -0.01019626\n",
            "  -0.06938562  0.00052021 -0.10295121  0.08561063 -0.04777267  0.01923161\n",
            "  -0.08910578  0.04440987  0.05931528 -0.08307655  0.11638648 -0.0363659\n",
            "  -0.05951105  0.06051779]\n",
            " [-0.04655656 -0.03656323  0.04274834  0.04753626 -0.12464498 -0.28773445\n",
            "  -0.05049339  0.01202018  0.0876396   0.02869234  0.16293544 -0.07412809\n",
            "   0.04951278 -0.25496006  0.14066565 -0.15270078 -0.00770001  0.19730687\n",
            "  -0.00663305 -0.02106366  0.13334194  0.29008567  0.09595728  0.01311407\n",
            "  -0.17934094  0.09001363 -0.07344604  0.00958544  0.04866437 -0.24058101\n",
            "  -0.26808947 -0.03960526  0.19813265  0.00266484 -0.0834408   0.09991624\n",
            "   0.32336247 -0.01856872  0.05616411 -0.0897842   0.00880307 -0.13076314\n",
            "   0.00699178 -0.0655961   0.16270041  0.16893971 -0.04169448  0.06281222\n",
            "  -0.0870932   0.1042368   0.15995634  0.02226448  0.10786603  0.03239812\n",
            "   0.0697051  -0.01656957  0.09910575 -0.10526793 -0.03319596  0.12780821\n",
            "   0.0183264  -0.06956814  0.01761118  0.01842819 -0.06252848 -0.02973371\n",
            "  -0.02388089 -0.09834158  0.03901184 -0.0546715   0.06990026 -0.03863223\n",
            "  -0.05248497 -0.06052505 -0.25141498  0.06796636  0.06177276  0.39370057\n",
            "  -0.1355157   0.18340617 -0.12111765 -0.01345038  0.15782806  0.25730616\n",
            "  -0.08472843  0.10820681  0.11512201 -0.05721503  0.15787014  0.11649606\n",
            "  -0.07602608  0.04320846 -0.08037652  0.16033606  0.25686121 -0.13357183\n",
            "  -0.15968208 -0.2315208   0.11857019 -0.11599316 -0.08619498  0.05326046\n",
            "   0.14067581  0.2709294  -0.00603593  0.06046483  0.07076791 -0.19945243\n",
            "   0.06042035 -0.09200229  0.04589754 -0.09237014  0.11281388 -0.00347989\n",
            "  -0.10848705  0.00650901 -0.15983771  0.13646299 -0.07595762  0.02875447\n",
            "  -0.12898025  0.06098489  0.09168398 -0.12445097  0.18013944 -0.05614671\n",
            "  -0.09908716  0.0895521 ]\n",
            " [-0.04112015 -0.03047254  0.0313697   0.03586952 -0.10725853 -0.24700528\n",
            "  -0.04804906  0.01011048  0.08229772  0.03221561  0.14480232 -0.06198044\n",
            "   0.0417206  -0.23044078  0.12182869 -0.12736483 -0.00945099  0.1749649\n",
            "  -0.00950279 -0.00875824  0.11397955  0.24618788  0.08341399  0.01528031\n",
            "  -0.16785401  0.07594374 -0.06749722  0.00434219  0.04784162 -0.20896153\n",
            "  -0.23637421 -0.03451758  0.16140054  0.00392304 -0.06643859  0.09103919\n",
            "   0.27525634 -0.0155637   0.05456893 -0.07461032  0.00515218 -0.11580438\n",
            "   0.00665349 -0.05940952  0.1490843   0.14566377 -0.03502584  0.05277383\n",
            "  -0.07769541  0.09328726  0.14603046  0.02493846  0.09650721  0.02338309\n",
            "   0.06049962 -0.01301832  0.08422471 -0.0891247  -0.02861424  0.10976239\n",
            "   0.00616095 -0.06161483  0.01389549  0.02167639 -0.05078018 -0.03262016\n",
            "  -0.01773676 -0.08617331  0.03333109 -0.04819953  0.05375933 -0.039965\n",
            "  -0.04554479 -0.0510384  -0.2141806   0.05425691  0.06239812  0.34794042\n",
            "  -0.11362843  0.16292621 -0.10358663 -0.01142     0.1393995   0.22419053\n",
            "  -0.08135039  0.09696622  0.10151511 -0.05317742  0.14671844  0.09616496\n",
            "  -0.06817326  0.03568736 -0.07667695  0.13882065  0.21887714 -0.11704855\n",
            "  -0.13990477 -0.19826615  0.10604647 -0.10693749 -0.0768421   0.04087039\n",
            "   0.12117764  0.23876134 -0.00560124  0.0528237   0.06168098 -0.18255478\n",
            "   0.06139087 -0.07295129  0.04134827 -0.07357964  0.10107006 -0.00846275\n",
            "  -0.0936726   0.00174898 -0.13768417  0.11428876 -0.06826796  0.02786807\n",
            "  -0.11657967  0.05451152  0.08357084 -0.10959513  0.1577367  -0.04522966\n",
            "  -0.08383857  0.07511681]]\n",
            "GloVe shape: (26, 128)\n",
            "GloVe embeddings: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 3.55777558e-01 -2.84589748e-01  1.58124390e-01 -4.49625118e-01\n",
            "  -2.34479698e-01  1.59628614e-01  5.16432431e-01  5.91838298e-01\n",
            "  -5.62957136e-01 -7.60846802e-01  3.55518843e-01 -5.95589271e-01\n",
            "   4.02334099e-01  2.85359598e-01  4.61384199e-01  6.43594317e-01\n",
            "   3.81255708e-01 -4.00065950e-01 -2.35468236e-01  1.97672703e-01\n",
            "   4.59291390e-01  1.89182498e-01 -2.56116137e-01  5.02830929e-01\n",
            "  -5.84163458e-01  6.56442604e-01 -3.37902993e-01  4.11871683e-01\n",
            "  -5.89257421e-01 -4.37873320e-01  4.04892650e-01 -1.60258287e-01\n",
            "   4.72145821e-01  4.34523616e-01 -6.11569651e-01 -4.78280331e-01\n",
            "   9.31152675e-03  4.38140614e-01 -3.31671117e-02 -4.52279117e-01\n",
            "   2.99345406e-02  6.04339217e-01 -2.77076091e-01  2.15272243e-01\n",
            "   4.27422383e-01 -5.78140951e-01  3.66636192e-01  4.16591247e-01\n",
            "   1.53119960e-01 -3.12115871e-02 -2.77278657e-01 -5.43602416e-01\n",
            "  -1.20645189e-01  1.65586769e-01 -5.43155885e-01  3.59758341e-01\n",
            "   2.89476638e-01 -6.66789428e-01  4.65117980e-01  4.53114911e-01\n",
            "  -2.33444105e-01 -2.90890983e-01  3.23240406e-01 -4.58158619e-01\n",
            "  -6.50989450e-01 -2.52847738e-01 -1.88654827e-01 -2.62354539e-01\n",
            "   2.90887808e-01 -3.75630421e-01 -4.34492938e-01  2.43220148e-01\n",
            "  -1.82965690e-01  1.31845248e-01  2.38795264e-01 -2.25624388e-01\n",
            "   2.10490512e-01  3.48143108e-01  4.85092930e-01  2.46152228e-01\n",
            "   4.23731538e-01 -4.49241372e-01  2.87516550e-01 -5.22551148e-01\n",
            "  -6.25565237e-01  2.55355235e-01 -4.62971257e-01  9.80951655e-02\n",
            "   4.90010979e-02 -2.97873717e-01 -3.43475899e-02  2.14127509e-01\n",
            "  -5.86129667e-01  2.42840714e-01  5.44654727e-01 -1.23109605e-01\n",
            "   1.53032023e-01  3.41148454e-01 -2.06755558e-01  3.11695431e-01\n",
            "   1.37015179e-01 -2.84290556e-01  4.18452974e-01 -2.30864897e-01\n",
            "  -3.94364167e-01  5.43645866e-01 -4.01671518e-01 -3.98222476e-01\n",
            "  -4.86448583e-01  1.53084641e-01 -5.76991990e-01  3.79230868e-01\n",
            "   2.32952386e-01 -1.62505718e-01  1.77696129e-02  3.72227819e-01\n",
            "  -2.15148055e-01  5.76752757e-01 -1.56037328e-01  1.47591166e-01\n",
            "  -3.68447771e-01  5.35495403e-01  3.22011585e-01 -3.09173468e-01\n",
            "   3.78559213e-01  2.51170681e-01 -5.10594775e-01  4.80761869e-01]\n",
            " [ 3.92200432e-01  1.20337584e-01  1.47503241e-01 -3.96638862e-01\n",
            "  -5.86273382e-01  2.65812768e-01  2.30785955e-01  1.68064407e-01\n",
            "  -1.35871512e-01 -6.54216432e-01  3.17073459e-01 -6.09706585e-01\n",
            "   6.86866335e-01  1.50871296e-01  5.57001905e-01  3.30065340e-01\n",
            "   2.20623748e-01 -4.26007425e-01 -3.16513823e-01  7.25077835e-02\n",
            "   2.79554848e-01  2.94667952e-01 -3.83329854e-01  4.51668896e-01\n",
            "  -3.17326443e-01  6.12164305e-01 -4.56296239e-01  4.13782653e-01\n",
            "  -2.11258890e-01 -4.11148943e-01  6.08891704e-01 -1.54375124e-01\n",
            "   3.70856237e-01  2.77058487e-01 -2.93206889e-01 -2.61701011e-01\n",
            "   2.21988749e-01  6.39638203e-01 -1.32930985e-01 -2.00458408e-01\n",
            "   6.54820146e-02  5.68924229e-01 -3.67643988e-01  1.59187530e-01\n",
            "   2.26627478e-01 -2.31814238e-01  4.49324589e-01  5.36490345e-01\n",
            "   1.60880821e-01 -2.63744574e-01 -1.58629277e-01 -5.46390818e-01\n",
            "  -2.18392737e-01  2.14996165e-02 -3.46471937e-01  2.97818798e-01\n",
            "   5.14172834e-01 -5.48072149e-01  3.93303614e-01  8.15264552e-02\n",
            "  -3.15566582e-01 -3.17561618e-01 -3.20693366e-02 -3.95241741e-01\n",
            "  -6.03727786e-01 -4.74622229e-01 -3.11519797e-01 -1.53653993e-01\n",
            "   4.05948804e-01 -2.92697466e-01 -4.23708204e-01  3.89497563e-01\n",
            "   3.30945358e-02  3.75674913e-02  1.41471031e-01 -3.83889548e-02\n",
            "   7.55258016e-02  1.03630955e-01  3.20607337e-01  3.84413581e-01\n",
            "   7.10214688e-02 -6.52313879e-01  2.08169069e-01 -2.62713731e-01\n",
            "  -2.03455165e-01 -1.99988573e-03 -5.20702143e-01  3.25139184e-02\n",
            "   1.09068037e-01 -3.89728285e-01 -2.89006938e-01  9.47875157e-02\n",
            "  -1.60149574e-01  2.90538234e-01  3.30457615e-01 -2.95277356e-01\n",
            "   1.90424824e-01  6.76035727e-01 -1.78105149e-01  2.38044374e-01\n",
            "   1.09756202e-01 -4.89890829e-02  3.16846537e-01 -5.52626859e-01\n",
            "  -1.14289001e-01  4.37519445e-01 -4.29883138e-01 -3.22101111e-01\n",
            "  -4.32437694e-01  3.73441993e-01 -1.98127015e-01  7.33904503e-02\n",
            "   4.53739072e-01 -9.40628374e-02  3.46673867e-04  3.39762827e-01\n",
            "  -4.71004949e-01  3.54020001e-01 -1.59789711e-01 -7.52565693e-02\n",
            "  -2.91325872e-01  2.76028875e-01  5.54427154e-01 -5.44131087e-01\n",
            "   5.89532717e-01  1.42864148e-01 -3.56589191e-01  4.01753372e-01]\n",
            " [ 6.09767636e-01 -4.62513110e-01  1.05418476e-01 -4.40765307e-01\n",
            "  -5.03983252e-01  3.90878649e-02  2.65900933e-01  3.50746665e-01\n",
            "  -1.27000373e-01 -5.08762371e-01  6.27310452e-01 -6.39080526e-01\n",
            "   4.55336994e-01  3.54157442e-01  4.48072331e-01  3.56064723e-01\n",
            "   5.34930011e-01 -2.41990180e-01 -5.92210098e-01  2.07202040e-01\n",
            "   2.10611509e-01  5.97374570e-01 -2.35943232e-01  4.78935234e-01\n",
            "  -6.76317361e-01  4.43265104e-01 -5.15868160e-01  5.32476052e-01\n",
            "  -4.66869456e-01 -3.93536133e-01  3.96661196e-01 -7.67587024e-02\n",
            "   3.23803281e-01  2.95689871e-03 -4.94638552e-01 -3.90760580e-01\n",
            "   3.30796248e-01  2.81246780e-01  1.20218915e-01 -5.88926052e-01\n",
            "   2.54554336e-01  6.70032073e-01 -3.14662157e-01  3.02003010e-01\n",
            "   4.02134758e-01 -3.70738891e-01  3.94378433e-01  4.08981216e-01\n",
            "   1.55294105e-01  7.07784002e-02 -4.26281882e-01 -2.48522206e-01\n",
            "  -1.21630297e-01  1.54496462e-01 -3.32595204e-01  1.78569431e-01\n",
            "   6.63698115e-01 -5.70495935e-01  5.48291438e-01  5.61099809e-01\n",
            "  -5.17946132e-01 -3.79010922e-01 -7.21171545e-02 -5.59583220e-01\n",
            "  -2.67814226e-01 -2.73328314e-01 -4.05023828e-01 -3.79911947e-01\n",
            "   3.03005913e-01 -4.32381846e-01 -5.13556651e-01  4.08623846e-01\n",
            "  -4.25441286e-01  3.17006672e-01  2.58259066e-01 -1.73330729e-01\n",
            "   2.12293514e-01  7.01236824e-01  4.57731161e-01  2.25327252e-01\n",
            "   2.97963943e-01 -2.45244086e-01  3.19895199e-01 -5.19710158e-01\n",
            "  -6.38572487e-01  1.54750061e-01 -5.35250956e-01  5.55839668e-01\n",
            "   1.91625832e-01 -6.21360476e-01 -1.91858599e-01  3.23641371e-01\n",
            "  -3.58401821e-01  4.99689700e-01  3.72718797e-01 -1.07616432e-01\n",
            "   1.21491375e-01  7.00066911e-01 -3.08578790e-01 -6.44027816e-02\n",
            "   3.41496648e-01 -3.03912972e-01  5.73609358e-01 -5.79982548e-01\n",
            "  -5.92263758e-01  6.08260228e-01 -5.22335810e-01 -5.52317174e-01\n",
            "  -9.24867921e-02  2.29177887e-01 -7.20477103e-01  3.09414824e-01\n",
            "   3.95137725e-01 -2.82621829e-03 -1.72712746e-01  6.49401528e-01\n",
            "  -2.51842956e-01  4.57860041e-01 -4.39252056e-01  1.37918169e-01\n",
            "  -5.16181787e-01  6.36351255e-01  4.13132933e-01 -6.51511896e-01\n",
            "   6.15645175e-01  3.25138211e-01 -4.29383000e-01  4.99875456e-01]\n",
            " [ 3.41687668e-01 -5.30249747e-01 -1.48585525e-01  8.33030538e-02\n",
            "  -4.50773472e-02 -4.94156264e-02  7.01485892e-01  3.76897144e-01\n",
            "  -1.13887504e-01 -3.65256931e-01  6.15410877e-01 -5.95256862e-01\n",
            "   1.96525815e-02 -2.19391073e-02 -4.42626110e-02  4.05901350e-01\n",
            "   2.07476651e-01 -5.54956525e-01 -5.42120709e-01  1.32095060e-01\n",
            "   1.23413551e-01  4.02419587e-01 -5.66635074e-01  3.46665492e-01\n",
            "  -1.59802408e-01  4.26499336e-01 -6.57365257e-01  4.59910220e-01\n",
            "  -2.28532451e-01 -3.01559799e-01  1.18326755e-01  5.17379297e-01\n",
            "  -1.99944493e-02 -3.02958733e-01 -4.86666739e-01  1.43628526e-01\n",
            "   3.22371154e-02  1.54907649e-02  3.32367052e-01 -3.85683894e-01\n",
            "   3.45037760e-03  6.00166336e-01 -1.07373258e-01  3.48855501e-01\n",
            "   3.98261443e-01 -6.87162179e-02  1.38580960e-02  5.30279159e-01\n",
            "  -1.80623566e-01  3.21445071e-01 -2.30153365e-01 -4.05851339e-01\n",
            "   1.57990625e-01  2.32010906e-01 -4.29507123e-01  5.62536292e-01\n",
            "   4.22157044e-01 -4.05021815e-01  3.96782524e-01  3.30065467e-01\n",
            "  -5.58667436e-01  9.41448328e-02 -5.98173819e-01 -3.22195945e-01\n",
            "  -3.26307593e-01 -4.25134084e-01 -4.46022037e-01 -2.81337666e-01\n",
            "   2.99482830e-01 -2.70697280e-01 -4.44089380e-01  9.25708458e-02\n",
            "  -2.79174745e-01 -5.29489700e-01  3.19034533e-01 -3.68585142e-01\n",
            "   5.17643450e-01  2.69239435e-01  4.94345826e-01  1.36246812e-01\n",
            "   5.15297852e-01  2.10458922e-01  8.57966675e-02 -3.13759922e-01\n",
            "  -5.36196855e-01  1.37347736e-01 -4.91055226e-01  6.29036200e-01\n",
            "   7.21591465e-01 -6.94373577e-01 -1.77810648e-01  1.38968089e-01\n",
            "  -2.80454379e-01  4.17395913e-01  6.21114933e-01 -2.30855028e-01\n",
            "  -7.05660733e-02  2.39871595e-01 -8.99830641e-02  6.89475267e-02\n",
            "   5.17254755e-01 -1.05896593e-01  7.14843112e-01 -4.64085317e-01\n",
            "  -1.76847245e-01  2.27128511e-01 -4.85788031e-01 -8.83310282e-02\n",
            "  -3.04818533e-02  1.12945779e-01 -4.94260245e-01  3.22681587e-01\n",
            "   9.45256811e-02 -9.00047399e-02  2.15934427e-01  3.49882846e-01\n",
            "  -4.96091109e-01  1.35221256e-01 -2.97893156e-02  5.38923019e-01\n",
            "  -2.84851247e-01  4.00269595e-02  7.93487329e-02 -5.42662998e-01\n",
            "   3.32037061e-01  1.59707840e-01 -1.65347768e-02  3.78271592e-01]]\n",
            "\n",
            "Comparison for word 'lion' (ID varies):\n",
            "'lion' not found in vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install numpy==1.26.4 gensim==4.3.3 mittens==0.2 spacy==3.7.2 stop-words==2018.7.23 pandas scipy -q\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import io as sio\n",
        "import numpy as np\n",
        "from multiprocessing import cpu_count\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "\n",
        "# Tokenization class (from earlier)\n",
        "specialchar_dic = {\n",
        " \"’\": \"'\", \"„\": \"\\\"\", \"“\": \"\\\"\", \"”\": \"\\\"\", \"«\": \"<<\", \"»\": \">>\",\n",
        " \"…\": \"...\", \"—\": \"--\", \"¡\": \"!\", \"¿\": \"?\", \"©\": \" \", \"–\": \" \"\n",
        "}\n",
        "punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "specialchar_re = re.compile('(%s)' % '|'.join(specialchar_dic.keys()))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "cachedStopWords_en = stopWordsEN() # Assume stopWordsEN() is defined as before\n",
        "\n",
        "class Tokenization:\n",
        " def applyFE(self, text):\n",
        " final_text = text.replace('cannot', 'can not').replace('can\\'t', 'can not')\n",
        " final_text = final_text.replace('won\\'t', 'will not').replace('n\\'t', ' not').replace(' not ', ' not')\n",
        " return final_text\n",
        "\n",
        " def removeStopWords(self, text):\n",
        " return ' '.join([word for word in text.split() if word not in cachedStopWords_en])\n",
        "\n",
        " def removePunctuation(self, text, punctuation=punctuation):\n",
        " for c in punctuation:\n",
        " text = text.replace(c, ' ')\n",
        " return text\n",
        "\n",
        " def replaceUTF8Char(self, text, specialchars=specialchar_dic):\n",
        " def replace(match):\n",
        " return specialchars[match.group(0)]\n",
        " return specialchar_re.sub(replace, text)\n",
        "\n",
        " def createCorpus(self, text, remove_punctuation=True, remove_stopwords=True, apply_FE=True):\n",
        " corpus = []\n",
        " try:\n",
        " text = self.replaceUTF8Char(text).replace(\"\\n\", \" \")\n",
        " doc = nlp(text)\n",
        " processed_text = ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text if not t.ent_type_ else t.text for t in doc])\n",
        " processed_text = processed_text.replace(\"\\s\\s+\", ' ')\n",
        " doc = nlp(processed_text.lower())\n",
        " rawText = not (remove_punctuation or remove_stopwords or apply_FE)\n",
        " for sentence in doc.sents:\n",
        " sent = str(sentence.text)\n",
        " if len(sent) == 0:\n",
        " continue\n",
        " if not rawText:\n",
        " if apply_FE:\n",
        " sent = self.applyFE(text=sent)\n",
        " if remove_punctuation:\n",
        " sent = self.removePunctuation(text=sent)\n",
        " if remove_stopwords:\n",
        " sent = self.removeStopWords(text=sent)\n",
        " sent = sent.lower().split()\n",
        " if sent:\n",
        " corpus.append(sent)\n",
        " except Exception as exp:\n",
        " print('exception=', str(exp))\n",
        " print('text=', text)\n",
        " return corpus\n",
        "\n",
        "# WordEmbeddings class (without Mittens)\n",
        "class WordEmbeddings:\n",
        " def __init__(self, corpus, normalize_tfidf=False):\n",
        " self.corpus = corpus\n",
        " self.normalize_tfidf = normalize_tfidf\n",
        " self.documents = []\n",
        " self.sentences = []\n",
        " self.word2id = {}\n",
        " self.no_words = 0\n",
        " self.max_size = 0\n",
        " self.no_docs = len(self.corpus)\n",
        "\n",
        " def prepareDocuments(self):\n",
        " word_id = 1\n",
        " for document in self.corpus:\n",
        " doc = []\n",
        " for sentence in document:\n",
        " self.sentences.append(sentence)\n",
        " for word in sentence:\n",
        " if self.word2id.get(word) is None:\n",
        " self.word2id[word] = word_id\n",
        " word_id += 1\n",
        " doc.append(self.word2id[word])\n",
        " if self.max_size < len(doc):\n",
        " self.max_size = len(doc)\n",
        " self.documents.append(doc)\n",
        " self.no_words = len(self.word2id) + 1\n",
        " return self.documents\n",
        "\n",
        " def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        " self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        " model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        " workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        " self.word2vec[0] = np.zeros(no_components)\n",
        " for word in self.word2id:\n",
        " self.word2vec[self.word2id[word]] = model.wv[word]\n",
        " return self.word2vec\n",
        "\n",
        " def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        " self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        " model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        " vocab = list(self.word2id.keys())\n",
        " cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        " for sentence in self.sentences:\n",
        " for i, word in enumerate(sentence):\n",
        " word_idx = self.word2id[word] - 1\n",
        " for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        " if i != j:\n",
        " cooc_idx = self.word2id[sentence[j]] - 1\n",
        " cooc_matrix[word_idx, cooc_idx] += 1\n",
        " embeddings = model.fit(cooc_matrix)\n",
        " self.word2glove[0] = np.zeros(no_components)\n",
        " for word, idx in self.word2id.items():\n",
        " self.word2glove[idx] = embeddings[idx - 1]\n",
        " return self.word2glove\n",
        "\n",
        " def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        " self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        " model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        " workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        " self.word2fasttext[0] = np.zeros(no_components)\n",
        " for word in self.word2id:\n",
        " self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        " return self.word2fasttext\n",
        "\n",
        "# Helper function for parallel processing\n",
        "def processElement(elem):\n",
        " id_line = elem[0]\n",
        " text = elem[1]\n",
        " tkn = Tokenization()\n",
        " text = tkn.createCorpus(text, remove_stopwords=False)\n",
        " return id_line, text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        " # For Colab, replace sys.argv[1] with a hardcoded file path or upload mechanism\n",
        " # Example: Upload file in Colab\n",
        " from google.colab import files\n",
        " uploaded = files.upload()\n",
        " fn = list(uploaded.keys())[0] # Get the uploaded filename\n",
        " df = pd.read_csv(fn, sep=',')\n",
        "\n",
        " print(df)\n",
        " labels = df['label'].unique()\n",
        " num_classes = len(labels)\n",
        " label2id = {'mostly true': 0, 'mixture of true and false': 1, 'no factual content': 1, 'mostly false': 1}\n",
        " for label in labels:\n",
        " df.loc[df['label'] == label, 'label'] = label2id[label]\n",
        "\n",
        " y = df['label'].astype(int).to_list()\n",
        " sio.savemat('labels.mat', {'y': y})\n",
        "\n",
        " X_network = df[['num_comments', 'num_shares', 'num_likes', 'num_loves', 'num_wows', 'num_hahas', 'num_sads', 'num_angrys']].to_numpy()\n",
        " scaler_std = StandardScaler()\n",
        " X_net_std = scaler_std.fit_transform(X_network)\n",
        " X_net_std = X_net_std.reshape((X_net_std.shape[0], 1, X_net_std.shape[1]))\n",
        " print(\"X_network:\", X_network)\n",
        " print(\"X_net_std:\", X_net_std)\n",
        " sio.savemat('network.mat', {'X_net_std': X_net_std})\n",
        "\n",
        " print(\"Start Tokenization\")\n",
        " texts = df[['id', 'content']].to_numpy().tolist()\n",
        " corpus = [None] * len(texts)\n",
        " no_threads = cpu_count() - 1\n",
        " with ProcessPoolExecutor(max_workers=no_threads) as worker:\n",
        " for result in worker.map(processElement, texts):\n",
        " if result:\n",
        " corpus[result[0]] = result[1]\n",
        "\n",
        " print(\"Corpus sample:\")\n",
        " for idx, doc in enumerate(corpus[:5]): # Show first 5 for brevity\n",
        " print(idx, doc)\n",
        "\n",
        " print(\"Start Document Tokenization\")\n",
        " we = WordEmbeddings(corpus)\n",
        " documents = we.prepareDocuments()\n",
        " vocabulary_size = we.no_words\n",
        " max_size = we.max_size\n",
        " print(\"Vocabulary size:\", vocabulary_size)\n",
        " print(\"Max Document size:\", max_size)\n",
        "\n",
        " X_docs = []\n",
        " for document in documents:\n",
        " doc_size = len(document)\n",
        " X_docs.append(document + [0] * (max_size - doc_size))\n",
        " X_docs = np.array(X_docs)\n",
        " sio.savemat('corpus.mat', {'X': X_docs})\n",
        "\n",
        " print(\"Start W2V CBOW\")\n",
        " w2v_cbow = we.word2vecEmbedding(sg=0)\n",
        " sio.savemat('w2v_cbow.mat', {'w2v_cbow': w2v_cbow})\n",
        "\n",
        " print(\"Start W2V SG\")\n",
        " w2v_sg = we.word2vecEmbedding(sg=1)\n",
        " sio.savemat('w2v_sg.mat', {'w2v_sg': w2v_sg})\n",
        "\n",
        " print(\"Start FT CBOW\")\n",
        " ft_cbow = we.word2FastTextEmbeddings(sg=0)\n",
        " sio.savemat('ft_cbow.mat', {'ft_cbow': ft_cbow})\n",
        "\n",
        " print(\"Start FT SG\")\n",
        " ft_sg = we.word2FastTextEmbeddings(sg=1)\n",
        " sio.savemat('ft_sg.mat', {'ft_sg': ft_sg})\n",
        "\n",
        " print(\"Start GLOVE\")\n",
        " glove = we.word2GloVeEmbedding()\n",
        " sio.savemat('glove.mat', {'glove': glove})"
      ],
      "metadata": {
        "id": "IOWHWvin1v-g",
        "outputId": "e15a8af9-bef4-4121-d0cd-4593162cf1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 26 (<ipython-input-12-37b5f3cf75e6>, line 27)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-37b5f3cf75e6>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    final_text = text.replace('cannot', 'can not').replace('can\\'t', 'can not')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 26\n"
          ]
        }
      ]
    }
  ]
}