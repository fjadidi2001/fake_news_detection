{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsV3HCG5DlgFO+eXAHWtXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/DansE_Mar29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "oA7Z8kvmccRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive in Colab"
      ],
      "metadata": {
        "id": "PxIpn64y8_JC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Mzv5ciD71hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c34f4ef-4ab2-48c2-b771-eed627578666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "9J1bk_HnDc7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Replace with your actual file path\n",
        "file_path = '/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path, encoding='latin-1')"
      ],
      "metadata": {
        "id": "bhyykHvM89kI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Data Inspection"
      ],
      "metadata": {
        "id": "TDzZ-uobD38a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(2))  # See first 2 rows\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSYdMJU3DiFH",
        "outputId": "1b8339e9-e749-4b69-c2d3-5d5e40c697d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     account_id       post_id    Category               Page  \\\n",
            "0  1.840000e+14  1.040000e+15  mainstream  ABC News Politics   \n",
            "1  1.840000e+14  1.040000e+15  mainstream  ABC News Politics   \n",
            "\n",
            "                                            Post URL Date Published Post Type  \\\n",
            "0  https://www.facebook.com/ABCNewsPolitics/posts...      9/19/2016     video   \n",
            "1  https://www.facebook.com/ABCNewsPolitics/posts...      9/19/2016      link   \n",
            "\n",
            "               Rating Debate  share_count  reaction_count  comment_count  \\\n",
            "0  no factual content    NaN          NaN           146.0           15.0   \n",
            "1         mostly true    NaN          1.0            33.0           34.0   \n",
            "\n",
            "                                        Context Post  \n",
            "0  WATCH: &quot;JEB EXCLAMATION POINT!&quot; - Je...  \n",
            "1  Can either candidate move the needle in the de...  \n",
            "\n",
            "Missing values:\n",
            " account_id           0\n",
            "post_id              0\n",
            "Category             0\n",
            "Page                 0\n",
            "Post URL             0\n",
            "Date Published       0\n",
            "Post Type            0\n",
            "Rating               0\n",
            "Debate            1984\n",
            "share_count         70\n",
            "reaction_count       2\n",
            "comment_count        2\n",
            "Context Post         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Values"
      ],
      "metadata": {
        "id": "_hVZN6j7FnqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 1: Fill categorical columns\n",
        "df['Rating'] = df['Rating'].fillna('Unknown')\n",
        "df['Debate'] = df['Debate'].fillna('Not Specified')\n",
        "\n",
        "# Strategy 2: Fill numerical columns with median\n",
        "numeric_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Alternative: Drop rows with critical missing values\n",
        "# df = df.dropna(subset=['important_column'])"
      ],
      "metadata": {
        "id": "CCesok3vEbWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Preprocessing"
      ],
      "metadata": {
        "id": "GFWw2cFnGKbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date to datetime format\n",
        "df['Date Published'] = pd.to_datetime(df['Date Published'], format='%m/%d/%Y')\n",
        "\n",
        "# Clean text columns\n",
        "df['Context Post'] = df['Context Post'].str.replace('\"', '')"
      ],
      "metadata": {
        "id": "-jLm9vpHGC-h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['account_id'] = df['account_id'].astype(str)\n",
        "df['post_id'] = df['post_id'].astype(str)"
      ],
      "metadata": {
        "id": "o2_ZzlhgGlq1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['Category', 'Page', 'Post Type']\n",
        "df[categorical_cols] = df[categorical_cols].fillna('Unknown')"
      ],
      "metadata": {
        "id": "DuV4oIehGoMR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgVsdZkBG2KU",
        "outputId": "5d16b487-68eb-47d9-fcff-c88889ad8418"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2282 entries, 0 to 2281\n",
            "Data columns (total 13 columns):\n",
            " #   Column          Non-Null Count  Dtype         \n",
            "---  ------          --------------  -----         \n",
            " 0   account_id      2282 non-null   object        \n",
            " 1   post_id         2282 non-null   object        \n",
            " 2   Category        2282 non-null   object        \n",
            " 3   Page            2282 non-null   object        \n",
            " 4   Post URL        2282 non-null   object        \n",
            " 5   Date Published  2282 non-null   datetime64[ns]\n",
            " 6   Post Type       2282 non-null   object        \n",
            " 7   Rating          2282 non-null   object        \n",
            " 8   Debate          2282 non-null   object        \n",
            " 9   share_count     2282 non-null   float64       \n",
            " 10  reaction_count  2282 non-null   float64       \n",
            " 11  comment_count   2282 non-null   float64       \n",
            " 12  Context Post    2282 non-null   object        \n",
            "dtypes: datetime64[ns](1), float64(3), object(9)\n",
            "memory usage: 231.9+ KB\n",
            "None\n",
            "account_id        0\n",
            "post_id           0\n",
            "Category          0\n",
            "Page              0\n",
            "Post URL          0\n",
            "Date Published    0\n",
            "Post Type         0\n",
            "Rating            0\n",
            "Debate            0\n",
            "share_count       0\n",
            "reaction_count    0\n",
            "comment_count     0\n",
            "Context Post      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processElement(elem):\n",
        "    id_line = elem[0]\n",
        "    text = elem[1]\n",
        "    # Use 'Context Post' instead of 'content' if needed\n",
        "    text = tkn.createCorpus(text, remove_stopwords=False)\n",
        "    return id_line, text"
      ],
      "metadata": {
        "id": "hGiF99i4JzJb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main source"
      ],
      "metadata": {
        "id": "LVGdPq9osmqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download and save to Drive (run once)\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip -O /content/drive/MyDrive/glove.6B.zip\n",
        "!unzip /content/drive/MyDrive/glove.6B.zip -d /content/drive/MyDrive/glove\n",
        "\n",
        "# Load from Drive in future sessions\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/glove/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K5rbfZArlgD",
        "outputId": "b4013e1c-35b5-4486-c05b-3c22230a7495"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/glove.6B.zip\n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.50d.txt  \n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.100d.txt  \n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.200d.txt  \n",
            "  inflating: /content/drive/MyDrive/glove/glove.6B.300d.txt  \n",
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wordembeddings"
      ],
      "metadata": {
        "id": "kzfAQRwVzfGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean up the environment\n",
        "!pip uninstall -y numpy mittens gensim scipy smart-open wrapt tensorflow tensorflow-datasets dm-tree numba\n",
        "\n",
        "# Step 2: Install compatible versions\n",
        "!pip install numpy==1.26.4 mittens==0.2 gensim==4.3.3 scipy==1.13.1 smart-open==7.1.0 wrapt==1.17.2\n",
        "\n",
        "# Step 3: Restart runtime (run this once, then comment out)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# Step 4: After restart, run the code\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-iINw1nwrYN",
        "outputId": "d1acb732-f0b4-4cfb-a026-a2af9beaa851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping mittens as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: scipy 1.14.1\n",
            "Uninstalling scipy-1.14.1:\n",
            "  Successfully uninstalled scipy-1.14.1\n",
            "Found existing installation: smart-open 7.1.0\n",
            "Uninstalling smart-open-7.1.0:\n",
            "  Successfully uninstalled smart-open-7.1.0\n",
            "Found existing installation: wrapt 1.17.2\n",
            "Uninstalling wrapt-1.17.2:\n",
            "  Successfully uninstalled wrapt-1.17.2\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: tensorflow-datasets 4.9.8\n",
            "Uninstalling tensorflow-datasets-4.9.8:\n",
            "  Successfully uninstalled tensorflow-datasets-4.9.8\n",
            "Found existing installation: dm-tree 0.1.9\n",
            "Uninstalling dm-tree-0.1.9:\n",
            "  Successfully uninstalled dm-tree-0.1.9\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mittens==0.2\n",
            "  Downloading mittens-0.2-py3-none-any.whl.metadata (377 bytes)\n",
            "Collecting gensim==4.3.3\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting scipy==1.13.1\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open==7.1.0\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt==1.17.2\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, mittens, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "tensorflow-probability 0.25.0 requires dm-tree, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "shap 0.47.0 requires numba>=0.54, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 mittens-0.2 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)\n",
        "import osfit(cooccurrence, vocab=None, initial_embedding_dict=None)\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "    def word2MittensEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2mittens = np.empty(shape=(self.no_words, no_components))\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        model = Mittens(n=no_components, max_iter=epochs, mittens=learning_rate)\n",
        "        embeddings = model.fit(cooc_matrix, vocab=vocab)\n",
        "        self.word2mittens[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2mittens[idx] = embeddings[idx - 1]\n",
        "        return self.word2mittens\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    w2m = we.word2MittensEmbedding()\n",
        "    print(\"Mittens:\", w2m.shape)\n",
        "    print(w2m)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])\n",
        "    print(\"Mittens:\", w2m[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tJ-FKOOovXUR",
        "outputId": "2e02fed7-7d80-488e-b76e-531ea717c3a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version: 0.2\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.35759968 -0.41002173 -0.11296739 ... -0.13695379 -0.08978107\n",
            "  -0.02064198]\n",
            " [ 0.03458208 -0.26300714  0.31167143 ... -0.42121385  0.17037075\n",
            "   0.06615572]\n",
            " ...\n",
            " [ 0.00209569  0.26936345  0.07195031 ...  0.01113254  0.02607798\n",
            "   0.16050002]\n",
            " [ 0.00497766  0.02229993  0.00638193 ...  0.05898192 -0.15140509\n",
            "  -0.05942085]\n",
            " [ 0.08744059  0.12552654  0.06976608 ... -0.14350026 -0.07537804\n",
            "   0.00438942]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Mittens' object has no attribute 'has_embedding'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5e2d75135b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mw2m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2MittensEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mittens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a5e2d75135b9>\u001b[0m in \u001b[0;36mword2MittensEmbedding\u001b[0;34m(self, window_size, no_components, epochs, workers, learning_rate)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMittens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmittens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcooc_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2mittens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mittens/mittens_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_coincidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         return self._fit(X, weights, log_coincidence,\n\u001b[0m\u001b[1;32m     82\u001b[0m                          \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                          \u001b[0minitial_embedding_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_embedding_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mittens/np_mittens.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, coincidence, weights, log_coincidence, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             gradients, error = self._get_gradients_and_error(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 pred, log_coincidence, weights)\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mittens/np_mittens.py\u001b[0m in \u001b[0;36m_get_gradients_and_error\u001b[0;34m(self, predictions, log_coincidence, weights)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmittens\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mcurr_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mwgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmittens\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Mittens' object has no attribute 'has_embedding'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)\n",
        "\n",
        "# Minimal test for Mittens\n",
        "vocab = ['hello', 'world']\n",
        "cooc_matrix = np.array([[0, 1], [1, 0]])  # Simple co-occurrence matrix\n",
        "mittens_model = Mittens(n=10, max_iter=10)\n",
        "try:\n",
        "    embeddings = mittens_model.fit(cooc_matrix, vocab=vocab)\n",
        "    print(\"Mittens test successful:\", embeddings.shape)\n",
        "except Exception as e:\n",
        "    print(\"Mittens test failed:\", str(e))\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "    def word2MittensEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2mittens = np.empty(shape=(self.no_words, no_components))\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        model = Mittens(n=no_components, max_iter=epochs, mittens=learning_rate)\n",
        "        try:\n",
        "            embeddings = model.fit(cooc_matrix, vocab=vocab)  # Explicitly test this\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for word, idx in self.word2id.items():\n",
        "                self.word2mittens[idx] = embeddings[idx - 1]\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error in Mittens.fit: {e}\")\n",
        "            embeddings = np.zeros((len(vocab), no_components))  # Fallback\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for i, word in enumerate(vocab, 1):\n",
        "                self.word2mittens[i] = embeddings[i - 1]\n",
        "        return self.word2mittens\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    w2m = we.word2MittensEmbedding()\n",
        "    print(\"Mittens:\", w2m.shape)\n",
        "    print(w2m)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])\n",
        "    print(\"Mittens:\", w2m[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzniUIiExhUt",
        "outputId": "976c519e-ad97-46f8-df34-92413a655258"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version: 0.2\n",
            "Mittens test failed: 'Mittens' object has no attribute 'has_embedding'\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.24156722  0.19728762  0.28115077 ...  0.25290197 -0.28166244\n",
            "   0.03103951]\n",
            " [-0.20704135 -0.10191444  0.2596386  ...  0.16890988  0.03980409\n",
            "   0.13393758]\n",
            " ...\n",
            " [-0.10637504 -0.02374604 -0.0016289  ...  0.03591276  0.28356568\n",
            "   0.06366436]\n",
            " [-0.05972623  0.04286632  0.09204827 ...  0.1193088   0.33685324\n",
            "  -0.31354328]\n",
            " [-0.12742595 -0.09153549  0.28084373 ... -0.32305798  0.05837046\n",
            "  -0.34258802]]\n",
            "Error in Mittens.fit: 'Mittens' object has no attribute 'has_embedding'\n",
            "Mittens: (15, 128)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "Comparison for word ID 1:\n",
            "Word2Vec: [-6.21626532e-05  3.19088437e-03 -7.12745590e-03  6.01633405e-03\n",
            "  4.82634921e-03  3.97735788e-03  5.58952475e-03  6.64290274e-03\n",
            "  6.49750407e-04 -1.30172889e-03  4.28103493e-04 -7.31625035e-03\n",
            "  6.59953337e-03 -5.00487117e-03  6.55153207e-03 -3.32944584e-03\n",
            "  5.09716512e-04 -7.12373713e-03 -7.48809706e-03 -6.06476609e-03\n",
            " -6.01806538e-03  2.62050075e-04 -5.66331577e-03 -3.86920548e-03\n",
            " -4.16113203e-03 -3.37171252e-03  5.41871926e-03  3.72564164e-03\n",
            "  6.80003501e-03  5.55603579e-03 -4.45879949e-03  5.70788607e-03\n",
            " -7.26995803e-03 -2.01427122e-03 -6.02612738e-03  3.31634958e-03\n",
            "  1.43352617e-03  5.52080385e-03  2.32943310e-03 -5.44838468e-03\n",
            "  5.98756177e-03 -4.64449916e-03  7.02972896e-03  2.28456082e-03\n",
            " -3.11649730e-03 -3.70101258e-03 -3.46256793e-03 -4.79195034e-03\n",
            "  7.32414331e-03 -2.00591167e-03  6.06871955e-03 -7.54190609e-03\n",
            "  1.68094877e-03 -9.79775796e-04  5.90900565e-03 -7.03375321e-03\n",
            "  5.83272101e-03 -3.95497913e-03 -4.70296340e-03 -4.40138392e-03\n",
            " -2.66821543e-03 -2.65624467e-03 -2.50413013e-03 -5.86187281e-03\n",
            "  6.00978325e-04 -4.53708722e-04 -1.28188857e-03  2.92269792e-03\n",
            " -5.91868116e-03 -2.48495163e-03  4.01353044e-03  6.70103775e-03\n",
            " -7.66575430e-03  5.61965723e-03  4.15526563e-03 -3.09771299e-03\n",
            "  6.73769321e-03 -7.15261744e-03  5.66246081e-03  4.26008087e-03\n",
            "  9.85841965e-04 -4.10628133e-03 -3.25019588e-03 -2.61222548e-03\n",
            "  1.28108205e-03  1.24865468e-03  5.82506089e-03  7.80742289e-03\n",
            "  6.93350285e-03 -3.08938394e-03  7.56247900e-03 -5.23670344e-04\n",
            "  3.73711600e-03  1.97167136e-03 -4.63132892e-04  2.85691721e-03\n",
            " -4.20561666e-03 -4.55231313e-03 -5.94439125e-03  1.47928542e-03\n",
            "  5.05184289e-03  6.73822535e-04  9.28732043e-04  2.46044085e-03\n",
            "  6.36986131e-03 -6.00371370e-03  1.73082878e-03 -5.86407073e-03\n",
            "  2.87816045e-03  7.43294507e-03  5.87216392e-03  5.02722338e-03\n",
            "  6.27435837e-03  5.13774622e-03  5.31336619e-03  6.73875213e-03\n",
            " -3.89733445e-03  7.20354123e-03  3.95858521e-03 -1.59854104e-03\n",
            "  6.62540551e-03  3.92472045e-03  7.54022226e-03  2.18087039e-03\n",
            "  7.74372742e-03  9.92520479e-04  7.14840600e-03  2.83964048e-03]\n",
            "FastText: [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04  1.07396161e-03\n",
            "  7.47827522e-04  2.43436918e-03  1.25494599e-03  8.32708203e-04\n",
            "  1.39498618e-04 -1.04375544e-03  1.66534202e-03  5.92023076e-04\n",
            " -1.01467571e-03  4.17726085e-04  1.15030364e-03 -8.53901089e-04\n",
            "  3.93663533e-04  1.66030577e-03  8.04261887e-04 -1.37844379e-03\n",
            " -1.00217399e-03 -1.71076367e-03 -6.08318776e-04 -3.13780096e-04\n",
            "  7.09959713e-04  6.52309740e-04 -1.03251881e-03  6.82479527e-04\n",
            "  1.76711939e-04 -1.86265504e-03  6.39586418e-04  1.27322774e-03\n",
            " -2.63598538e-03  1.30318059e-03  1.28268311e-03  7.31006585e-05\n",
            " -8.57610779e-04 -1.77271932e-03  4.76834684e-05  7.52914173e-04\n",
            "  6.25460816e-04 -3.61822196e-04 -2.49825674e-03  6.06271380e-04\n",
            "  2.61506008e-04 -1.91198278e-03 -7.39933399e-04 -4.00340045e-03\n",
            "  3.98896955e-04 -6.40470942e-04 -9.73124232e-04 -4.94266103e-04\n",
            "  9.33958218e-04  3.16818740e-04 -1.39790284e-03  8.03306350e-04\n",
            "  4.68277285e-04 -7.05544895e-04  8.22283386e-04 -1.07828462e-04\n",
            "  3.76482669e-04  2.11784281e-04 -4.11284942e-04  9.16630845e-04\n",
            " -3.64862324e-04 -6.43940584e-04 -1.22504728e-03 -4.09014639e-04\n",
            "  2.33947922e-05 -4.68008395e-04 -1.46471866e-04  1.28898071e-03\n",
            " -1.87438796e-03 -1.11290443e-04 -2.82968773e-04  1.62297476e-03\n",
            " -3.61244201e-05 -1.74025889e-03  2.76980782e-03 -8.65978887e-04\n",
            "  1.14296249e-03 -1.82072268e-04 -1.75701268e-03  7.34397385e-04\n",
            " -1.71435822e-03  3.74486670e-04 -2.81040557e-05 -2.88294628e-04\n",
            "  2.09705951e-03 -7.59013637e-04  1.51762320e-03 -4.17047850e-04\n",
            "  4.31056396e-04 -4.13735892e-04 -1.18768564e-03 -2.78128078e-03\n",
            " -3.55213997e-04 -2.31009349e-03  5.31876693e-04  1.93082809e-03\n",
            "  2.91791657e-04  5.18289220e-04 -7.66348530e-05 -5.61056775e-04\n",
            "  2.11288570e-03  1.20306201e-03  1.37995798e-04  7.44218880e-04\n",
            "  6.21078652e-04  2.57000350e-03  2.15380220e-03  1.53649191e-03\n",
            " -5.03137708e-04 -5.04195632e-04  1.45434562e-04  1.80694833e-03\n",
            "  3.40078317e-04 -9.81442863e-04 -7.77254521e-04 -1.38233358e-04\n",
            " -6.73915958e-04 -6.80455332e-06 -7.82594958e-04  1.34958001e-03\n",
            " -1.20654621e-03  4.79545008e-04  1.65466184e-03 -2.04305412e-04]\n",
            "GloVe: [-0.24156722  0.19728762  0.28115077 -0.01267708 -0.15987858  0.21574398\n",
            " -0.1480353  -0.17230529  0.18451083  0.30139633 -0.01786616 -0.10788706\n",
            " -0.02538971 -0.17632268  0.17546725  0.23223604 -0.04676747 -0.32730955\n",
            " -0.17357518  0.28444662  0.22395164 -0.15287594  0.02411032  0.16742667\n",
            " -0.30741247 -0.08741171 -0.17499335 -0.23875255 -0.10647899 -0.43408586\n",
            "  0.03226503 -0.02690753 -0.03439709 -0.06679865 -0.41822772  0.00441864\n",
            " -0.33425292  0.33217133 -0.02931358 -0.3106526   0.13540257  0.08195526\n",
            " -0.28979497 -0.00119301 -0.14958068 -0.11357246 -0.33342334  0.1647589\n",
            "  0.00192798  0.12798077  0.35912439  0.47988574  0.29816269  0.02062277\n",
            "  0.341725   -0.09348523  0.00970518  0.09629346 -0.13512328  0.03271592\n",
            " -0.22528884  0.12753717 -0.00965846  0.05195969 -0.09110937 -0.03771555\n",
            "  0.11739977 -0.04516209 -0.02465634 -0.05721831 -0.11449855 -0.16619301\n",
            "  0.05787712  0.03165011 -0.10521808 -0.23580368 -0.32708301  0.56562534\n",
            "  0.05927055  0.00094551  0.31607433 -0.35772211 -0.27208042 -0.19823527\n",
            "  0.04679147  0.07974814  0.15175472 -0.03920601  0.17394178  0.27804593\n",
            " -0.00697557 -0.43042942  0.23622731 -0.13813303 -0.2330843  -0.25773563\n",
            "  0.29068773 -0.32492003  0.02964249 -0.24167678  0.12750401 -0.00378335\n",
            " -0.28357644  0.20254182 -0.03352816  0.08491384  0.15584422  0.15268531\n",
            " -0.26024194  0.17879857 -0.18562622 -0.24850885 -0.13399434  0.43196456\n",
            "  0.26941041  0.27321471  0.08527431  0.17110002  0.12516943 -0.36612575\n",
            " -0.07978609 -0.01374283  0.14129145  0.29812763  0.42890394  0.25290197\n",
            " -0.28166244  0.03103951]\n",
            "Mittens: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import Mittens, GloVe\n",
        "import mittens\n",
        "print(\"Mittens version:\", mittens.__version__)\n",
        "\n",
        "from scipy.sparse import csr_matrix  # Add sparse matrix support\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "    def word2MittensEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2mittens = np.empty(shape=(self.no_words, no_components))\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        # Convert to sparse matrix\n",
        "        cooc_matrix_sparse = csr_matrix(cooc_matrix)\n",
        "        model = Mittens(n=no_components, max_iter=epochs, mittens=learning_rate)\n",
        "        try:\n",
        "            embeddings = model.fit(cooc_matrix_sparse, vocab=vocab)\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for word, idx in self.word2id.items():\n",
        "                self.word2mittens[idx] = embeddings[idx - 1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Mittens.fit: {e}\")\n",
        "            embeddings = np.zeros((len(vocab), no_components))  # Fallback\n",
        "            self.word2mittens[0] = np.zeros(no_components)\n",
        "            for i, word in enumerate(vocab, 1):\n",
        "                self.word2mittens[i] = embeddings[i - 1]\n",
        "        return self.word2mittens\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    w2m = we.word2MittensEmbedding()\n",
        "    print(\"Mittens:\", w2m.shape)\n",
        "    print(w2m)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])\n",
        "    print(\"Mittens:\", w2m[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHvZNe7qyhTU",
        "outputId": "9a141780-d10a-4947-c293-d2fb5d271c78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version: 0.2\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0001"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.24973279 -0.47211752 -0.3471582  ... -0.01941465  0.43606771\n",
            "  -0.0892403 ]\n",
            " [-0.2158313  -0.12271662 -0.21352648 ... -0.07537541  0.31608701\n",
            "   0.02839591]\n",
            " ...\n",
            " [ 0.16518111  0.128661   -0.0561627  ... -0.03275083  0.00815867\n",
            "  -0.02345717]\n",
            " [-0.09888651 -0.27747496  0.19922241 ... -0.0432742   0.0807169\n",
            "   0.25686838]\n",
            " [ 0.07700253 -0.14335636  0.38736752 ...  0.15984586  0.09920613\n",
            "   0.03748307]]\n",
            "Error in Mittens.fit: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().\n",
            "Mittens: (15, 128)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "Comparison for word ID 1:\n",
            "Word2Vec: [-6.21626532e-05  3.19088437e-03 -7.12745590e-03  6.01633405e-03\n",
            "  4.82634921e-03  3.97735788e-03  5.58952475e-03  6.64290274e-03\n",
            "  6.49750407e-04 -1.30172889e-03  4.28103493e-04 -7.31625035e-03\n",
            "  6.59953337e-03 -5.00487117e-03  6.55153207e-03 -3.32944584e-03\n",
            "  5.09716512e-04 -7.12373713e-03 -7.48809706e-03 -6.06476609e-03\n",
            " -6.01806538e-03  2.62050075e-04 -5.66331577e-03 -3.86920548e-03\n",
            " -4.16113203e-03 -3.37171252e-03  5.41871926e-03  3.72564164e-03\n",
            "  6.80003501e-03  5.55603579e-03 -4.45879949e-03  5.70788607e-03\n",
            " -7.26995803e-03 -2.01427122e-03 -6.02612738e-03  3.31634958e-03\n",
            "  1.43352617e-03  5.52080385e-03  2.32943310e-03 -5.44838468e-03\n",
            "  5.98756177e-03 -4.64449916e-03  7.02972896e-03  2.28456082e-03\n",
            " -3.11649730e-03 -3.70101258e-03 -3.46256793e-03 -4.79195034e-03\n",
            "  7.32414331e-03 -2.00591167e-03  6.06871955e-03 -7.54190609e-03\n",
            "  1.68094877e-03 -9.79775796e-04  5.90900565e-03 -7.03375321e-03\n",
            "  5.83272101e-03 -3.95497913e-03 -4.70296340e-03 -4.40138392e-03\n",
            " -2.66821543e-03 -2.65624467e-03 -2.50413013e-03 -5.86187281e-03\n",
            "  6.00978325e-04 -4.53708722e-04 -1.28188857e-03  2.92269792e-03\n",
            " -5.91868116e-03 -2.48495163e-03  4.01353044e-03  6.70103775e-03\n",
            " -7.66575430e-03  5.61965723e-03  4.15526563e-03 -3.09771299e-03\n",
            "  6.73769321e-03 -7.15261744e-03  5.66246081e-03  4.26008087e-03\n",
            "  9.85841965e-04 -4.10628133e-03 -3.25019588e-03 -2.61222548e-03\n",
            "  1.28108205e-03  1.24865468e-03  5.82506089e-03  7.80742289e-03\n",
            "  6.93350285e-03 -3.08938394e-03  7.56247900e-03 -5.23670344e-04\n",
            "  3.73711600e-03  1.97167136e-03 -4.63132892e-04  2.85691721e-03\n",
            " -4.20561666e-03 -4.55231313e-03 -5.94439125e-03  1.47928542e-03\n",
            "  5.05184289e-03  6.73822535e-04  9.28732043e-04  2.46044085e-03\n",
            "  6.36986131e-03 -6.00371370e-03  1.73082878e-03 -5.86407073e-03\n",
            "  2.87816045e-03  7.43294507e-03  5.87216392e-03  5.02722338e-03\n",
            "  6.27435837e-03  5.13774622e-03  5.31336619e-03  6.73875213e-03\n",
            " -3.89733445e-03  7.20354123e-03  3.95858521e-03 -1.59854104e-03\n",
            "  6.62540551e-03  3.92472045e-03  7.54022226e-03  2.18087039e-03\n",
            "  7.74372742e-03  9.92520479e-04  7.14840600e-03  2.83964048e-03]\n",
            "FastText: [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04  1.07396161e-03\n",
            "  7.47827522e-04  2.43436918e-03  1.25494599e-03  8.32708203e-04\n",
            "  1.39498618e-04 -1.04375544e-03  1.66534202e-03  5.92023076e-04\n",
            " -1.01467571e-03  4.17726085e-04  1.15030364e-03 -8.53901089e-04\n",
            "  3.93663533e-04  1.66030577e-03  8.04261887e-04 -1.37844379e-03\n",
            " -1.00217399e-03 -1.71076367e-03 -6.08318776e-04 -3.13780096e-04\n",
            "  7.09959713e-04  6.52309740e-04 -1.03251881e-03  6.82479527e-04\n",
            "  1.76711939e-04 -1.86265504e-03  6.39586418e-04  1.27322774e-03\n",
            " -2.63598538e-03  1.30318059e-03  1.28268311e-03  7.31006585e-05\n",
            " -8.57610779e-04 -1.77271932e-03  4.76834684e-05  7.52914173e-04\n",
            "  6.25460816e-04 -3.61822196e-04 -2.49825674e-03  6.06271380e-04\n",
            "  2.61506008e-04 -1.91198278e-03 -7.39933399e-04 -4.00340045e-03\n",
            "  3.98896955e-04 -6.40470942e-04 -9.73124232e-04 -4.94266103e-04\n",
            "  9.33958218e-04  3.16818740e-04 -1.39790284e-03  8.03306350e-04\n",
            "  4.68277285e-04 -7.05544895e-04  8.22283386e-04 -1.07828462e-04\n",
            "  3.76482669e-04  2.11784281e-04 -4.11284942e-04  9.16630845e-04\n",
            " -3.64862324e-04 -6.43940584e-04 -1.22504728e-03 -4.09014639e-04\n",
            "  2.33947922e-05 -4.68008395e-04 -1.46471866e-04  1.28898071e-03\n",
            " -1.87438796e-03 -1.11290443e-04 -2.82968773e-04  1.62297476e-03\n",
            " -3.61244201e-05 -1.74025889e-03  2.76980782e-03 -8.65978887e-04\n",
            "  1.14296249e-03 -1.82072268e-04 -1.75701268e-03  7.34397385e-04\n",
            " -1.71435822e-03  3.74486670e-04 -2.81040557e-05 -2.88294628e-04\n",
            "  2.09705951e-03 -7.59013637e-04  1.51762320e-03 -4.17047850e-04\n",
            "  4.31056396e-04 -4.13735892e-04 -1.18768564e-03 -2.78128078e-03\n",
            " -3.55213997e-04 -2.31009349e-03  5.31876693e-04  1.93082809e-03\n",
            "  2.91791657e-04  5.18289220e-04 -7.66348530e-05 -5.61056775e-04\n",
            "  2.11288570e-03  1.20306201e-03  1.37995798e-04  7.44218880e-04\n",
            "  6.21078652e-04  2.57000350e-03  2.15380220e-03  1.53649191e-03\n",
            " -5.03137708e-04 -5.04195632e-04  1.45434562e-04  1.80694833e-03\n",
            "  3.40078317e-04 -9.81442863e-04 -7.77254521e-04 -1.38233358e-04\n",
            " -6.73915958e-04 -6.80455332e-06 -7.82594958e-04  1.34958001e-03\n",
            " -1.20654621e-03  4.79545008e-04  1.65466184e-03 -2.04305412e-04]\n",
            "GloVe: [-0.24973279 -0.47211752 -0.3471582   0.09206807 -0.13866986  0.15487264\n",
            "  0.10668666 -0.1446956  -0.37882732 -0.31554635 -0.12138892  0.0336272\n",
            " -0.00930984  0.03009921 -0.13521719 -0.08467253 -0.19571198 -0.13949303\n",
            " -0.35111745  0.027482    0.11905168  0.18794745  0.11306527  0.12954\n",
            "  0.05691607  0.13720159  0.22953001  0.10246665  0.20592919 -0.21759341\n",
            " -0.23108193  0.21138186 -0.26241941  0.04116371 -0.21286277 -0.11801152\n",
            "  0.31120586 -0.29845012 -0.05790725 -0.18902083  0.22907882 -0.12611849\n",
            "  0.40500281 -0.22766443  0.04437517 -0.03016994 -0.00181209  0.24601004\n",
            " -0.1288323   0.14103511  0.25497694 -0.09054451  0.16266434  0.11801647\n",
            " -0.16773795 -0.12649524  0.21566807  0.01240493  0.01311322  0.2398281\n",
            "  0.24605117 -0.48681179  0.17264584  0.11088587 -0.30783911  0.25423832\n",
            "  0.05147336 -0.12579411  0.15714605 -0.10685451  0.31093266 -0.17077477\n",
            " -0.03805187 -0.31860494 -0.23026643 -0.17669139  0.26581533 -0.40427327\n",
            "  0.03385626  0.34479212 -0.06676299  0.09452871 -0.0890423  -0.02015828\n",
            " -0.25413745 -0.20022885  0.31813517 -0.0295259  -0.39070183 -0.29587508\n",
            " -0.04877057  0.56845457 -0.27300501  0.20099248  0.56908743  0.04327456\n",
            " -0.28636521  0.52484852  0.06302865 -0.00368409  0.30760488 -0.1834542\n",
            "  0.0405381  -0.01405919 -0.37281245  0.10027112  0.00985007 -0.19940571\n",
            " -0.19570702  0.02242911 -0.33625058 -0.22106823  0.44265247 -0.15831874\n",
            " -0.50237319  0.218131    0.34700928 -0.14113709  0.17201621  0.20608975\n",
            " -0.35104507 -0.23447789 -0.19862362  0.01224652 -0.30237286 -0.01941465\n",
            "  0.43606771 -0.0892403 ]\n",
            "Mittens: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### slimmed-down version without Mittens\n",
        "\n"
      ],
      "metadata": {
        "id": "UlYgCEY3y8LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify versions after restart\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "\n",
        "from mittens import GloVe\n",
        "import mittens\n",
        "print(\"Mittens version (GloVe only):\", mittens.__version__)\n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, corpus, normalize_tfidf=False):\n",
        "        self.corpus = corpus\n",
        "        self.normalize_tfidf = normalize_tfidf\n",
        "        self.documents = []\n",
        "        self.sentences = []\n",
        "        self.word2id = {}\n",
        "        self.no_words = 0\n",
        "        self.max_size = 0\n",
        "        self.no_docs = len(self.corpus)\n",
        "\n",
        "    def prepareDocuments(self):\n",
        "        word_id = 1\n",
        "        for document in self.corpus:\n",
        "            doc = []\n",
        "            for sentence in document:\n",
        "                self.sentences.append(sentence)\n",
        "                for word in sentence:\n",
        "                    if self.word2id.get(word) is None:\n",
        "                        self.word2id[word] = word_id\n",
        "                        word_id += 1\n",
        "                    doc.append(self.word2id[word])\n",
        "            if self.max_size < len(doc):\n",
        "                self.max_size = len(doc)\n",
        "            self.documents.append(doc)\n",
        "\n",
        "        self.no_words = len(self.word2id) + 1\n",
        "        return self.documents\n",
        "\n",
        "    def word2vecEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2vec = np.empty(shape=(self.no_words, no_components))\n",
        "        model = Word2Vec(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2vec[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2vec[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2vec\n",
        "\n",
        "    def word2GloVeEmbedding(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), learning_rate=0.05):\n",
        "        self.word2glove = np.empty(shape=(self.no_words, no_components))\n",
        "        model = GloVe(n=no_components, learning_rate=learning_rate)\n",
        "\n",
        "        vocab = list(self.word2id.keys())\n",
        "        cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "        for sentence in self.sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                word_idx = self.word2id[word] - 1\n",
        "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        cooc_idx = self.word2id[sentence[j]] - 1\n",
        "                        cooc_matrix[word_idx, cooc_idx] += 1\n",
        "\n",
        "        embeddings = model.fit(cooc_matrix)\n",
        "        self.word2glove[0] = np.zeros(no_components)\n",
        "        for word, idx in self.word2id.items():\n",
        "            self.word2glove[idx] = embeddings[idx - 1]\n",
        "        return self.word2glove\n",
        "\n",
        "    def word2FastTextEmbeddings(self, window_size=10, no_components=128, epochs=10, workers=os.cpu_count(), sg=0, learning_rate=0.05):\n",
        "        self.word2fasttext = np.empty(shape=(self.no_words, no_components))\n",
        "        model = FastText(self.sentences, vector_size=no_components, window=window_size, min_count=1,\n",
        "                         workers=workers, sg=sg, alpha=learning_rate, epochs=epochs)\n",
        "        self.word2fasttext[0] = np.zeros(no_components)\n",
        "        for word in self.word2id:\n",
        "            self.word2fasttext[self.word2id[word]] = model.wv[word]\n",
        "        return self.word2fasttext\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    corpus = [\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "        [['Hello', 'this', 'tutorial', 'on', 'how', 'convert', 'word', 'integer', 'format'],\n",
        "         ['this', 'beautiful', 'day'],\n",
        "         ['Jack', 'going', 'office']],\n",
        "    ]\n",
        "\n",
        "    we = WordEmbeddings(corpus)\n",
        "    docs = we.prepareDocuments()\n",
        "    print(np.array(docs, dtype=object).shape)\n",
        "    print(docs)\n",
        "\n",
        "    w2v = we.word2vecEmbedding()\n",
        "    print(\"Word2Vec:\", w2v.shape)\n",
        "    print(w2v)\n",
        "\n",
        "    w2f = we.word2FastTextEmbeddings()\n",
        "    print(\"FastText:\", w2f.shape)\n",
        "    print(w2f)\n",
        "\n",
        "    w2g = we.word2GloVeEmbedding()\n",
        "    print(\"GloVe:\", w2g.shape)\n",
        "    print(w2g)\n",
        "\n",
        "    print(\"\\n\\nComparison for word ID 1:\")\n",
        "    print(\"Word2Vec:\", w2v[1])\n",
        "    print(\"FastText:\", w2f[1])\n",
        "    print(\"GloVe:\", w2g[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fuNa6-WuR4a",
        "outputId": "96a23ddc-0209-42e2-8fd2-ec5579559d96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "Mittens version (GloVe only): 0.2\n",
            "(3, 15)\n",
            "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 10, 11, 12, 13, 14]]\n",
            "Word2Vec: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-6.21626532e-05  3.19088437e-03 -7.12745590e-03 ...  9.92520479e-04\n",
            "   7.14840600e-03  2.83964048e-03]\n",
            " [-3.82514030e-04  1.51936125e-04  3.97204095e-03 ...  2.31946167e-03\n",
            "  -3.84517061e-03  3.48054501e-03]\n",
            " ...\n",
            " [-1.44330040e-03 -3.37802432e-03 -5.04739862e-03 ...  4.78180777e-03\n",
            "   3.40414606e-03  2.03621481e-03]\n",
            " [ 5.68562094e-03 -4.50939965e-03  6.46589976e-03 ...  4.88198549e-03\n",
            "  -7.37946481e-03  7.49228429e-03]\n",
            " [-1.35904271e-03  5.24326880e-03  7.78503902e-03 ... -6.09789602e-03\n",
            "  -7.12639559e-03 -4.63685114e-03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration 100: error 0.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText: (15, 128)\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04 ...  4.79545008e-04\n",
            "   1.65466184e-03 -2.04305412e-04]\n",
            " [-1.36351539e-03  1.12285069e-03 -1.62398699e-03 ...  1.27794396e-03\n",
            "  -4.78518370e-04  1.83548301e-03]\n",
            " ...\n",
            " [-2.22318270e-03  2.95931986e-05 -4.96662979e-04 ...  1.28600537e-03\n",
            "   9.15645447e-04 -2.91286968e-04]\n",
            " [-3.23804357e-04  9.54689051e-04 -1.18813978e-03 ...  6.51234936e-04\n",
            "   2.29042576e-04  1.05157425e-03]\n",
            " [-4.99513757e-04  2.49632495e-03  2.18920293e-03 ...  4.68000828e-04\n",
            "  -1.34677067e-03 -1.70546729e-04]]\n",
            "GloVe: (15, 128)\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.14491981 -0.09447325  0.12144629 ... -0.2624261  -0.18998113\n",
            "  -0.09980709]\n",
            " [ 0.32938427 -0.05994402  0.05077884 ...  0.01185149  0.20612515\n",
            "   0.28760553]\n",
            " ...\n",
            " [-0.01241104 -0.08506157  0.29257517 ... -0.17227083 -0.19028408\n",
            "  -0.19335479]\n",
            " [ 0.37489702 -0.22289123  0.05223132 ... -0.01077101 -0.10361216\n",
            "   0.00802199]\n",
            " [ 0.04938111  0.12498871  0.37583299 ...  0.16933881  0.26277163\n",
            "  -0.23265269]]\n",
            "\n",
            "\n",
            "Comparison for word ID 1:\n",
            "Word2Vec: [-6.21626532e-05  3.19088437e-03 -7.12745590e-03  6.01633405e-03\n",
            "  4.82634921e-03  3.97735788e-03  5.58952475e-03  6.64290274e-03\n",
            "  6.49750407e-04 -1.30172889e-03  4.28103493e-04 -7.31625035e-03\n",
            "  6.59953337e-03 -5.00487117e-03  6.55153207e-03 -3.32944584e-03\n",
            "  5.09716512e-04 -7.12373713e-03 -7.48809706e-03 -6.06476609e-03\n",
            " -6.01806538e-03  2.62050075e-04 -5.66331577e-03 -3.86920548e-03\n",
            " -4.16113203e-03 -3.37171252e-03  5.41871926e-03  3.72564164e-03\n",
            "  6.80003501e-03  5.55603579e-03 -4.45879949e-03  5.70788607e-03\n",
            " -7.26995803e-03 -2.01427122e-03 -6.02612738e-03  3.31634958e-03\n",
            "  1.43352617e-03  5.52080385e-03  2.32943310e-03 -5.44838468e-03\n",
            "  5.98756177e-03 -4.64449916e-03  7.02972896e-03  2.28456082e-03\n",
            " -3.11649730e-03 -3.70101258e-03 -3.46256793e-03 -4.79195034e-03\n",
            "  7.32414331e-03 -2.00591167e-03  6.06871955e-03 -7.54190609e-03\n",
            "  1.68094877e-03 -9.79775796e-04  5.90900565e-03 -7.03375321e-03\n",
            "  5.83272101e-03 -3.95497913e-03 -4.70296340e-03 -4.40138392e-03\n",
            " -2.66821543e-03 -2.65624467e-03 -2.50413013e-03 -5.86187281e-03\n",
            "  6.00978325e-04 -4.53708722e-04 -1.28188857e-03  2.92269792e-03\n",
            " -5.91868116e-03 -2.48495163e-03  4.01353044e-03  6.70103775e-03\n",
            " -7.66575430e-03  5.61965723e-03  4.15526563e-03 -3.09771299e-03\n",
            "  6.73769321e-03 -7.15261744e-03  5.66246081e-03  4.26008087e-03\n",
            "  9.85841965e-04 -4.10628133e-03 -3.25019588e-03 -2.61222548e-03\n",
            "  1.28108205e-03  1.24865468e-03  5.82506089e-03  7.80742289e-03\n",
            "  6.93350285e-03 -3.08938394e-03  7.56247900e-03 -5.23670344e-04\n",
            "  3.73711600e-03  1.97167136e-03 -4.63132892e-04  2.85691721e-03\n",
            " -4.20561666e-03 -4.55231313e-03 -5.94439125e-03  1.47928542e-03\n",
            "  5.05184289e-03  6.73822535e-04  9.28732043e-04  2.46044085e-03\n",
            "  6.36986131e-03 -6.00371370e-03  1.73082878e-03 -5.86407073e-03\n",
            "  2.87816045e-03  7.43294507e-03  5.87216392e-03  5.02722338e-03\n",
            "  6.27435837e-03  5.13774622e-03  5.31336619e-03  6.73875213e-03\n",
            " -3.89733445e-03  7.20354123e-03  3.95858521e-03 -1.59854104e-03\n",
            "  6.62540551e-03  3.92472045e-03  7.54022226e-03  2.18087039e-03\n",
            "  7.74372742e-03  9.92520479e-04  7.14840600e-03  2.83964048e-03]\n",
            "FastText: [-1.05216913e-03 -7.23858946e-04 -2.48263852e-04  1.07396161e-03\n",
            "  7.47827522e-04  2.43436918e-03  1.25494599e-03  8.32708203e-04\n",
            "  1.39498618e-04 -1.04375544e-03  1.66534202e-03  5.92023076e-04\n",
            " -1.01467571e-03  4.17726085e-04  1.15030364e-03 -8.53901089e-04\n",
            "  3.93663533e-04  1.66030577e-03  8.04261887e-04 -1.37844379e-03\n",
            " -1.00217399e-03 -1.71076367e-03 -6.08318776e-04 -3.13780096e-04\n",
            "  7.09959713e-04  6.52309740e-04 -1.03251881e-03  6.82479527e-04\n",
            "  1.76711939e-04 -1.86265504e-03  6.39586418e-04  1.27322774e-03\n",
            " -2.63598538e-03  1.30318059e-03  1.28268311e-03  7.31006585e-05\n",
            " -8.57610779e-04 -1.77271932e-03  4.76834684e-05  7.52914173e-04\n",
            "  6.25460816e-04 -3.61822196e-04 -2.49825674e-03  6.06271380e-04\n",
            "  2.61506008e-04 -1.91198278e-03 -7.39933399e-04 -4.00340045e-03\n",
            "  3.98896955e-04 -6.40470942e-04 -9.73124232e-04 -4.94266103e-04\n",
            "  9.33958218e-04  3.16818740e-04 -1.39790284e-03  8.03306350e-04\n",
            "  4.68277285e-04 -7.05544895e-04  8.22283386e-04 -1.07828462e-04\n",
            "  3.76482669e-04  2.11784281e-04 -4.11284942e-04  9.16630845e-04\n",
            " -3.64862324e-04 -6.43940584e-04 -1.22504728e-03 -4.09014639e-04\n",
            "  2.33947922e-05 -4.68008395e-04 -1.46471866e-04  1.28898071e-03\n",
            " -1.87438796e-03 -1.11290443e-04 -2.82968773e-04  1.62297476e-03\n",
            " -3.61244201e-05 -1.74025889e-03  2.76980782e-03 -8.65978887e-04\n",
            "  1.14296249e-03 -1.82072268e-04 -1.75701268e-03  7.34397385e-04\n",
            " -1.71435822e-03  3.74486670e-04 -2.81040557e-05 -2.88294628e-04\n",
            "  2.09705951e-03 -7.59013637e-04  1.51762320e-03 -4.17047850e-04\n",
            "  4.31056396e-04 -4.13735892e-04 -1.18768564e-03 -2.78128078e-03\n",
            " -3.55213997e-04 -2.31009349e-03  5.31876693e-04  1.93082809e-03\n",
            "  2.91791657e-04  5.18289220e-04 -7.66348530e-05 -5.61056775e-04\n",
            "  2.11288570e-03  1.20306201e-03  1.37995798e-04  7.44218880e-04\n",
            "  6.21078652e-04  2.57000350e-03  2.15380220e-03  1.53649191e-03\n",
            " -5.03137708e-04 -5.04195632e-04  1.45434562e-04  1.80694833e-03\n",
            "  3.40078317e-04 -9.81442863e-04 -7.77254521e-04 -1.38233358e-04\n",
            " -6.73915958e-04 -6.80455332e-06 -7.82594958e-04  1.34958001e-03\n",
            " -1.20654621e-03  4.79545008e-04  1.65466184e-03 -2.04305412e-04]\n",
            "GloVe: [-0.14491981 -0.09447325  0.12144629  0.33654058 -0.26823103 -0.00851899\n",
            " -0.01720095  0.00417736  0.07016572  0.15440059 -0.05897453 -0.03388286\n",
            "  0.00166583  0.02728988 -0.02364711  0.20430687 -0.27785534  0.17192418\n",
            " -0.4214381  -0.31993437  0.37089122  0.31378692 -0.16434697  0.06254594\n",
            " -0.37126779  0.02074445  0.02755287 -0.03419243 -0.04800101  0.23402676\n",
            " -0.03905685  0.11388236 -0.09545866 -0.06881654  0.46783014 -0.1684731\n",
            " -0.30359971 -0.18722352 -0.31340323 -0.04368029 -0.0997356   0.20585347\n",
            " -0.12803517 -0.03103505 -0.22988654 -0.20921109  0.13892224 -0.25125431\n",
            " -0.34561802 -0.02544955 -0.11478891  0.2110641  -0.10692891  0.14797026\n",
            "  0.21640493  0.0203965   0.02068296 -0.02507762 -0.06478238 -0.21582785\n",
            " -0.18612787  0.33198676 -0.23596947 -0.1166765   0.08838274 -0.11613951\n",
            "  0.17417221  0.04276799 -0.29862126  0.33014894  0.04703465 -0.2152572\n",
            " -0.17628108  0.08623899 -0.0541081  -0.12096651 -0.07759373  0.12235248\n",
            "  0.01088267 -0.03379119  0.01154564 -0.0255124  -0.11684421 -0.08885345\n",
            "  0.17115128  0.17162357  0.25984815 -0.23705988 -0.06767285  0.23263982\n",
            "  0.17997825 -0.36049717 -0.27579434 -0.04070212  0.20846426 -0.25062646\n",
            " -0.22516775 -0.03231266  0.21774265  0.23756354  0.01277131  0.23864298\n",
            " -0.20071755  0.1087316  -0.3951387   0.05903905 -0.21734861  0.11410184\n",
            "  0.02475853  0.34337829 -0.08874847  0.25982878 -0.00071343  0.04635114\n",
            " -0.17474179 -0.08504665 -0.14526755  0.01800153 -0.04054321 -0.16867252\n",
            "  0.0332301   0.03932167 -0.35854248  0.15627902  0.40057809 -0.2624261\n",
            " -0.18998113 -0.09980709]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenization"
      ],
      "metadata": {
        "id": "Vomn4xUHzZEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import spacy\n",
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "specialchar_dic={\n",
        "    \"’\": \"'\",\n",
        "    \"„\": \"\\\"\",\n",
        "    \"“\": \"\\\"\",\n",
        "    \"”\": \"\\\"\",\n",
        "    \"«\": \"<<\",\n",
        "    \"»\": \">>\",\n",
        "    \"…\": \"...\",\n",
        "    \"—\": \"--\",\n",
        "    \"¡\": \"!\",\n",
        "    \"¿\": \"?\",\n",
        "    \"©\": \" \",\n",
        "    \"–\": \" \"\n",
        "}\n",
        "\n",
        "def stopWordsEN():\n",
        "    sw_stop_words = get_stop_words('en')\n",
        "    sw_nltk = stopwords.words('english')\n",
        "    sw_spacy = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
        "    sw_mallet = ['a', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', 'different', 'do', 'does', 'doing', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', 'way', 'we', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'would', 'x', 'y', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n",
        "    return list(set(sw_stop_words + sw_nltk + sw_mallet + sw_spacy))\n",
        "\n",
        "punctuation = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "specialchar_re = re.compile('(%s)' % '|'.join(specialchar_dic.keys()))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "cachedStopWords_en = stopWordsEN()\n",
        "\n",
        "class Tokenization:\n",
        "    def applyFE(self, text):\n",
        "        \"\"\"This method will combine the negation with the words\n",
        "        Will result in a bigger vocabulary but with less bias\n",
        "        \"\"\"\n",
        "        final_text = text.replace('cannot', 'can not')\n",
        "        final_text = final_text.replace('can\\'t', 'can not')\n",
        "        final_text = final_text.replace('won\\'t', 'will not')\n",
        "        final_text = final_text.replace('n\\'t', ' not')\n",
        "        final_text = final_text.replace(' not ', ' not')\n",
        "\n",
        "        return final_text\n",
        "\n",
        "    def removeStopWords(self, text):\n",
        "        return ' '.join([word for word in text.split() if word not in cachedStopWords_en])\n",
        "\n",
        "    def removePunctuation(self, text, punctuation = punctuation):\n",
        "        for c in punctuation:\n",
        "            text = text.replace(c, ' ')\n",
        "        return text\n",
        "\n",
        "    def replaceUTF8Char(self, text, specialchars=specialchar_dic):\n",
        "        def replace(match):\n",
        "            return specialchars[match.group(0)]\n",
        "        return specialchar_re.sub(replace, text)\n",
        "\n",
        "    def createCorpus(self, text, remove_punctuation=True, remove_stopwords=True, apply_FE=True):\n",
        "        corpus = []\n",
        "        try:\n",
        "            text = self.replaceUTF8Char(text).replace(\"\\n\", \" \")\n",
        "            doc = nlp(text)\n",
        "            processed_text =  ' '.join([t.lemma_ if t.lemma_ != '-PRON-' else t.text if not t.ent_type_ else t.text for t in doc])\n",
        "            processed_text = processed_text.replace(\"\\s\\s+\", ' ')\n",
        "\n",
        "            doc = nlp(processed_text.lower())\n",
        "\n",
        "            rawText = not (remove_punctuation or remove_stopwords or apply_FE)\n",
        "\n",
        "            for sentence in doc.sents:\n",
        "                sent = str(sentence.text)\n",
        "                if len(sent) == 0:\n",
        "                    continue\n",
        "                if not rawText:\n",
        "                    if apply_FE:\n",
        "                        sent = self.applyFE(text=sent)\n",
        "                    if remove_punctuation:\n",
        "                        sent = self.removePunctuation(text=sent)\n",
        "                    if remove_stopwords:\n",
        "                        sent = self.removeStopWords(text=sent)\n",
        "                sent = sent.lower().split()\n",
        "                if sent:\n",
        "                    corpus.append(sent)\n",
        "        except Exception as exp:\n",
        "            print('exception=', str(exp))\n",
        "            print('text=', text)\n",
        "        # print(corpus)\n",
        "        return corpus\n",
        "\n",
        "    def __del__(self):\n",
        "        print(\"Destructor Tokenization\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tkn = Tokenization()\n",
        "    text = \"Apple data-intensive is looking at buying U.K. startup for $1 billion. This is great! The new D.P. model is funcitonal and ready\"\n",
        "    print(tkn.createCorpus(text))\n",
        "    text = \"\"\"The lion may be known as the king of the jungle, but lions do not live in jungles. They’re the rulers of the African savannahs that are covered in brown grasses and speckled with sparse trees. Lions’ coloring helps them blend in perfectly with the tall grass so they can ambush their prey as best as possible. And lions are ferocious. Although they’re one of the most powerful predators on land, lions are in danger. Hunters and poachers target lions to prove to the world their machismo.\\n\\nAnd while hunters seek to wipe lions off the face of the earth to bolster their egos, the Kevin Richardson Wildlife Sanctuary hopes to stop them and protect the big African cat at all cost.\\n\\nRichardson has earned the nickname the “Lion Whisperer” for a reason. He aims to educate the world about lions. And for those lucky enough to volunteer alongside Richardson, he encourages them to learn more about lions and help protect the wild species.\\n\\n“To raise awareness, Kevin has now set up his YouTube Channel ‘LionWhispererTV’. The channel is all about raising awareness about not only the declining numbers of lions but also how this rapid decrease is happening. By watching these videos, you are directly contributing to our scheme of land acquisition,” he writes in his bio.\\n\\nAs part of the volunteer program, Richardson hosts a “volunteer enrichment and lion enrichment” walk. As the name suggests, Richardson takes his group of volunteers out into the savannah of South Africa to hang out with two lions. There, the volunteers meet a male lion, Bobcat, and a female lioness, Gabby. Both lions look ferocious, but are truly “affectionate,” at least that’s what Richardson says. And remember, he’s the lion whisperer, so he’s got an advantage with these deadly big cats.\\n\\nAs Richardson showers the pair of lions with love, the volunteers stay locked in the truck, unwilling to put their lives in danger. And while they are in the vehicle, the lions are just feet from them – and if something goes wrong, they could wind up injured anyway.\\n\\nRichardson shared the video on his “The Lion Whisperer” YouTube channel. With more than one million hits, this video has proven to be one of his most famous.\\n\\nThe video describes the moment caught on tape as follows:\\n\\n“It’s an enrichment walk for both the volunteers and the lions as Kevin shows off his lovely lions as well as giving some amazing lion facts to the volunteers.”\\n\\nViewers like you are overwhelmed with the magnificent footage. The following are a few comments shared on the video.\\n\\n“I hope to someday volunteer there with Kevin. I believe in the work and his perspective about conservation. This video makes me want to all the more! Bobcat and Gabby are lovely lions.” “Every time I watch a one of your videos I somehow end up smiling from ear to ear!” “That was so beautiful, wish I could rub my head against a lion.”\\n\\nTake a moment to watch this video. Would you ever want to volunteer with Kevin Richardson and his lions?\"\"\"\n",
        "    corpus = tkn.createCorpus(text, remove_stopwords=False)\n",
        "    print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "HAIG9UunzNiF",
        "outputId": "da1e687f-4270-4e68-f624-7cf9f9f15f29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'stop_words'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-de469b7d88c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}