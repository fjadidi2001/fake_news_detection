{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT4NZbm2bSfRh+ldEE4fTx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/DANSE_Mar26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step-by-Step Workflow to Apply DANES Methodology on `facebook-fact-check.csv` Dataset**"
      ],
      "metadata": {
        "id": "QuMB86_BSu5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **📌 Step 1: Install Required Libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0lTiI3RcTC0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **📌 Step 2: Load and Explore the Dataset**\n",
        "Start by loading and understanding your dataset."
      ],
      "metadata": {
        "id": "qTD0YNhcTId0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/facebook-fact-check.csv\", encoding='latin1')\n",
        "\n",
        "# Check initial shape and missing values\n",
        "print(\"Initial shape:\", df.shape)  # Should be (2282, 14)\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhBS12yFUt1F",
        "outputId": "49c61799-4e39-4786-e18d-d2cfad947b09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape: (2282, 13)\n",
            "\n",
            "Missing values:\n",
            " account_id           0\n",
            "post_id              0\n",
            "Category             0\n",
            "Page                 0\n",
            "Post URL             0\n",
            "Date Published       0\n",
            "Post Type            0\n",
            "Rating               0\n",
            "Debate            1984\n",
            "share_count         70\n",
            "reaction_count       2\n",
            "comment_count        2\n",
            "Context Post         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔹 **Check for Missing Values**\n",
        "---"
      ],
      "metadata": {
        "id": "PsI0yJ4mTO6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **📌 Step 3: Define Target Variable**\n",
        "- If `Rating` column contains fact-checking labels, convert it into a binary/multi-class target variable.\n",
        "---"
      ],
      "metadata": {
        "id": "qSIGJlpaTfMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique ratings\n",
        "print(\"Unique Ratings:\", df['Rating'].unique())\n",
        "\n",
        "# Example mapping (adjust based on your data)\n",
        "\n",
        "\n",
        "# For multi-class, use LabelEncoder instead:\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "df['label'] = LabelEncoder().fit_transform(df['Rating'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owfjtXLJDKVu",
        "outputId": "9a8cb136-58fd-4e8e-8811-68243e73ec0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Ratings: ['no factual content' 'mostly true' 'mixture of true and false'\n",
            " 'mostly false']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **📌 Step 4: Preprocess Text Data (Text Branch)**\n",
        "Since DANES uses deep learning models, we need to **clean, tokenize, and embed** the text."
      ],
      "metadata": {
        "id": "APTjmwY4Tns5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxZyNiB4D_XZ",
        "outputId": "1670fc97-13ad-4cac-a9be-fa1336e10ab5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "df['processed_text'] = df['Context Post'].apply(preprocess_text)\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(df['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['processed_text'].apply(lambda x: ' '.join(x)))\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_text'].apply(lambda x: ' '.join(x)))\n",
        "max_len = 100  # Adjust based on data analysis\n",
        "X_text = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Create embedding matrix\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "om-xnwx-D2yh",
        "outputId": "b913454a-027f-4d5b-fbcb-155254e5c20d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ecc3d3b32c2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔹 **Convert Text to Word Embeddings**\n",
        "Use **Word2Vec, FastText, or GloVe** to obtain word embeddings.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VKKsiF_zTwBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 5: Prepare Social Context Features (Social Branch)**\n",
        "We normalize numerical social features (`share_count`, `reaction_count`, `comment_count`).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "T20kXUhFT2p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **📌 Step 6: Train-Test Split**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XD1nBvBhUE-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **📌 Step 7: Build the DANES Model (Deep Learning)**\n",
        " Create the **Text Branch (LSTM)** and **Social Branch (MLP/CNN)** and combine them.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pzwWWhf5UL1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **📌 Step 8: Train the Model**\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 9: Evaluate the Model**\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 10: Save the Model for Future Use**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wpXJHiZbSpmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt0mre53SOag"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's proceed with implementing Steps 3 to 10:\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 3: Define Target Variable**\n",
        "Convert the `Rating` column into a binary target variable.\n",
        "\n",
        "```python\n",
        "# Check unique ratings\n",
        "print(\"Unique Ratings:\", df['Rating'].unique())\n",
        "\n",
        "# Example mapping (adjust based on your data)\n",
        "# Binary classification: 'True' vs others\n",
        "df['label'] = df['Rating'].apply(lambda x: 1 if x == 'True' else 0)\n",
        "\n",
        "# For multi-class, use LabelEncoder instead:\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# df['label'] = LabelEncoder().fit_transform(df['Rating'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 4: Preprocess Text Data**\n",
        "Clean the text and convert it into embeddings.\n",
        "\n",
        "```python\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "df['processed_text'] = df['Context Post'].apply(preprocess_text)\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(df['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['processed_text'].apply(lambda x: ' '.join(x)))\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_text'].apply(lambda x: ' '.join(x)))\n",
        "max_len = 100  # Adjust based on data analysis\n",
        "X_text = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Create embedding matrix\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 5: Prepare Social Context Features**\n",
        "Handle missing values and scale numerical features.\n",
        "\n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "social_features = imputer.fit_transform(df[['share_count', 'reaction_count', 'comment_count']])\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_social = scaler.fit_transform(social_features)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 6: Train-Test Split**\n",
        "Split the dataset into training and testing sets.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_text_train, X_text_test, X_social_train, X_social_test, y_train, y_test = train_test_split(\n",
        "    X_text, X_social, df['label'], test_size=0.2, stratify=df['label'], random_state=42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 7: Build the DANES Model**\n",
        "Create a dual-input neural network.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "\n",
        "# Text branch\n",
        "text_input = Input(shape=(max_len,))\n",
        "x = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(text_input)\n",
        "x = LSTM(64)(x)\n",
        "\n",
        "# Social branch\n",
        "social_input = Input(shape=(3,))\n",
        "y = Dense(32, activation='relu')(social_input)\n",
        "\n",
        "# Combine branches\n",
        "combined = Concatenate()([x, y])\n",
        "z = Dense(64, activation='relu')(combined)\n",
        "output = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "model = Model(inputs=[text_input, social_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 8: Train the Model**\n",
        "Train the model on the training data.\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    [X_text_train, X_social_train], y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 9: Evaluate the Model**\n",
        "Assess performance on the test set.\n",
        "\n",
        "```python\n",
        "# Evaluate accuracy\n",
        "loss, accuracy = model.evaluate([X_text_test, X_social_test], y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate classification report\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = (model.predict([X_text_test, X_social_test]) > 0.5).astype(int)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Step 10: Save the Model**\n",
        "Save the trained model for future use.\n",
        "\n",
        "```python\n",
        "model.save('danes_model.h5')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This completes the implementation of the DANES framework on your dataset. Adjust hyperparameters (epochs, embedding dimensions, etc.) based on your specific requirements."
      ],
      "metadata": {
        "id": "e6LBwhGdW3f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C plan"
      ],
      "metadata": {
        "id": "rrQm83WcEndz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "hTlb5TDrEzmf",
        "outputId": "3e2e25fc-ac06-4eb5-e9bf-6ddf341e0321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Step 1: Install Required Libraries\n",
        "# Note: Ensure these are installed via pip or conda\n",
        "# pip install pandas numpy matplotlib seaborn scikit-learn tensorflow gensim\n",
        "\n",
        "# Step 2: Load and Explore the Dataset\n",
        "def load_and_explore_dataset(filepath):\n",
        "    # Load dataset with latin1 encoding\n",
        "    df = pd.read_csv(filepath, encoding='latin1')\n",
        "\n",
        "    # Initial dataset information\n",
        "    print(\"Initial Dataset Shape:\", df.shape)\n",
        "    print(\"\\nColumn Names:\", list(df.columns))\n",
        "\n",
        "    # Missing values analysis\n",
        "    print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "\n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
        "    plt.title('Missing Values Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Basic dataset statistics\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "filepath = \"/content/facebook-fact-check.csv\"\n",
        "df = load_and_explore_dataset(filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Z6n93iK9EXto",
        "outputId": "b2b36f5d-ba9e-424c-bccd-5b3b20eea7dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cc302b7adddd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Step 1: Install Required Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Train the Model\n",
        "def train_danes_model(model, X_text_train, X_social_train, y_train, validation_split=0.2, epochs=50, batch_size=32):\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_text_train, X_social_train],\n",
        "        y_train,\n",
        "        validation_split=validation_split,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# Train the model\n",
        "history = train_danes_model(danes_model, X_text_train, X_social_train, y_train)\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "def evaluate_model(model, X_text_test, X_social_test, y_test):\n",
        "    # Model evaluation\n",
        "    test_loss, test_accuracy = model.evaluate(\n",
        "        [X_text_test, X_social_test],\n",
        "        y_test\n",
        "    )\n",
        "\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Detailed Classification Report\n",
        "    y_pred = model.predict([X_text_test, X_social_test])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "    # Visualization of training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(danes_model, X_text_test, X_social_test, y_test)\n",
        "\n",
        "# Step 10: Save the Model for Future Use\n",
        "def save_danes_model(model, tokenizer, label_encoder, scaler, base_path='danes_model/'):\n",
        "    import os\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Save model\n",
        "    model.save(os.path.join(base_path, 'model.h5'))\n",
        "\n",
        "    # Save additional components\n",
        "    import joblib\n",
        "    joblib.dump(tokenizer, os.path.join(base_path, 'tokenizer.pkl'))\n",
        "    joblib.dump(label_encoder, os.path.join(base_path, 'label_encoder.pkl'))\n",
        "    joblib.dump(scaler, os.path.join(base_path, 'social_scaler.pkl'))\n",
        "\n",
        "    print(\"Model and supporting files saved successfully!\")\n",
        "\n",
        "# Save the model\n",
        "save_danes_model(danes_model, tokenizer, label_encoder, scaler)"
      ],
      "metadata": {
        "id": "s8zsKg17EQLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}