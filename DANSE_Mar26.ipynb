{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4wrQEtHunHiUE9tzlEm+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/DANSE_Mar26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step-by-Step Workflow to Apply DANES Methodology on `facebook-fact-check.csv` Dataset**"
      ],
      "metadata": {
        "id": "QuMB86_BSu5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 1: Install Required Libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0lTiI3RcTC0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **ðŸ“Œ Step 2: Load and Explore the Dataset**\n",
        "Start by loading and understanding your dataset."
      ],
      "metadata": {
        "id": "qTD0YNhcTId0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ **Check for Missing Values**\n",
        "---"
      ],
      "metadata": {
        "id": "PsI0yJ4mTO6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **ðŸ“Œ Step 3: Define Target Variable**\n",
        "- If `Rating` column contains fact-checking labels, convert it into a binary/multi-class target variable.\n",
        "---"
      ],
      "metadata": {
        "id": "qSIGJlpaTfMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 4: Preprocess Text Data (Text Branch)**\n",
        "Since DANES uses deep learning models, we need to **clean, tokenize, and embed** the text."
      ],
      "metadata": {
        "id": "APTjmwY4Tns5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ **Convert Text to Word Embeddings**\n",
        "Use **Word2Vec, FastText, or GloVe** to obtain word embeddings.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VKKsiF_zTwBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df[\"cleaned_text\"])\n",
        "\n",
        "# Convert to sequences and pad them\n",
        "sequences = tokenizer.texts_to_sequences(df[\"cleaned_text\"])\n",
        "max_length = 100  # Adjust based on text length distribution\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 5: Prepare Social Context Features (Social Branch)**\n",
        "We normalize numerical social features (`share_count`, `reaction_count`, `comment_count`).\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "social_features = [\"share_count\", \"reaction_count\", \"comment_count\"]\n",
        "scaler = StandardScaler()\n",
        "df[social_features] = scaler.fit_transform(df[social_features])\n",
        "```\n",
        "\n",
        "If categorical features like `Post Type` are useful:\n",
        "```python\n",
        "df = pd.get_dummies(df, columns=[\"Post Type\"], drop_first=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 6: Train-Test Split**\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X_text = np.array(padded_sequences)\n",
        "X_social = df[social_features].values\n",
        "y = df[\"Rating\"].values  # Target variable\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_text_train, X_text_test, X_social_train, X_social_test, y_train, y_test = train_test_split(\n",
        "    X_text, X_social, y, test_size=0.2, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 7: Build the DANES Model (Deep Learning)**\n",
        "Now, we create the **Text Branch (LSTM)** and **Social Branch (MLP/CNN)** and combine them.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Concatenate, Dropout\n",
        "\n",
        "# Define Text Branch\n",
        "input_text = Input(shape=(max_length,))\n",
        "embedding_layer = Embedding(input_dim=5000, output_dim=128, input_length=max_length)(input_text)\n",
        "lstm_layer = LSTM(64, return_sequences=False)(embedding_layer)\n",
        "\n",
        "# Define Social Branch\n",
        "input_social = Input(shape=(len(social_features),))\n",
        "dense_layer = Dense(32, activation=\"relu\")(input_social)\n",
        "\n",
        "# Concatenate Text & Social Branch\n",
        "merged = Concatenate()([lstm_layer, dense_layer])\n",
        "dropout = Dropout(0.3)(merged)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)  # Binary classification\n",
        "\n",
        "# Build Model\n",
        "model = Model(inputs=[input_text, input_social], outputs=output)\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 8: Train the Model**\n",
        "```python\n",
        "model.fit([X_text_train, X_social_train], y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 9: Evaluate the Model**\n",
        "```python\n",
        "loss, accuracy = model.evaluate([X_text_test, X_social_test], y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "To check classification performance:\n",
        "```python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = (model.predict([X_text_test, X_social_test]) > 0.5).astype(\"int\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Step 10: Save the Model for Future Use**\n",
        "```python\n",
        "model.save(\"danes_facebook_fake_news.h5\")\n",
        "```"
      ],
      "metadata": {
        "id": "wpXJHiZbSpmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt0mre53SOag"
      },
      "outputs": [],
      "source": []
    }
  ]
}