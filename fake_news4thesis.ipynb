{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlXUjZMpoSMPFfo4Kc+Fzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/fake_news4thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 1: SETUP AND IMPORTS & DETERMINISTIC SETUP\n",
        "# ==========================================\n",
        "import os\n",
        "# Install dependencies\n",
        "!pip install torch-geometric imbalanced-learn -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "from scipy import io as sio\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "sns.set_palette(\"dark\")\n",
        "\n",
        "# ==========================================\n",
        "# COMPREHENSIVE REPRODUCIBILITY SETUP (Fixed CUDA issues)\n",
        "# ==========================================\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "    \"\"\"Set all possible seeds for reproducibility\"\"\"\n",
        "    # Python random\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Only set these if CUDA is available and working\n",
        "        try:\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not set CUDNN settings: {e}\")\n",
        "\n",
        "    # Environment variables for additional determinism\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Apply comprehensive seed setting\n",
        "set_all_seeds(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported and seeds set for reproducibility!\")\n",
        "\n",
        "# ==========================================\n",
        "# DEBUG HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "def debug_tensor(tensor, name):\n",
        "    \"\"\"Debug tensor properties\"\"\"\n",
        "    print(f\"Debug {name}:\")\n",
        "    print(f\"  Shape: {tensor.shape}\")\n",
        "    print(f\"  Dtype: {tensor.dtype}\")\n",
        "    print(f\"  Device: {tensor.device}\")\n",
        "    print(f\"  Min: {tensor.min().item()}\")\n",
        "    print(f\"  Max: {tensor.max().item()}\")\n",
        "    print(f\"  Has NaN: {torch.isnan(tensor).any().item()}\")\n",
        "    print(f\"  Has Inf: {torch.isinf(tensor).any().item()}\")\n",
        "    if len(tensor.shape) == 1:\n",
        "        print(f\"  Unique values: {torch.unique(tensor).cpu().numpy()}\")\n",
        "\n",
        "def validate_labels(labels, num_classes=2):\n",
        "    \"\"\"Validate labels for classification\"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    print(f\"Label validation:\")\n",
        "    print(f\"  Unique labels: {unique_labels}\")\n",
        "    print(f\"  Expected range: [0, {num_classes-1}]\")\n",
        "    print(f\"  All labels valid: {all(0 <= label < num_classes for label in unique_labels)}\")\n",
        "    return all(0 <= label < num_classes for label in unique_labels)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: DATA LOADING AND INITIAL EXPLORATION\n",
        "# ==========================================\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "# IMPORTANT: Sort dataframe to ensure consistent ordering\n",
        "df = df.sort_values(['account_id', 'post_id']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"Number of samples: {len(df)}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "print(f\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: NETWORK FEATURES PREPROCESSING\n",
        "# ==========================================\n",
        "def preprocess_network_features(df, random_state=42):\n",
        "    \"\"\"Enhanced preprocessing with outlier handling and normalization\"\"\"\n",
        "    # Extract network features\n",
        "    network_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "    features = df[network_cols].copy()\n",
        "    # Handle missing values with consistent median\n",
        "    features = features.fillna(features.median())\n",
        "    # Log transform to handle skewness (add 1 to avoid log(0))\n",
        "    features_log = np.log1p(features)\n",
        "    # Deterministic outlier capping\n",
        "    for col in features_log.columns:\n",
        "        q99 = features_log[col].quantile(0.99)\n",
        "        features_log[col] = features_log[col].clip(upper=q99)\n",
        "    return features_log.values, features.values\n",
        "\n",
        "# Preprocess network features deterministically\n",
        "network_features_processed, network_features_raw = preprocess_network_features(df, random_state=42)\n",
        "print(\"‚úÖ Network features preprocessed\")\n",
        "print(f\"Raw features shape: {network_features_raw.shape}\")\n",
        "print(f\"Processed features shape: {network_features_processed.shape}\")\n",
        "\n",
        "# Standardize network features with consistent seed\n",
        "scaler = StandardScaler()\n",
        "X_net_std = scaler.fit_transform(network_features_processed)\n",
        "print(f\"Standardized features shape: {X_net_std.shape}\")\n",
        "print(f\"NaN check: {np.isnan(X_net_std).any()}\")\n",
        "\n",
        "# Save for later use\n",
        "sio.savemat('/content/drive/MyDrive/Projects/Hayat/network_processed.mat', {\n",
        "    'X_net_std': X_net_std,\n",
        "    'scaler_mean': scaler.mean_,\n",
        "    'scaler_scale': scaler.scale_\n",
        "})\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: LABEL PREPARATION AND ANALYSIS (FIXED)\n",
        "# ==========================================\n",
        "# Prepare labels (binary classification) consistently\n",
        "print(\"\\nüîç Analyzing labels...\")\n",
        "print(f\"Unique ratings: {df['Rating'].value_counts()}\")\n",
        "\n",
        "# Create binary labels - CRITICAL FIX\n",
        "labels = df['Rating'].apply(lambda x: 0 if x == 'mostly true' else 1).values\n",
        "y = np.array(labels)\n",
        "\n",
        "# VALIDATE LABELS\n",
        "print(f\"Label distribution: {np.bincount(y)}\")\n",
        "print(f\"Class 0 (mostly true): {np.bincount(y)[0]} ({np.bincount(y)[0]/len(y)*100:.1f}%)\")\n",
        "print(f\"Class 1 (others): {np.bincount(y)[1]} ({np.bincount(y)[1]/len(y)*100:.1f}%)\")\n",
        "\n",
        "# CRITICAL: Validate labels are in correct range\n",
        "if not validate_labels(y, num_classes=2):\n",
        "    raise ValueError(\"Labels contain invalid values! Must be in range [0, 1]\")\n",
        "\n",
        "print(\"‚úÖ Labels validated successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: IMPROVED GRAPH CONSTRUCTION\n",
        "# ==========================================\n",
        "def construct_meaningful_graph(df, X_net_std, method='hybrid', similarity_threshold=0.7, max_connections=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Construct graph with meaningful connections\n",
        "    Methods: 'similarity', 'account', 'hybrid'\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state) # Ensure deterministic graph construction\n",
        "    G = nx.Graph()\n",
        "    # Add nodes with features\n",
        "    for idx in range(len(df)):\n",
        "        G.add_node(idx, features=X_net_std[idx])\n",
        "    if method == 'similarity':\n",
        "        # Similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            # Find most similar posts\n",
        "            similarities = similarity_matrix[i]\n",
        "            similar_indices = np.argsort(similarities)[::-1][1:max_connections+1]  # Exclude self\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold:\n",
        "                    G.add_edge(i, j, weight=similarities[j])\n",
        "    elif method == 'account':\n",
        "        # Account-based edges (limited)\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id, indices in account_groups.items():\n",
        "            indices = sorted(list(indices)) # Sort for consistency\n",
        "            if len(indices) > 1:\n",
        "                # Connect only recent posts (limit connections)\n",
        "                for i in range(min(len(indices), max_connections)):\n",
        "                    for j in range(i + 1, min(len(indices), max_connections)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "    elif method == 'hybrid':\n",
        "        # Combination of both methods\n",
        "        # First add account-based edges\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id, indices in account_groups.items():\n",
        "            indices = sorted(list(indices)) # Sort for consistency\n",
        "            if len(indices) > 1:\n",
        "                for i in range(min(len(indices), 3)):  # Limit account connections\n",
        "                    for j in range(i + 1, min(len(indices), 3)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "        # Then add similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            similarities = similarity_matrix[i]\n",
        "            similar_indices = np.argsort(similarities)[::-1][1:4]  # Top 3 similar\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold and not G.has_edge(i, j):\n",
        "                    G.add_edge(i, j, weight=similarities[j])\n",
        "    return G\n",
        "\n",
        "# Construct graph deterministically\n",
        "print(f\"\\nüîÑ Constructing graph with hybrid method...\")\n",
        "final_graph = construct_meaningful_graph(df, X_net_std, method='hybrid', random_state=42)\n",
        "print(f\"‚úÖ Final graph selected: {final_graph.number_of_nodes()} nodes, {final_graph.number_of_edges()} edges\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: CONVERT GRAPH FOR PYG & DEFINE IMPROVED GCN MODEL\n",
        "# ==========================================\n",
        "# Convert NetworkX graph to PyTorch Geometric format with consistent ordering\n",
        "edges = sorted(list(final_graph.edges)) # Sort edges for consistency\n",
        "if len(edges) == 0:\n",
        "    # Create self-loops for all nodes if graph is empty\n",
        "    edge_index = torch.arange(len(df), dtype=torch.long).repeat(2, 1)\n",
        "else:\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    # Ensure edge_index is of shape [2, num_edges]\n",
        "    assert edge_index.shape[0] == 2, f\"Edge index must have shape [2, num_edges], got {edge_index.shape}\"\n",
        "\n",
        "# CRITICAL: Validate edge indices\n",
        "max_node_idx = len(df) - 1\n",
        "if edge_index.numel() > 0:  # Check if edge_index is not empty\n",
        "    if edge_index.max().item() > max_node_idx or edge_index.min().item() < 0:\n",
        "        raise ValueError(f\"Invalid edge indices! Max: {edge_index.max().item()}, Min: {edge_index.min().item()}, Expected range: [0, {max_node_idx}]\")\n",
        "\n",
        "x = torch.tensor(X_net_std, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Add self-loops deterministically\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "print(f\"PyTorch Geometric Data: {data}\")\n",
        "\n",
        "# Define improved GCN model with deterministic initialization\n",
        "class ImprovedGCN(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, out_channels=128, dropout=0.3):\n",
        "        super(ImprovedGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x shape: [num_nodes, in_channels]\n",
        "        # edge_index shape: [2, num_edges]\n",
        "\n",
        "        # Debug inputs\n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            raise ValueError(\"Input features contain NaN or Inf values!\")\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        # x shape: [num_nodes, out_channels]\n",
        "        return x\n",
        "\n",
        "# Initialize and get GCN embeddings (Pre-compute GCN embeddings)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move to CPU first for debugging\n",
        "print(\"üîç Running GCN on CPU first for debugging...\")\n",
        "gcn_model = ImprovedGCN()\n",
        "data_cpu = data.to('cpu')\n",
        "\n",
        "# Get GCN embeddings (from untrained model - used as pre-computed features)\n",
        "gcn_model.eval()\n",
        "with torch.no_grad():\n",
        "    try:\n",
        "        gcn_embeddings = gcn_model(data_cpu.x, data_cpu.edge_index) # Pass features and edge_index\n",
        "        print(\"‚úÖ GCN forward pass successful on CPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GCN forward pass failed on CPU: {e}\")\n",
        "        raise\n",
        "\n",
        "# Now move to GPU if available\n",
        "if device.type == 'cuda':\n",
        "    print(\"üîÑ Moving to GPU...\")\n",
        "    try:\n",
        "        gcn_model = gcn_model.to(device)\n",
        "        data = data.to(device)\n",
        "\n",
        "        gcn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            gcn_embeddings = gcn_model(data.x, data.edge_index)\n",
        "            print(\"‚úÖ GCN forward pass successful on GPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GCN forward pass failed on GPU, using CPU results: {e}\")\n",
        "        gcn_model = gcn_model.to('cpu')\n",
        "        data = data.to('cpu')\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "print(f\"GCN embeddings shape: {gcn_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(gcn_embeddings).any().item()}\")\n",
        "\n",
        "# Save GCN model and embeddings\n",
        "torch.save(gcn_model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/improved_gcn_model.pth')\n",
        "torch.save(gcn_embeddings.cpu(), '/content/drive/MyDrive/Projects/Hayat/gcn_embeddings.pt')\n",
        "print(\"‚úÖ GCN model and embeddings saved\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7: BERT MODEL AND TEXT BRANCH\n",
        "# ==========================================\n",
        "# Define improved attention mechanism with deterministic initialization\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, seq_len, hidden_dim = embeddings.shape\n",
        "        # Generate Q, K, V\n",
        "        Q = self.query(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.key(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = self.value(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        # Transpose for attention computation\n",
        "        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        # Apply attention\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        # Reshape and output\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
        "        output = self.output(context)\n",
        "        # Global average pooling\n",
        "        output = output.mean(dim=1)  # (batch_size, hidden_dim)\n",
        "        return output, attention_weights\n",
        "\n",
        "# Initialize BERT and attention\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "# Freeze BERT weights (common practice for feature extraction)\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "attention_layer = MultiHeadAttention(hidden_dim=768, num_heads=8).to(device)\n",
        "attention_layer.init_weights() # Ensure deterministic init\n",
        "\n",
        "# Process texts with BERT deterministically\n",
        "batch_size = 16\n",
        "bert_embeddings = []\n",
        "texts = df['Context Post'].fillna(\"\").tolist()\n",
        "print(f\"Processing {len(texts)} texts for initial analysis...\")\n",
        "\n",
        "# Calculate optimal max_length consistently\n",
        "text_lengths = [len(tokenizer.tokenize(text)) for text in texts[:100]] # Sample\n",
        "optimal_max_length = int(np.percentile(text_lengths, 95))\n",
        "print(f\"Optimal max_length: {optimal_max_length}\")\n",
        "\n",
        "# Pre-compute BERT embeddings for SMOTE and analysis (using frozen BERT)\n",
        "try:\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=min(optimal_max_length, 512)\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            context_vector, _ = attention_layer(token_embeddings)\n",
        "            bert_embeddings.append(context_vector.cpu())\n",
        "\n",
        "    bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "    print(\"‚úÖ BERT processing successful\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå BERT processing failed: {e}\")\n",
        "    print(\"Using CPU for BERT processing...\")\n",
        "    # Fallback to CPU\n",
        "    bert_model = bert_model.to('cpu')\n",
        "    attention_layer = attention_layer.to('cpu')\n",
        "    device_bert = torch.device('cpu')\n",
        "\n",
        "    bert_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=min(optimal_max_length, 512)\n",
        "        )\n",
        "        inputs = {k: v.to(device_bert) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            context_vector, _ = attention_layer(token_embeddings)\n",
        "            bert_embeddings.append(context_vector.cpu())\n",
        "\n",
        "    bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "    print(\"‚úÖ BERT processing successful on CPU\")\n",
        "\n",
        "print(f\"Pre-computed BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(bert_embeddings).any().item()}\")\n",
        "\n",
        "# Save BERT embeddings\n",
        "torch.save(bert_embeddings, '/content/drive/MyDrive/Projects/Hayat/bert_embeddings_improved.pt')\n",
        "print(\"‚úÖ BERT embeddings saved\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 8: TWO-BRANCH ARCHITECTURE IMPLEMENTATION (Using pre-computed embeddings)\n",
        "# ==========================================\n",
        "class TwoBranchClassifier(nn.Module):\n",
        "    def __init__(self, gcn_dim=128, bert_dim=768, hidden_dim=256, num_classes=2, dropout=0.4):\n",
        "        super(TwoBranchClassifier, self).__init__()\n",
        "\n",
        "        # --- Social Branch (uses pre-computed GCN features) ---\n",
        "        self.social_branch = nn.Sequential(\n",
        "            nn.Linear(gcn_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        # --- Text Branch (uses pre-computed BERT features) ---\n",
        "        self.text_branch = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        # --- Fusion layer with attention ---\n",
        "        self.fusion_attention = nn.Sequential(\n",
        "            nn.Linear(num_classes * 2, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 4, 2),  # Attention weights for social and text\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Optional final classifier layer\n",
        "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
        "\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, gcn_features, bert_features):\n",
        "        # Debug inputs\n",
        "        if torch.isnan(gcn_features).any() or torch.isinf(gcn_features).any():\n",
        "            raise ValueError(\"GCN features contain NaN or Inf values!\")\n",
        "        if torch.isnan(bert_features).any() or torch.isinf(bert_features).any():\n",
        "            raise ValueError(\"BERT features contain NaN or Inf values!\")\n",
        "\n",
        "        # Get predictions from both branches\n",
        "        social_out = self.social_branch(gcn_features) # [batch_size, num_classes]\n",
        "        text_out = self.text_branch(bert_features)   # [batch_size, num_classes]\n",
        "\n",
        "        # Concatenate for attention computation\n",
        "        combined = torch.cat([social_out, text_out], dim=1) # [batch_size, num_classes*2]\n",
        "        attention_weights = self.fusion_attention(combined) # [batch_size, 2]\n",
        "\n",
        "        weighted_social = social_out * attention_weights[:, 0:1]\n",
        "        weighted_text = text_out * attention_weights[:, 1:2]\n",
        "\n",
        "        final_features = weighted_social + weighted_text\n",
        "        final_out = self.final_classifier(final_features) # [batch_size, num_classes]\n",
        "\n",
        "        return final_out, social_out, text_out, attention_weights\n",
        "\n",
        "# Initialize the model with deterministic weights\n",
        "two_branch_model = TwoBranchClassifier().to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in two_branch_model.parameters()):,}\")\n",
        "print(\"‚úÖ Two-Branch Classifier (using pre-computed embeddings) defined\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 9: DATA PREPARATION AND SMOTE (Using pre-computed embeddings) - FIXED\n",
        "# ==========================================\n",
        "# Address class imbalance with SMOTE (deterministic)\n",
        "# Use pre-computed GCN and BERT embeddings for SMOTE\n",
        "print(f\"\\nOriginal class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# CRITICAL: Validate labels again before SMOTE\n",
        "if not validate_labels(y, num_classes=2):\n",
        "    raise ValueError(\"Labels contain invalid values before SMOTE!\")\n",
        "\n",
        "# Prepare data for SMOTE: Use pre-computed GCN and BERT embeddings\n",
        "X_for_smote = np.concatenate([gcn_embeddings.cpu().numpy(), bert_embeddings.numpy()], axis=1)\n",
        "print(f\"Combined embeddings for SMOTE shape: {X_for_smote.shape}\")\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_for_smote, y)\n",
        "\n",
        "# CRITICAL: Validate resampled labels\n",
        "print(f\"After SMOTE - class distribution: {np.bincount(y_resampled)}\")\n",
        "if not validate_labels(y_resampled, num_classes=2):\n",
        "    raise ValueError(\"Resampled labels contain invalid values!\")\n",
        "\n",
        "# Split resampled data back into components\n",
        "X_gcn_resampled = X_resampled[:, :gcn_embeddings.shape[1]] # First part: GCN embeddings\n",
        "X_bert_resampled = X_resampled[:, gcn_embeddings.shape[1]:] # Second part: BERT embeddings\n",
        "\n",
        "print(f\"Resampled GCN embeddings shape: {X_gcn_resampled.shape}\")\n",
        "print(f\"Resampled BERT embeddings shape: {X_bert_resampled.shape}\")\n",
        "\n",
        "# Train-test split (deterministic)\n",
        "X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test = train_test_split(\n",
        "    X_gcn_resampled, X_bert_resampled, y_resampled,\n",
        "    test_size=0.2, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# CRITICAL: Validate split labels\n",
        "print(f\"Training set size: {len(X_gcn_train)}\")\n",
        "print(f\"Test set size: {len(X_gcn_test)}\")\n",
        "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "if not validate_labels(y_train, num_classes=2) or not validate_labels(y_test, num_classes=2):\n",
        "    raise ValueError(\"Train/test labels contain invalid values!\")\n",
        "\n",
        "# Create DataLoaders using TensorDataset (no custom collate needed)\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Initialize worker with deterministic seed\"\"\"\n",
        "    np.random.seed(42 + worker_id)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_gcn_train, dtype=torch.float),\n",
        "                              torch.tensor(X_bert_train, dtype=torch.float),\n",
        "                              torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_gcn_test, dtype=torch.float),\n",
        "                             torch.tensor(X_bert_test, dtype=torch.float),\n",
        "                             torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          generator=torch.Generator().manual_seed(42),\n",
        "                          worker_init_fn=worker_init_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                         worker_init_fn=worker_init_fn)\n",
        "\n",
        "print(\"‚úÖ DataLoaders created using pre-computed embeddings\")\n",
        "\n",
        "# Debug first batch\n",
        "print(\"\\nüîç Debugging first batch...\")\n",
        "for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "    debug_tensor(batch_gcn, \"batch_gcn\")\n",
        "    debug_tensor(batch_bert, \"batch_bert\")\n",
        "    debug_tensor(batch_y, \"batch_y\")\n",
        "    break\n",
        "\n",
        "# ==========================================\n",
        "# STEP 10: MODEL TRAINING WITH COMPREHENSIVE MONITORING (FIXED)\n",
        "# ==========================================\n",
        "def train_model(model, train_loader, test_loader, num_epochs=50, learning_rate=0.001):\n",
        "    \"\"\"Enhanced training function with comprehensive monitoring and CUDA error prevention\"\"\"\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'attention_weights': [],\n",
        "        'social_acc': [], # Accuracy of social branch *output* on training data\n",
        "        'text_acc': [],   # Accuracy of text branch *output* on training data\n",
        "        'learning_rate': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        social_correct = 0\n",
        "        text_correct = 0\n",
        "        epoch_attention_weights = []\n",
        "\n",
        "        try:\n",
        "            for batch_idx, (batch_gcn, batch_bert, batch_y) in enumerate(train_loader):\n",
        "                # CRITICAL: Move to device and validate\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "\n",
        "                # Debug batch on first iteration\n",
        "                if epoch == 0 and batch_idx == 0:\n",
        "                    print(f\"üîç First batch debug:\")\n",
        "                    debug_tensor(batch_gcn, \"batch_gcn\")\n",
        "                    debug_tensor(batch_bert, \"batch_bert\")\n",
        "                    debug_tensor(batch_y, \"batch_y\")\n",
        "\n",
        "                # CRITICAL: Validate labels are in correct range\n",
        "                if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                    raise ValueError(f\"Invalid labels in batch: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: Pass pre-computed embeddings\n",
        "                try:\n",
        "                    final_out, social_out, text_out, attention_weights = model(\n",
        "                        batch_gcn, batch_bert\n",
        "                    )\n",
        "\n",
        "                    # Debug outputs on first iteration\n",
        "                    if epoch == 0 and batch_idx == 0:\n",
        "                        debug_tensor(final_out, \"final_out\")\n",
        "                        debug_tensor(social_out, \"social_out\")\n",
        "                        debug_tensor(text_out, \"text_out\")\n",
        "                        debug_tensor(attention_weights, \"attention_weights\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Forward pass failed at batch {batch_idx}: {e}\")\n",
        "                    print(f\"Batch shapes: GCN={batch_gcn.shape}, BERT={batch_bert.shape}, Y={batch_y.shape}\")\n",
        "                    raise\n",
        "\n",
        "                # Calculate loss with error checking\n",
        "                try:\n",
        "                    loss = criterion(final_out, batch_y)\n",
        "\n",
        "                    # Check for invalid loss\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        raise ValueError(f\"Invalid loss detected: {loss.item()}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Loss calculation failed: {e}\")\n",
        "                    print(f\"final_out shape: {final_out.shape}, batch_y shape: {batch_y.shape}\")\n",
        "                    print(f\"final_out min/max: {final_out.min().item()}/{final_out.max().item()}\")\n",
        "                    print(f\"batch_y unique: {torch.unique(batch_y).cpu().numpy()}\")\n",
        "                    raise\n",
        "\n",
        "                # Backward pass with gradient clipping\n",
        "                try:\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Backward pass failed: {e}\")\n",
        "                    raise\n",
        "\n",
        "                # Statistics\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                _, social_pred = torch.max(social_out.data, 1) # Accuracy of social branch output\n",
        "                _, text_pred = torch.max(text_out.data, 1)     # Accuracy of text branch output\n",
        "\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "                social_correct += (social_pred == batch_y).sum().item()\n",
        "                text_correct += (text_pred == batch_y).sum().item()\n",
        "\n",
        "                epoch_attention_weights.append(attention_weights.cpu().detach().numpy())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed at epoch {epoch}: {e}\")\n",
        "            # Save current state for debugging\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/Projects/Hayat/debug_model_epoch_{epoch}.pth')\n",
        "            raise\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                    batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "\n",
        "                    # Validate labels\n",
        "                    if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                        raise ValueError(f\"Invalid validation labels: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "\n",
        "                    final_out, _, _, _ = model(batch_gcn, batch_bert)\n",
        "                    loss = criterion(final_out, batch_y)\n",
        "\n",
        "                    # Check for invalid loss\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        raise ValueError(f\"Invalid validation loss: {loss.item()}\")\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(final_out.data, 1)\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Validation failed at epoch {epoch}: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        val_loss_avg = val_loss / len(test_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        social_acc = 100 * social_correct / train_total # Accuracy of social branch *prediction* on training data\n",
        "        text_acc = 100 * text_correct / train_total     # Accuracy of text branch *prediction* on training data\n",
        "\n",
        "        # Concatenate attention weights for the epoch\n",
        "        if epoch_attention_weights:\n",
        "             concatenated_attention_weights = np.concatenate(epoch_attention_weights, axis=0)\n",
        "        else:\n",
        "             concatenated_attention_weights = np.array([])\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss_avg)\n",
        "        history['val_loss'].append(val_loss_avg)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['social_acc'].append(social_acc)\n",
        "        history['text_acc'].append(text_acc)\n",
        "        if concatenated_attention_weights.size > 0:\n",
        "            history['attention_weights'].append(concatenated_attention_weights)\n",
        "        else:\n",
        "            history['attention_weights'].append(np.array([[0.5, 0.5]] * len(train_loader))) # Placeholder\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss_avg)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        history['learning_rate'].append(current_lr)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/best_two_branch_model_fixed.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "            print(f'  Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}')\n",
        "            print(f'  Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "            print(f'  Social Acc (Train): {social_acc:.2f}%, Text Acc (Train): {text_acc:.2f}%')\n",
        "            print(f'  LR: {current_lr:.6f}')\n",
        "            if concatenated_attention_weights.size > 0:\n",
        "                avg_social_attn = np.mean(concatenated_attention_weights[:, 0])\n",
        "                avg_text_attn = np.mean(concatenated_attention_weights[:, 1])\n",
        "                print(f'  Avg Attention - Social: {avg_social_attn:.3f}, Text: {avg_text_attn:.3f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "    return history\n",
        "\n",
        "# Test model forward pass before training\n",
        "print(\"\\nüîç Testing model forward pass before training...\")\n",
        "try:\n",
        "    model_test = TwoBranchClassifier().to(device)\n",
        "    # Get a small batch for testing\n",
        "    for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "        batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "\n",
        "        # Test forward pass\n",
        "        with torch.no_grad():\n",
        "            final_out, social_out, text_out, attention_weights = model_test(batch_gcn, batch_bert)\n",
        "            print(f\"‚úÖ Forward pass test successful!\")\n",
        "            print(f\"Output shapes: final={final_out.shape}, social={social_out.shape}, text={text_out.shape}, attention={attention_weights.shape}\")\n",
        "        break\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Forward pass test failed: {e}\")\n",
        "    print(\"Switching to CPU for training...\")\n",
        "    device = torch.device('cpu')\n",
        "    two_branch_model = two_branch_model.to(device)\n",
        "\n",
        "# Train the model (using pre-computed GCN embeddings)\n",
        "print(\"\\nüöÄ Starting model training using PRE-COMPUTED GCN embeddings...\")\n",
        "try:\n",
        "    history = train_model(two_branch_model, train_loader, test_loader, num_epochs=50, learning_rate=0.001)\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"Attempting training with reduced complexity...\")\n",
        "    # Try with fewer epochs and simpler settings\n",
        "    try:\n",
        "        history = train_model(two_branch_model, train_loader, test_loader, num_epochs=10, learning_rate=0.01)\n",
        "        print(\"‚úÖ Reduced training completed!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Reduced training also failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "# ==========================================\n",
        "# STEP 11: COMPREHENSIVE EVALUATION AND ANALYSIS (FIXED)\n",
        "# ==========================================\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Comprehensive model evaluation with error handling\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_social_preds = []\n",
        "    all_text_preds = []\n",
        "    all_attention_weights = []\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "\n",
        "                # Validate labels\n",
        "                if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                    raise ValueError(f\"Invalid evaluation labels: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "\n",
        "                final_out, social_out, text_out, attention_weights = model(\n",
        "                    batch_gcn, batch_bert\n",
        "                )\n",
        "\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                _, social_pred = torch.max(social_out.data, 1)\n",
        "                _, text_pred = torch.max(text_out.data, 1)\n",
        "\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(batch_y.cpu().numpy())\n",
        "                all_social_preds.extend(social_pred.cpu().numpy())\n",
        "                all_text_preds.extend(text_pred.cpu().numpy())\n",
        "                all_attention_weights.extend(attention_weights.cpu().numpy())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    return (np.array(all_predictions), np.array(all_labels),\n",
        "            np.array(all_social_preds), np.array(all_text_preds),\n",
        "            np.array(all_attention_weights))\n",
        "\n",
        "# Load best model and evaluate\n",
        "print(\"\\nüîÑ Loading best model for evaluation...\")\n",
        "try:\n",
        "    two_branch_model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/best_two_branch_model_fixed.pth'))\n",
        "    predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device)\n",
        "    print(\"‚úÖ Evaluation completed successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Evaluation failed: {e}\")\n",
        "    print(\"Using current model state for evaluation...\")\n",
        "    predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device)\n",
        "\n",
        "# Final validation of results\n",
        "if not validate_labels(predictions, num_classes=2) or not validate_labels(labels, num_classes=2):\n",
        "    raise ValueError(\"Final predictions or labels contain invalid values!\")\n",
        "\n",
        "# Calculate metrics\n",
        "final_acc = accuracy_score(labels, predictions)\n",
        "social_acc = accuracy_score(labels, social_preds) # Accuracy of social branch *prediction* on test set\n",
        "text_acc = accuracy_score(labels, text_preds)     # Accuracy of text branch *prediction* on test set\n",
        "final_f1 = f1_score(labels, predictions, average='weighted')\n",
        "social_f1 = f1_score(labels, social_preds, average='weighted')\n",
        "text_f1 = f1_score(labels, text_preds, average='weighted')\n",
        "\n",
        "print(f\"\\nüéØ Final Results:\")\n",
        "print(f\"Combined Model Accuracy: {final_acc:.4f}, F1 Score: {final_f1:.4f}\")\n",
        "print(f\"Social Branch Accuracy (Test): {social_acc:.4f}, F1 Score: {social_f1:.4f}\")\n",
        "print(f\"Text Branch Accuracy (Test): {text_acc:.4f}, F1 Score: {text_f1:.4f}\")\n",
        "print(f\"Improvement over Social: {final_acc - social_acc:.4f}\")\n",
        "print(f\"Improvement over Text: {final_acc - text_acc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nüìä Detailed Classification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=['Mostly True', 'Others']))\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n============================================================\")\n",
        "print(f\"üéâ FINAL SUMMARY AND INSIGHTS (Using pre-computed embeddings)\")\n",
        "print(f\"============================================================\")\n",
        "print(f\"üìà Performance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Combined Model Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Social Branch Accuracy (Test): {social_acc:.4f} ({social_acc*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Text Branch Accuracy (Test): {text_acc:.4f} ({text_acc*100:.2f}%)\")\n",
        "\n",
        "if attention_weights.size > 0:\n",
        "    avg_social_attn = np.mean(attention_weights[:, 0])\n",
        "    avg_text_attn = np.mean(attention_weights[:, 1])\n",
        "    print(f\"üîç Model Analysis:\")\n",
        "    print(f\"  ‚Ä¢ Average Social Attention: {avg_social_attn:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Average Text Attention: {avg_text_attn:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Attention Correlation: {np.corrcoef(attention_weights[:, 0], attention_weights[:, 1])[0, 1]:.3f}\")\n",
        "    print(f\"üí° Key Insights:\")\n",
        "    print(f\"  ‚úÖ The combined model outperforms individual branches\")\n",
        "    if avg_social_attn > avg_text_attn:\n",
        "        print(f\"  üìä Social features are more important on average\")\n",
        "    else:\n",
        "        print(f\"  üìù Text features are more important on average\")\n",
        "else:\n",
        "    print(\"üîç Model Analysis: Attention weights not available.\")\n",
        "    print(\"üí° Key Insights:\")\n",
        "    print(\"  ‚úÖ The combined model architecture is working.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT7mR0yRF4nn",
        "outputId": "40712f6c-4e1a-4d02-8113-8d6534741516"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported and seeds set for reproducibility!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset shape: (2282, 13)\n",
            "Columns: ['account_id', 'post_id', 'Category', 'Page', 'Post URL', 'Date Published', 'Post Type', 'Rating', 'Debate', 'share_count', 'reaction_count', 'comment_count', 'Context Post']\n",
            "\n",
            "Dataset Info:\n",
            "Number of samples: 2282\n",
            "Number of features: 13\n",
            "Missing values per column:\n",
            "account_id           0\n",
            "post_id              0\n",
            "Category             0\n",
            "Page                 0\n",
            "Post URL             0\n",
            "Date Published       0\n",
            "Post Type            0\n",
            "Rating               0\n",
            "Debate            1984\n",
            "share_count         70\n",
            "reaction_count       2\n",
            "comment_count        2\n",
            "Context Post         0\n",
            "dtype: int64\n",
            "‚úÖ Network features preprocessed\n",
            "Raw features shape: (2282, 3)\n",
            "Processed features shape: (2282, 3)\n",
            "Standardized features shape: (2282, 3)\n",
            "NaN check: False\n",
            "\n",
            "üîç Analyzing labels...\n",
            "Unique ratings: Rating\n",
            "mostly true                  1669\n",
            "no factual content            264\n",
            "mixture of true and false     245\n",
            "mostly false                  104\n",
            "Name: count, dtype: int64\n",
            "Label distribution: [1669  613]\n",
            "Class 0 (mostly true): 1669 (73.1%)\n",
            "Class 1 (others): 613 (26.9%)\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "‚úÖ Labels validated successfully\n",
            "\n",
            "üîÑ Constructing graph with hybrid method...\n",
            "‚úÖ Final graph selected: 2282 nodes, 4319 edges\n",
            "PyTorch Geometric Data: Data(x=[2282, 3], edge_index=[2, 6601])\n",
            "Using device: cuda\n",
            "üîç Running GCN on CPU first for debugging...\n",
            "‚úÖ GCN forward pass successful on CPU\n",
            "üîÑ Moving to GPU...\n",
            "‚úÖ GCN forward pass successful on GPU\n",
            "GCN embeddings shape: torch.Size([2282, 128])\n",
            "NaN check: False\n",
            "‚úÖ GCN model and embeddings saved\n",
            "Processing 2282 texts for initial analysis...\n",
            "Optimal max_length: 45\n",
            "‚úÖ BERT processing successful\n",
            "Pre-computed BERT embeddings shape: torch.Size([2282, 768])\n",
            "NaN check: False\n",
            "‚úÖ BERT embeddings saved\n",
            "Model parameters: 298,188\n",
            "‚úÖ Two-Branch Classifier (using pre-computed embeddings) defined\n",
            "\n",
            "Original class distribution: [1669  613]\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Combined embeddings for SMOTE shape: (2282, 896)\n",
            "After SMOTE - class distribution: [1669 1669]\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Resampled GCN embeddings shape: (3338, 128)\n",
            "Resampled BERT embeddings shape: (3338, 768)\n",
            "Training set size: 2670\n",
            "Test set size: 668\n",
            "Train labels distribution: [1335 1335]\n",
            "Test labels distribution: [334 334]\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "‚úÖ DataLoaders created using pre-computed embeddings\n",
            "\n",
            "üîç Debugging first batch...\n",
            "Debug batch_gcn:\n",
            "  Shape: torch.Size([32, 128])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -1.2428632974624634\n",
            "  Max: 1.3140901327133179\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_bert:\n",
            "  Shape: torch.Size([32, 768])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -1.2282860279083252\n",
            "  Max: 1.298063039779663\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_y:\n",
            "  Shape: torch.Size([32])\n",
            "  Dtype: torch.int64\n",
            "  Device: cpu\n",
            "  Min: 0\n",
            "  Max: 1\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "  Unique values: [0 1]\n",
            "\n",
            "üîç Testing model forward pass before training...\n",
            "‚úÖ Forward pass test successful!\n",
            "Output shapes: final=torch.Size([32, 2]), social=torch.Size([32, 2]), text=torch.Size([32, 2]), attention=torch.Size([32, 2])\n",
            "\n",
            "üöÄ Starting model training using PRE-COMPUTED GCN embeddings...\n",
            "üîç First batch debug:\n",
            "Debug batch_gcn:\n",
            "  Shape: torch.Size([32, 128])\n",
            "  Dtype: torch.float32\n",
            "  Device: cuda:0\n",
            "  Min: -1.349294662475586\n",
            "  Max: 0.7967612147331238\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_bert:\n",
            "  Shape: torch.Size([32, 768])\n",
            "  Dtype: torch.float32\n",
            "  Device: cuda:0\n",
            "  Min: -1.184198021888733\n",
            "  Max: 1.1163891553878784\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_y:\n",
            "  Shape: torch.Size([32])\n",
            "  Dtype: torch.int64\n",
            "  Device: cuda:0\n",
            "  Min: 0\n",
            "  Max: 1\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "  Unique values: [0 1]\n",
            "Debug final_out:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cuda:0\n",
            "  Min: -3.0763654708862305\n",
            "  Max: 2.7452547550201416\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug social_out:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cuda:0\n",
            "  Min: -2.851959466934204\n",
            "  Max: 5.796411037445068\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug text_out:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cuda:0\n",
            "  Min: -6.04769229888916\n",
            "  Max: 3.4224820137023926\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug attention_weights:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cuda:0\n",
            "  Min: 0.2030828446149826\n",
            "  Max: 0.7969171404838562\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Epoch [5/50]\n",
            "  Train Loss: 0.4194, Val Loss: 0.4304\n",
            "  Train Acc: 80.67%, Val Acc: 78.29%\n",
            "  Social Acc (Train): 31.31%, Text Acc (Train): 29.85%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.329, Text: 0.671\n",
            "Epoch [10/50]\n",
            "  Train Loss: 0.3830, Val Loss: 0.4148\n",
            "  Train Acc: 82.55%, Val Acc: 80.84%\n",
            "  Social Acc (Train): 38.99%, Text Acc (Train): 29.59%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.343, Text: 0.657\n",
            "Epoch [15/50]\n",
            "  Train Loss: 0.3726, Val Loss: 0.4140\n",
            "  Train Acc: 83.78%, Val Acc: 81.44%\n",
            "  Social Acc (Train): 37.79%, Text Acc (Train): 29.40%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.365, Text: 0.635\n",
            "Epoch [20/50]\n",
            "  Train Loss: 0.3526, Val Loss: 0.4565\n",
            "  Train Acc: 84.04%, Val Acc: 81.44%\n",
            "  Social Acc (Train): 38.43%, Text Acc (Train): 29.33%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.333, Text: 0.667\n",
            "Early stopping triggered at epoch 22\n",
            "‚úÖ Training completed successfully!\n",
            "\n",
            "üîÑ Loading best model for evaluation...\n",
            "‚úÖ Evaluation completed successfully!\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "\n",
            "üéØ Final Results:\n",
            "Combined Model Accuracy: 0.8234, F1 Score: 0.8217\n",
            "Social Branch Accuracy (Test): 0.3877, F1 Score: 0.3841\n",
            "Text Branch Accuracy (Test): 0.2904, F1 Score: 0.2790\n",
            "Improvement over Social: 0.4356\n",
            "Improvement over Text: 0.5329\n",
            "\n",
            "üìä Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Mostly True       0.90      0.73      0.80       334\n",
            "      Others       0.77      0.92      0.84       334\n",
            "\n",
            "    accuracy                           0.82       668\n",
            "   macro avg       0.84      0.82      0.82       668\n",
            "weighted avg       0.84      0.82      0.82       668\n",
            "\n",
            "\n",
            "============================================================\n",
            "üéâ FINAL SUMMARY AND INSIGHTS (Using pre-computed embeddings)\n",
            "============================================================\n",
            "üìà Performance Metrics:\n",
            "  ‚Ä¢ Combined Model Accuracy: 0.8234 (82.34%)\n",
            "  ‚Ä¢ Social Branch Accuracy (Test): 0.3877 (38.77%)\n",
            "  ‚Ä¢ Text Branch Accuracy (Test): 0.2904 (29.04%)\n",
            "üîç Model Analysis:\n",
            "  ‚Ä¢ Average Social Attention: 0.368\n",
            "  ‚Ä¢ Average Text Attention: 0.632\n",
            "  ‚Ä¢ Attention Correlation: -1.000\n",
            "üí° Key Insights:\n",
            "  ‚úÖ The combined model outperforms individual branches\n",
            "  üìù Text features are more important on average\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 1: SETUP AND IMPORTS & DETERMISTIC SETUP\n",
        "# ==========================================\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "from scipy import io as sio\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "sns.set_palette(\"dark\")\n",
        "\n",
        "# ==========================================\n",
        "# ENHANCED COMPREHENSIVE REPRODUCIBILITY SETUP\n",
        "# ==========================================\n",
        "\n",
        "# CRITICAL: Disable CuDNN benchmarking for full determinism (may slow down training)\n",
        "# This is the most important step for reproducibility on GPU\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# Enable deterministic CuDNN operations (might impact performance)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "    \"\"\"Set all possible seeds for maximum reproducibility\"\"\"\n",
        "    # Python random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # PyTorch CPU and GPU\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # For multi-GPU setups\n",
        "    # Environment variables for additional determinism\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Set CuDNN deterministic flag (redundant with global setting but explicit)\n",
        "    # torch.backends.cudnn.deterministic = True # Set globally above\n",
        "    # torch.backends.cudnn.benchmark = False # Set globally above\n",
        "\n",
        "# Apply comprehensive seed setting\n",
        "set_all_seeds(42)\n",
        "\n",
        "# CRITICAL: Set worker initialization function for DataLoader reproducibility\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Initialize worker with deterministic seed\"\"\"\n",
        "    worker_seed = 42 + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "    torch.manual_seed(worker_seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(worker_seed)\n",
        "        torch.cuda.manual_seed_all(worker_seed)\n",
        "    # Note: Setting os.environ['PYTHONHASHSEED'] here might not be effective for all worker processes\n",
        "\n",
        "print(\"‚úÖ All libraries imported and seeds set for enhanced reproducibility!\")\n",
        "\n",
        "# ==========================================\n",
        "# DEBUG HELPER FUNCTIONS (No changes)\n",
        "# ==========================================\n",
        "def debug_tensor(tensor, name):\n",
        "    \"\"\"Debug tensor properties\"\"\"\n",
        "    print(f\"Debug {name}:\")\n",
        "    print(f\"  Shape: {tensor.shape}\")\n",
        "    print(f\"  Dtype: {tensor.dtype}\")\n",
        "    print(f\"  Device: {tensor.device}\")\n",
        "    print(f\"  Min: {tensor.min().item()}\")\n",
        "    print(f\"  Max: {tensor.max().item()}\")\n",
        "    print(f\"  Has NaN: {torch.isnan(tensor).any().item()}\")\n",
        "    print(f\"  Has Inf: {torch.isinf(tensor).any().item()}\")\n",
        "    if len(tensor.shape) == 1:\n",
        "        print(f\"  Unique values: {torch.unique(tensor).cpu().numpy()}\")\n",
        "\n",
        "def validate_labels(labels, num_classes=2):\n",
        "    \"\"\"Validate labels for classification\"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    print(f\"Label validation:\")\n",
        "    print(f\"  Unique labels: {unique_labels}\")\n",
        "    print(f\"  Expected range: [0, {num_classes-1}]\")\n",
        "    print(f\"  All labels valid: {all(0 <= label < num_classes for label in unique_labels)}\")\n",
        "    return all(0 <= label < num_classes for label in unique_labels)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: DATA LOADING AND INITIAL EXPLORATION (No changes)\n",
        "# ==========================================\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "# IMPORTANT: Sort dataframe to ensure consistent ordering\n",
        "df = df.sort_values(['account_id', 'post_id']).reset_index(drop=True)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "# Basic statistics\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"Number of samples: {len(df)}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "print(f\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: NETWORK FEATURES PREPROCESSING (No changes)\n",
        "# ==========================================\n",
        "def preprocess_network_features(df, random_state=42):\n",
        "    \"\"\"Enhanced preprocessing with outlier handling and normalization\"\"\"\n",
        "    # Extract network features\n",
        "    network_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "    features = df[network_cols].copy()\n",
        "    # Handle missing values with consistent median\n",
        "    features = features.fillna(features.median())\n",
        "    # Log transform to handle skewness (add 1 to avoid log(0))\n",
        "    features_log = np.log1p(features)\n",
        "    # Deterministic outlier capping\n",
        "    for col in features_log.columns:\n",
        "        q99 = features_log[col].quantile(0.99)\n",
        "        features_log[col] = features_log[col].clip(upper=q99)\n",
        "    return features_log.values, features.values\n",
        "\n",
        "# Preprocess network features deterministically\n",
        "network_features_processed, network_features_raw = preprocess_network_features(df, random_state=42)\n",
        "print(\"‚úÖ Network features preprocessed\")\n",
        "print(f\"Raw features shape: {network_features_raw.shape}\")\n",
        "print(f\"Processed features shape: {network_features_processed.shape}\")\n",
        "# Standardize network features with consistent seed\n",
        "scaler = StandardScaler()\n",
        "X_net_std = scaler.fit_transform(network_features_processed)\n",
        "print(f\"Standardized features shape: {X_net_std.shape}\")\n",
        "print(f\"NaN check: {np.isnan(X_net_std).any()}\")\n",
        "# Save for later use\n",
        "sio.savemat('/content/drive/MyDrive/Projects/Hayat/network_processed.mat', {\n",
        "    'X_net_std': X_net_std,\n",
        "    'scaler_mean': scaler.mean_,\n",
        "    'scaler_scale': scaler.scale_\n",
        "})\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: LABEL PREPARATION AND ANALYSIS (FIXED) (No changes)\n",
        "# ==========================================\n",
        "# Prepare labels (binary classification) consistently\n",
        "print(\"\\nüîç Analyzing labels...\")\n",
        "print(f\"Unique ratings: {df['Rating'].value_counts()}\")\n",
        "# Create binary labels - CRITICAL FIX\n",
        "labels = df['Rating'].apply(lambda x: 0 if x == 'mostly true' else 1).values\n",
        "y = np.array(labels)\n",
        "# VALIDATE LABELS\n",
        "print(f\"Label distribution: {np.bincount(y)}\")\n",
        "print(f\"Class 0 (mostly true): {np.bincount(y)[0]} ({np.bincount(y)[0]/len(y)*100:.1f}%)\")\n",
        "print(f\"Class 1 (others): {np.bincount(y)[1]} ({np.bincount(y)[1]/len(y)*100:.1f}%)\")\n",
        "# CRITICAL: Validate labels are in correct range\n",
        "if not validate_labels(y, num_classes=2):\n",
        "    raise ValueError(\"Labels contain invalid values! Must be in range [0, 1]\")\n",
        "print(\"‚úÖ Labels validated successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: IMPROVED GRAPH CONSTRUCTION (No changes)\n",
        "# ==========================================\n",
        "def construct_meaningful_graph(df, X_net_std, method='hybrid', similarity_threshold=0.7, max_connections=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Construct graph with meaningful connections\n",
        "    Methods: 'similarity', 'account', 'hybrid'\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state) # Ensure deterministic graph construction\n",
        "    G = nx.Graph()\n",
        "    # Add nodes with features\n",
        "    for idx in range(len(df)):\n",
        "        G.add_node(idx, features=X_net_std[idx])\n",
        "    if method == 'similarity':\n",
        "        # Similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            # Find most similar posts\n",
        "            similarities = similarity_matrix[i]\n",
        "            similar_indices = np.argsort(similarities)[::-1][1:max_connections+1]  # Exclude self\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold:\n",
        "                    G.add_edge(i, j, weight=similarities[j])\n",
        "    elif method == 'account':\n",
        "        # Account-based edges (limited)\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id, indices in account_groups.items():\n",
        "            indices = sorted(list(indices)) # Sort for consistency\n",
        "            if len(indices) > 1:\n",
        "                # Connect only recent posts (limit connections)\n",
        "                for i in range(min(len(indices), max_connections)):\n",
        "                    for j in range(i + 1, min(len(indices), max_connections)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "    elif method == 'hybrid':\n",
        "        # Combination of both methods\n",
        "        # First add account-based edges\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id, indices in account_groups.items():\n",
        "            indices = sorted(list(indices)) # Sort for consistency\n",
        "            if len(indices) > 1:\n",
        "                for i in range(min(len(indices), 3)):  # Limit account connections\n",
        "                    for j in range(i + 1, min(len(indices), 3)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "        # Then add similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            similarities = similarity_matrix[i]\n",
        "            similar_indices = np.argsort(similarities)[::-1][1:4]  # Top 3 similar\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold and not G.has_edge(i, j):\n",
        "                    G.add_edge(i, j, weight=similarities[j])\n",
        "    return G\n",
        "\n",
        "# Construct graph deterministically\n",
        "print(f\"\\nüîÑ Constructing graph with hybrid method...\")\n",
        "final_graph = construct_meaningful_graph(df, X_net_std, method='hybrid', random_state=42)\n",
        "print(f\"‚úÖ Final graph selected: {final_graph.number_of_nodes()} nodes, {final_graph.number_of_edges()} edges\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: CONVERT GRAPH FOR PYG & DEFINE IMPROVED GCN MODEL (No changes)\n",
        "# ==========================================\n",
        "# Convert NetworkX graph to PyTorch Geometric format with consistent ordering\n",
        "edges = sorted(list(final_graph.edges)) # Sort edges for consistency\n",
        "if len(edges) == 0:\n",
        "    # Create self-loops for all nodes if graph is empty\n",
        "    edge_index = torch.arange(len(df), dtype=torch.long).repeat(2, 1)\n",
        "else:\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    # Ensure edge_index is of shape [2, num_edges]\n",
        "    assert edge_index.shape[0] == 2, f\"Edge index must have shape [2, num_edges], got {edge_index.shape}\"\n",
        "\n",
        "# CRITICAL: Validate edge indices\n",
        "max_node_idx = len(df) - 1\n",
        "if edge_index.numel() > 0:  # Check if edge_index is not empty\n",
        "    if edge_index.max().item() > max_node_idx or edge_index.min().item() < 0:\n",
        "        raise ValueError(f\"Invalid edge indices! Max: {edge_index.max().item()}, Min: {edge_index.min().item()}, Expected range: [0, {max_node_idx}]\")\n",
        "\n",
        "x = torch.tensor(X_net_std, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Add self-loops deterministically\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "print(f\"PyTorch Geometric Data: {data}\")\n",
        "\n",
        "# Define improved GCN model with deterministic initialization\n",
        "class ImprovedGCN(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, out_channels=128, dropout=0.3):\n",
        "        super(ImprovedGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x shape: [num_nodes, in_channels]\n",
        "        # edge_index shape: [2, num_edges]\n",
        "        # Debug inputs\n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            raise ValueError(\"Input features contain NaN or Inf values!\")\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        # x shape: [num_nodes, out_channels]\n",
        "        return x\n",
        "\n",
        "# Initialize and get GCN embeddings (Pre-compute GCN embeddings)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move to CPU first for debugging\n",
        "print(\"üîç Running GCN on CPU first for debugging...\")\n",
        "gcn_model = ImprovedGCN()\n",
        "data_cpu = data.to('cpu')\n",
        "\n",
        "# Get GCN embeddings (from untrained model - used as pre-computed features)\n",
        "gcn_model.eval()\n",
        "with torch.no_grad():\n",
        "    try:\n",
        "        gcn_embeddings = gcn_model(data_cpu.x, data_cpu.edge_index) # Pass features and edge_index\n",
        "        print(\"‚úÖ GCN forward pass successful on CPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GCN forward pass failed on CPU: {e}\")\n",
        "        raise\n",
        "\n",
        "# Now move to GPU if available\n",
        "if device.type == 'cuda':\n",
        "    print(\"üîÑ Moving to GPU...\")\n",
        "    try:\n",
        "        gcn_model = gcn_model.to(device)\n",
        "        data = data.to(device)\n",
        "        gcn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            gcn_embeddings = gcn_model(data.x, data.edge_index)\n",
        "            print(\"‚úÖ GCN forward pass successful on GPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GCN forward pass failed on GPU, using CPU results: {e}\")\n",
        "        gcn_model = gcn_model.to('cpu')\n",
        "        data = data.to('cpu')\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "print(f\"GCN embeddings shape: {gcn_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(gcn_embeddings).any().item()}\")\n",
        "\n",
        "# Save GCN model and embeddings\n",
        "torch.save(gcn_model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/improved_gcn_model.pth')\n",
        "torch.save(gcn_embeddings.cpu(), '/content/drive/MyDrive/Projects/Hayat/gcn_embeddings.pt')\n",
        "print(\"‚úÖ GCN model and embeddings saved\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7: BERT MODEL AND TEXT BRANCH (No changes)\n",
        "# ==========================================\n",
        "# Define improved attention mechanism with deterministic initialization\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, seq_len, hidden_dim = embeddings.shape\n",
        "        # Generate Q, K, V\n",
        "        Q = self.query(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.key(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = self.value(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        # Transpose for attention computation\n",
        "        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        # Apply attention\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        # Reshape and output\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
        "        output = self.output(context)\n",
        "        # Global average pooling\n",
        "        output = output.mean(dim=1)  # (batch_size, hidden_dim)\n",
        "        return output, attention_weights\n",
        "\n",
        "# Initialize BERT and attention\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "# Freeze BERT weights (common practice for feature extraction)\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "attention_layer = MultiHeadAttention(hidden_dim=768, num_heads=8).to(device)\n",
        "attention_layer.init_weights() # Ensure deterministic init\n",
        "\n",
        "# Process texts with BERT deterministically\n",
        "batch_size = 16\n",
        "bert_embeddings = []\n",
        "texts = df['Context Post'].fillna(\"\").tolist()\n",
        "print(f\"Processing {len(texts)} texts for initial analysis...\")\n",
        "\n",
        "# Calculate optimal max_length consistently\n",
        "text_lengths = [len(tokenizer.tokenize(text)) for text in texts[:100]] # Sample\n",
        "optimal_max_length = int(np.percentile(text_lengths, 95))\n",
        "print(f\"Optimal max_length: {optimal_max_length}\")\n",
        "\n",
        "# Pre-compute BERT embeddings for SMOTE and analysis (using frozen BERT)\n",
        "try:\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=min(optimal_max_length, 512)\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            context_vector, _ = attention_layer(token_embeddings)\n",
        "            bert_embeddings.append(context_vector.cpu())\n",
        "    bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "    print(\"‚úÖ BERT processing successful\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå BERT processing failed: {e}\")\n",
        "    print(\"Using CPU for BERT processing...\")\n",
        "    # Fallback to CPU\n",
        "    bert_model = bert_model.to('cpu')\n",
        "    attention_layer = attention_layer.to('cpu')\n",
        "    device_bert = torch.device('cpu')\n",
        "    bert_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=min(optimal_max_length, 512)\n",
        "        )\n",
        "        inputs = {k: v.to(device_bert) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            context_vector, _ = attention_layer(token_embeddings)\n",
        "            bert_embeddings.append(context_vector.cpu())\n",
        "    bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "    print(\"‚úÖ BERT processing successful on CPU\")\n",
        "\n",
        "print(f\"Pre-computed BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(bert_embeddings).any().item()}\")\n",
        "\n",
        "# Save BERT embeddings\n",
        "torch.save(bert_embeddings, '/content/drive/MyDrive/Projects/Hayat/bert_embeddings_improved.pt')\n",
        "print(\"‚úÖ BERT embeddings saved\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 8: TWO-BRANCH ARCHITECTURE IMPLEMENTATION (Using pre-computed embeddings) (No changes)\n",
        "# ==========================================\n",
        "class TwoBranchClassifier(nn.Module):\n",
        "    def __init__(self, gcn_dim=128, bert_dim=768, hidden_dim=256, num_classes=2, dropout=0.4):\n",
        "        super(TwoBranchClassifier, self).__init__()\n",
        "        # --- Social Branch (uses pre-computed GCN features) ---\n",
        "        self.social_branch = nn.Sequential(\n",
        "            nn.Linear(gcn_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        # --- Text Branch (uses pre-computed BERT features) ---\n",
        "        self.text_branch = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        # --- Fusion layer with attention ---\n",
        "        self.fusion_attention = nn.Sequential(\n",
        "            nn.Linear(num_classes * 2, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 4, 2),  # Attention weights for social and text\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        # Optional final classifier layer\n",
        "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, gcn_features, bert_features):\n",
        "        # Debug inputs\n",
        "        if torch.isnan(gcn_features).any() or torch.isinf(gcn_features).any():\n",
        "            raise ValueError(\"GCN features contain NaN or Inf values!\")\n",
        "        if torch.isnan(bert_features).any() or torch.isinf(bert_features).any():\n",
        "            raise ValueError(\"BERT features contain NaN or Inf values!\")\n",
        "        # Get predictions from both branches\n",
        "        social_out = self.social_branch(gcn_features) # [batch_size, num_classes]\n",
        "        text_out = self.text_branch(bert_features)   # [batch_size, num_classes]\n",
        "        # Concatenate for attention computation\n",
        "        combined = torch.cat([social_out, text_out], dim=1) # [batch_size, num_classes*2]\n",
        "        attention_weights = self.fusion_attention(combined) # [batch_size, 2]\n",
        "        weighted_social = social_out * attention_weights[:, 0:1]\n",
        "        weighted_text = text_out * attention_weights[:, 1:2]\n",
        "        final_features = weighted_social + weighted_text\n",
        "        final_out = self.final_classifier(final_features) # [batch_size, num_classes]\n",
        "        return final_out, social_out, text_out, attention_weights\n",
        "\n",
        "# Initialize the model with deterministic weights\n",
        "two_branch_model = TwoBranchClassifier().to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in two_branch_model.parameters()):,}\")\n",
        "print(\"‚úÖ Two-Branch Classifier (using pre-computed embeddings) defined\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 9: DATA PREPARATION AND SMOTE (Using pre-computed embeddings) - FIXED (No changes)\n",
        "# ==========================================\n",
        "# Address class imbalance with SMOTE (deterministic)\n",
        "# Use pre-computed GCN and BERT embeddings for SMOTE\n",
        "print(f\"\\nOriginal class distribution: {np.bincount(y)}\")\n",
        "# CRITICAL: Validate labels again before SMOTE\n",
        "if not validate_labels(y, num_classes=2):\n",
        "    raise ValueError(\"Labels contain invalid values before SMOTE!\")\n",
        "\n",
        "# Prepare data for SMOTE: Use pre-computed GCN and BERT embeddings\n",
        "X_for_smote = np.concatenate([gcn_embeddings.cpu().numpy(), bert_embeddings.numpy()], axis=1)\n",
        "print(f\"Combined embeddings for SMOTE shape: {X_for_smote.shape}\")\n",
        "\n",
        "# CRITICAL: Use the global seed for SMOTE\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_for_smote, y)\n",
        "\n",
        "# CRITICAL: Validate resampled labels\n",
        "print(f\"After SMOTE - class distribution: {np.bincount(y_resampled)}\")\n",
        "if not validate_labels(y_resampled, num_classes=2):\n",
        "    raise ValueError(\"Resampled labels contain invalid values!\")\n",
        "\n",
        "# Split resampled data back into components\n",
        "X_gcn_resampled = X_resampled[:, :gcn_embeddings.shape[1]] # First part: GCN embeddings\n",
        "X_bert_resampled = X_resampled[:, gcn_embeddings.shape[1]:] # Second part: BERT embeddings\n",
        "print(f\"Resampled GCN embeddings shape: {X_gcn_resampled.shape}\")\n",
        "print(f\"Resampled BERT embeddings shape: {X_bert_resampled.shape}\")\n",
        "\n",
        "# Train-test split (deterministic)\n",
        "X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test = train_test_split(\n",
        "    X_gcn_resampled, X_bert_resampled, y_resampled,\n",
        "    test_size=0.2, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# CRITICAL: Validate split labels\n",
        "print(f\"Training set size: {len(X_gcn_train)}\")\n",
        "print(f\"Test set size: {len(X_gcn_test)}\")\n",
        "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "if not validate_labels(y_train, num_classes=2) or not validate_labels(y_test, num_classes=2):\n",
        "    raise ValueError(\"Train/test labels contain invalid values!\")\n",
        "\n",
        "# Create DataLoaders using TensorDataset (no custom collate needed)\n",
        "# Use the worker_init_fn defined above\n",
        "train_dataset = TensorDataset(torch.tensor(X_gcn_train, dtype=torch.float),\n",
        "                              torch.tensor(X_bert_train, dtype=torch.float),\n",
        "                              torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_gcn_test, dtype=torch.float),\n",
        "                             torch.tensor(X_bert_test, dtype=torch.float),\n",
        "                             torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "# CRITICAL: Pass worker_init_fn and use the same generator for DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          generator=torch.Generator().manual_seed(42),\n",
        "                          worker_init_fn=worker_init_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                         worker_init_fn=worker_init_fn) # No shuffle for test loader\n",
        "print(\"‚úÖ DataLoaders created using pre-computed embeddings\")\n",
        "\n",
        "# Debug first batch\n",
        "print(\"\\nüîç Debugging first batch...\")\n",
        "for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "    debug_tensor(batch_gcn, \"batch_gcn\")\n",
        "    debug_tensor(batch_bert, \"batch_bert\")\n",
        "    debug_tensor(batch_y, \"batch_y\")\n",
        "    break\n",
        "\n",
        "# ==========================================\n",
        "# STEP 10: MODEL TRAINING WITH COMPREHENSIVE MONITORING (FIXED & ENHANCED)\n",
        "# ==========================================\n",
        "\n",
        "# CRITICAL: Redefine the training function with longer epochs and less aggressive early stopping\n",
        "def train_model(model, train_loader, test_loader, num_epochs=100, learning_rate=0.001): # Increased epochs\n",
        "    \"\"\"Enhanced training function with comprehensive monitoring and CUDA error prevention\"\"\"\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # CRITICAL: Use AdamW for potentially better generalization and stability\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4) # Reduced weight decay\n",
        "    # CRITICAL: Less aggressive scheduler patience\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10) # Increased patience\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'attention_weights': [],\n",
        "        'social_acc': [], # Accuracy of social branch *output* on training data\n",
        "        'text_acc': [],   # Accuracy of text branch *output* on training data\n",
        "        'learning_rate': []\n",
        "    }\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    # CRITICAL: Increased early stopping patience for longer training\n",
        "    early_stopping_patience = 20 # Increased patience\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        social_correct = 0\n",
        "        text_correct = 0\n",
        "        epoch_attention_weights = []\n",
        "        try:\n",
        "            for batch_idx, (batch_gcn, batch_bert, batch_y) in enumerate(train_loader):\n",
        "                # CRITICAL: Move to device and validate\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                # Debug batch on first iteration\n",
        "                if epoch == 0 and batch_idx == 0:\n",
        "                    print(f\"üîç First batch debug:\")\n",
        "                    debug_tensor(batch_gcn, \"batch_gcn\")\n",
        "                    debug_tensor(batch_bert, \"batch_bert\")\n",
        "                    debug_tensor(batch_y, \"batch_y\")\n",
        "                # CRITICAL: Validate labels are in correct range\n",
        "                if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                    raise ValueError(f\"Invalid labels in batch: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "                optimizer.zero_grad()\n",
        "                # Forward pass: Pass pre-computed embeddings\n",
        "                try:\n",
        "                    final_out, social_out, text_out, attention_weights = model(\n",
        "                        batch_gcn, batch_bert\n",
        "                    )\n",
        "                    # Debug outputs on first iteration\n",
        "                    if epoch == 0 and batch_idx == 0:\n",
        "                        debug_tensor(final_out, \"final_out\")\n",
        "                        debug_tensor(social_out, \"social_out\")\n",
        "                        debug_tensor(text_out, \"text_out\")\n",
        "                        debug_tensor(attention_weights, \"attention_weights\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Forward pass failed at batch {batch_idx}: {e}\")\n",
        "                    print(f\"Batch shapes: GCN={batch_gcn.shape}, BERT={batch_bert.shape}, Y={batch_y.shape}\")\n",
        "                    raise\n",
        "                # Calculate loss with error checking\n",
        "                try:\n",
        "                    loss = criterion(final_out, batch_y)\n",
        "                    # Check for invalid loss\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        raise ValueError(f\"Invalid loss detected: {loss.item()}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Loss calculation failed: {e}\")\n",
        "                    print(f\"final_out shape: {final_out.shape}, batch_y shape: {batch_y.shape}\")\n",
        "                    print(f\"final_out min/max: {final_out.min().item()}/{final_out.max().item()}\")\n",
        "                    print(f\"batch_y unique: {torch.unique(batch_y).cpu().numpy()}\")\n",
        "                    raise\n",
        "                # Backward pass with gradient clipping\n",
        "                try:\n",
        "                    loss.backward()\n",
        "                    # CRITICAL: Use slightly higher clip norm\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0) # Increased clip norm\n",
        "                    optimizer.step()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Backward pass failed: {e}\")\n",
        "                    raise\n",
        "                # Statistics\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                _, social_pred = torch.max(social_out.data, 1) # Accuracy of social branch output\n",
        "                _, text_pred = torch.max(text_out.data, 1)     # Accuracy of text branch output\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "                social_correct += (social_pred == batch_y).sum().item()\n",
        "                text_correct += (text_pred == batch_y).sum().item()\n",
        "                epoch_attention_weights.append(attention_weights.cpu().detach().numpy())\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed at epoch {epoch}: {e}\")\n",
        "            # Save current state for debugging\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/Projects/Hayat/debug_model_epoch_{epoch}.pth')\n",
        "            raise\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                    batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                    # Validate labels\n",
        "                    if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                        raise ValueError(f\"Invalid validation labels: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "                    final_out, _, _, _ = model(batch_gcn, batch_bert)\n",
        "                    loss = criterion(final_out, batch_y)\n",
        "                    # Check for invalid loss\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        raise ValueError(f\"Invalid validation loss: {loss.item()}\")\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(final_out.data, 1)\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Validation failed at epoch {epoch}: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        val_loss_avg = val_loss / len(test_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        social_acc = 100 * social_correct / train_total # Accuracy of social branch *prediction* on training data\n",
        "        text_acc = 100 * text_correct / train_total     # Accuracy of text branch *prediction* on training data\n",
        "        # Concatenate attention weights for the epoch\n",
        "        if epoch_attention_weights:\n",
        "             concatenated_attention_weights = np.concatenate(epoch_attention_weights, axis=0)\n",
        "        else:\n",
        "             concatenated_attention_weights = np.array([])\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss_avg)\n",
        "        history['val_loss'].append(val_loss_avg)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['social_acc'].append(social_acc)\n",
        "        history['text_acc'].append(text_acc)\n",
        "        if concatenated_attention_weights.size > 0:\n",
        "            history['attention_weights'].append(concatenated_attention_weights)\n",
        "        else:\n",
        "            history['attention_weights'].append(np.array([[0.5, 0.5]] * len(train_loader))) # Placeholder\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss_avg)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        history['learning_rate'].append(current_lr)\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/best_two_branch_model_fixed.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        # Print progress\n",
        "        # CRITICAL: Print every epoch for closer monitoring\n",
        "        if (epoch + 1) % 1 == 0: # Print every epoch\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "            print(f'  Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}')\n",
        "            print(f'  Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "            print(f'  Social Acc (Train): {social_acc:.2f}%, Text Acc (Train): {text_acc:.2f}%')\n",
        "            print(f'  LR: {current_lr:.6f}')\n",
        "            if concatenated_attention_weights.size > 0:\n",
        "                avg_social_attn = np.mean(concatenated_attention_weights[:, 0])\n",
        "                avg_text_attn = np.mean(concatenated_attention_weights[:, 1])\n",
        "                print(f'  Avg Attention - Social: {avg_social_attn:.3f}, Text: {avg_text_attn:.3f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "            break\n",
        "    return history\n",
        "\n",
        "# Test model forward pass before training\n",
        "print(\"\\nüîç Testing model forward pass before training...\")\n",
        "try:\n",
        "    model_test = TwoBranchClassifier().to(device)\n",
        "    # Get a small batch for testing\n",
        "    for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "        batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "        # Test forward pass\n",
        "        with torch.no_grad():\n",
        "            final_out, social_out, text_out, attention_weights = model_test(batch_gcn, batch_bert)\n",
        "            print(f\"‚úÖ Forward pass test successful!\")\n",
        "            print(f\"Output shapes: final={final_out.shape}, social={social_out.shape}, text={text_out.shape}, attention={attention_weights.shape}\")\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Forward pass test failed: {e}\")\n",
        "    print(\"Switching to CPU for training...\")\n",
        "    device = torch.device('cpu')\n",
        "    two_branch_model = two_branch_model.to(device)\n",
        "\n",
        "# Train the model (using pre-computed GCN embeddings)\n",
        "# CRITICAL: Use the enhanced training function with longer epochs\n",
        "print(\"\\nüöÄ Starting model training using PRE-COMPUTED GCN embeddings (Enhanced Version)...\")\n",
        "try:\n",
        "    # CRITICAL: Use longer training epochs\n",
        "    history = train_model(two_branch_model, train_loader, test_loader, num_epochs=100, learning_rate=0.001) # Increased epochs\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"Attempting training with reduced complexity...\")\n",
        "    # Try with fewer epochs and simpler settings\n",
        "    try:\n",
        "        history = train_model(two_branch_model, train_loader, test_loader, num_epochs=20, learning_rate=0.01) # Fallback shorter run\n",
        "        print(\"‚úÖ Reduced training completed!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Reduced training also failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "# ==========================================\n",
        "# STEP 11: COMPREHENSIVE EVALUATION AND ANALYSIS (FIXED) (No changes)\n",
        "# ==========================================\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Comprehensive model evaluation with error handling\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_social_preds = []\n",
        "    all_text_preds = []\n",
        "    all_attention_weights = []\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                # Validate labels\n",
        "                if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                    raise ValueError(f\"Invalid evaluation labels: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "                final_out, social_out, text_out, attention_weights = model(\n",
        "                    batch_gcn, batch_bert\n",
        "                )\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                _, social_pred = torch.max(social_out.data, 1)\n",
        "                _, text_pred = torch.max(text_out.data, 1)\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(batch_y.cpu().numpy())\n",
        "                all_social_preds.extend(social_pred.cpu().numpy())\n",
        "                all_text_preds.extend(text_pred.cpu().numpy())\n",
        "                all_attention_weights.extend(attention_weights.cpu().numpy())\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation failed: {e}\")\n",
        "        raise\n",
        "    return (np.array(all_predictions), np.array(all_labels),\n",
        "            np.array(all_social_preds), np.array(all_text_preds),\n",
        "            np.array(all_attention_weights))\n",
        "\n",
        "# Load best model and evaluate\n",
        "print(\"\\nüîÑ Loading best model for evaluation...\")\n",
        "try:\n",
        "    two_branch_model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/best_two_branch_model_fixed.pth'))\n",
        "    predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device)\n",
        "    print(\"‚úÖ Evaluation completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Evaluation failed: {e}\")\n",
        "    print(\"Using current model state for evaluation...\")\n",
        "    predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device)\n",
        "\n",
        "# Final validation of results\n",
        "if not validate_labels(predictions, num_classes=2) or not validate_labels(labels, num_classes=2):\n",
        "    raise ValueError(\"Final predictions or labels contain invalid values!\")\n",
        "\n",
        "# Calculate metrics\n",
        "final_acc = accuracy_score(labels, predictions)\n",
        "social_acc = accuracy_score(labels, social_preds) # Accuracy of social branch *prediction* on test set\n",
        "text_acc = accuracy_score(labels, text_preds)     # Accuracy of text branch *prediction* on test set\n",
        "final_f1 = f1_score(labels, predictions, average='weighted')\n",
        "social_f1 = f1_score(labels, social_preds, average='weighted')\n",
        "text_f1 = f1_score(labels, text_preds, average='weighted')\n",
        "print(f\"\\nüéØ Final Results:\")\n",
        "print(f\"Combined Model Accuracy: {final_acc:.4f}, F1 Score: {final_f1:.4f}\")\n",
        "print(f\"Social Branch Accuracy (Test): {social_acc:.4f}, F1 Score: {social_f1:.4f}\")\n",
        "print(f\"Text Branch Accuracy (Test): {text_acc:.4f}, F1 Score: {text_f1:.4f}\")\n",
        "print(f\"Improvement over Social: {final_acc - social_acc:.4f}\")\n",
        "print(f\"Improvement over Text: {final_acc - text_acc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nüìä Detailed Classification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=['Mostly True', 'Others']))\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n============================================================\")\n",
        "print(f\"üéâ FINAL SUMMARY AND INSIGHTS (Using pre-computed embeddings)\")\n",
        "print(f\"============================================================\")\n",
        "print(f\"üìà Performance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Combined Model Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Social Branch Accuracy (Test): {social_acc:.4f} ({social_acc*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Text Branch Accuracy (Test): {text_acc:.4f} ({text_acc*100:.2f}%)\")\n",
        "if attention_weights.size > 0:\n",
        "    avg_social_attn = np.mean(attention_weights[:, 0])\n",
        "    avg_text_attn = np.mean(attention_weights[:, 1])\n",
        "    print(f\"üîç Model Analysis:\")\n",
        "    print(f\"  ‚Ä¢ Average Social Attention: {avg_social_attn:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Average Text Attention: {avg_text_attn:.3f}\")\n",
        "    print(f\"  ‚Ä¢ Attention Correlation: {np.corrcoef(attention_weights[:, 0], attention_weights[:, 1])[0, 1]:.3f}\")\n",
        "    print(f\"üí° Key Insights:\")\n",
        "    print(f\"  ‚úÖ The combined model outperforms individual branches\")\n",
        "    if avg_social_attn > avg_text_attn:\n",
        "        print(f\"  üìä Social features are more important on average\")\n",
        "    else:\n",
        "        print(f\"  üìù Text features are more important on average\")\n",
        "else:\n",
        "    print(\"üîç Model Analysis: Attention weights not available.\")\n",
        "    print(\"üí° Key Insights:\")\n",
        "    print(\"  ‚úÖ The combined model architecture is working.\")\n"
      ],
      "metadata": {
        "id": "TrUgEbF1MjJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}