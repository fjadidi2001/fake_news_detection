{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/fake_news4thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 1: SETUP AND IMPORTS & DETERMINISTIC SETUP\n",
        "# ==========================================\n",
        "import os\n",
        "import shutil # Needed to remove cache files\n",
        "# Install dependencies\n",
        "!pip install torch-geometric imbalanced-learn -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "from scipy import io as sio\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Set style for better plots\n",
        "sns.set_palette(\"dark\")\n",
        "\n",
        "# ==========================================\n",
        "# COMPREHENSIVE REPRODUCIBILITY SETUP\n",
        "# ==========================================\n",
        "def set_all_seeds(seed=42):\n",
        "    \"\"\"Set all possible seeds for reproducibility\"\"\"\n",
        "    # Python random\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Only set these if CUDA is available and working\n",
        "        try:\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "            # CRITICAL FIX: Set additional CUDA flags for full determinism\n",
        "            torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not set CUDNN settings: {e}\")\n",
        "    # Environment variables for additional determinism\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # CRITICAL FIX: Additional environment variables\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "# Apply comprehensive seed setting\n",
        "set_all_seeds(42)\n",
        "print(\"✅ All libraries imported and seeds set for reproducibility!\")\n",
        "\n",
        "# ==========================================\n",
        "# DEBUG HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "def debug_tensor(tensor, name):\n",
        "    \"\"\"Debug tensor properties\"\"\"\n",
        "    print(f\"Debug {name}:\")\n",
        "    print(f\"  Shape: {tensor.shape}\")\n",
        "    print(f\"  Dtype: {tensor.dtype}\")\n",
        "    print(f\"  Device: {tensor.device}\")\n",
        "    print(f\"  Min: {tensor.min().item()}\")\n",
        "    print(f\"  Max: {tensor.max().item()}\")\n",
        "    print(f\"  Has NaN: {torch.isnan(tensor).any().item()}\")\n",
        "    print(f\"  Has Inf: {torch.isinf(tensor).any().item()}\")\n",
        "    if len(tensor.shape) == 1:\n",
        "        print(f\"  Unique values: {torch.unique(tensor).cpu().numpy()}\")\n",
        "\n",
        "def validate_labels(labels, num_classes=2):\n",
        "    \"\"\"Validate labels for classification\"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    print(f\"Label validation:\")\n",
        "    print(f\"  Unique labels: {unique_labels}\")\n",
        "    print(f\"  Expected range: [0, {num_classes-1}]\")\n",
        "    print(f\"  All labels valid: {all(0 <= label < num_classes for label in unique_labels)}\")\n",
        "    return all(0 <= label < num_classes for label in unique_labels)\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: DATA LOADING AND INITIAL EXPLORATION\n",
        "# ==========================================\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# CRITICAL FIX: Force reprocessing by clearing/disabling cache\n",
        "USE_CACHE = False # <--- SET THIS TO False TO FORCE REPROCESSING\n",
        "\n",
        "dataset_cache_path = '/content/drive/MyDrive/Projects/Hayat/dataset_cache.pkl'\n",
        "# Clear cache file if it exists and we want to force reprocessing\n",
        "if not USE_CACHE and os.path.exists(dataset_cache_path):\n",
        "    try:\n",
        "        os.remove(dataset_cache_path)\n",
        "        print(\"🗑️ Cleared dataset cache file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not remove dataset cache: {e}\")\n",
        "\n",
        "if USE_CACHE:\n",
        "    try:\n",
        "        # Try to load cached dataset\n",
        "        import pickle\n",
        "        with open(dataset_cache_path, 'rb') as f:\n",
        "            df = pickle.load(f)\n",
        "        print(\"✅ Loaded cached dataset\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Cache load failed ({e}), loading from CSV...\")\n",
        "        df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "        # IMPORTANT: Sort dataframe to ensure consistent ordering\n",
        "        df = df.sort_values(['account_id', 'post_id']).reset_index(drop=True)\n",
        "        # Cache the processed dataset\n",
        "        try:\n",
        "            with open(dataset_cache_path, 'wb') as f:\n",
        "                pickle.dump(df, f)\n",
        "            print(\"✅ Dataset cached for future runs\")\n",
        "        except Exception as e2:\n",
        "            print(f\"⚠️ Could not cache dataset: {e2}\")\n",
        "else:\n",
        "    print(\"🔄 Loading dataset from CSV (cache disabled)...\")\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "    # IMPORTANT: Sort dataframe to ensure consistent ordering\n",
        "    df = df.sort_values(['account_id', 'post_id']).reset_index(drop=True)\n",
        "    print(\"✅ Dataset loaded from CSV\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "# Basic statistics\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"Number of samples: {len(df)}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "print(f\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: NETWORK FEATURES PREPROCESSING\n",
        "# ==========================================\n",
        "def preprocess_network_features(df, random_state=42):\n",
        "    \"\"\"Enhanced preprocessing with DETERMINISTIC outlier handling\"\"\"\n",
        "    # CRITICAL FIX: Set random state for pandas operations\n",
        "    np.random.seed(random_state)\n",
        "    # Extract network features in consistent order\n",
        "    network_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "    features = df[network_cols].copy()\n",
        "    # CRITICAL FIX: Handle missing values with DETERMINISTIC median\n",
        "    # Calculate medians once and reuse\n",
        "    medians = {}\n",
        "    for col in network_cols:\n",
        "        medians[col] = features[col].median()\n",
        "        features[col] = features[col].fillna(medians[col])\n",
        "    # Log transform to handle skewness (add 1 to avoid log(0))\n",
        "    features_log = np.log1p(features)\n",
        "    # CRITICAL FIX: Deterministic outlier capping with consistent percentiles\n",
        "    for col in features_log.columns:\n",
        "        # Use exactly the same quantile calculation method\n",
        "        q99 = np.percentile(features_log[col].values, 99)\n",
        "        features_log[col] = features_log[col].clip(upper=q99)\n",
        "    return features_log.values, features.values\n",
        "\n",
        "# Preprocess network features deterministically\n",
        "network_features_processed, network_features_raw = preprocess_network_features(df, random_state=42)\n",
        "print(\"✅ Network features preprocessed\")\n",
        "print(f\"Raw features shape: {network_features_raw.shape}\")\n",
        "print(f\"Processed features shape: {network_features_processed.shape}\")\n",
        "# CRITICAL FIX: Use deterministic StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# Set random state internally for sklearn operations\n",
        "set_all_seeds(42)\n",
        "X_net_std = scaler.fit_transform(network_features_processed)\n",
        "print(f\"Standardized features shape: {X_net_std.shape}\")\n",
        "print(f\"NaN check: {np.isnan(X_net_std).any()}\")\n",
        "# Save for later use (can keep saving, but loading is controlled by USE_CACHE)\n",
        "try:\n",
        "    sio.savemat('/content/drive/MyDrive/Projects/Hayat/network_processed.mat', {\n",
        "        'X_net_std': X_net_std,\n",
        "        'scaler_mean': scaler.mean_,\n",
        "        'scaler_scale': scaler.scale_\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not save network features: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: LABEL PREPARATION AND ANALYSIS\n",
        "# ==========================================\n",
        "print(\"\\n🔍 Analyzing labels...\")\n",
        "print(f\"Unique ratings: {df['Rating'].value_counts().sort_index()}\")  # FIXED: sort for consistency\n",
        "# Create binary labels - CRITICAL FIX: More explicit mapping\n",
        "def create_binary_labels(rating_series):\n",
        "    \"\"\"Create binary labels deterministically\"\"\"\n",
        "    labels = []\n",
        "    for rating in rating_series:\n",
        "        if rating == 'mostly true':\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n",
        "    return np.array(labels)\n",
        "y = create_binary_labels(df['Rating'])\n",
        "# VALIDATE LABELS\n",
        "print(f\"Label distribution: {np.bincount(y)}\")\n",
        "print(f\"Class 0 (mostly true): {np.bincount(y)[0]} ({np.bincount(y)[0]/len(y)*100:.1f}%)\")\n",
        "print(f\"Class 1 (others): {np.bincount(y)[1]} ({np.bincount(y)[1]/len(y)*100:.1f}%)\")\n",
        "# CRITICAL: Validate labels are in correct range\n",
        "if not validate_labels(y, num_classes=2):\n",
        "    raise ValueError(\"Labels contain invalid values! Must be in range [0, 1]\")\n",
        "print(\"✅ Labels validated successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: IMPROVED GRAPH CONSTRUCTION\n",
        "# ==========================================\n",
        "def construct_meaningful_graph(df, X_net_std, method='hybrid', similarity_threshold=0.7, max_connections=5, random_state=42):\n",
        "    \"\"\"\n",
        "    FIXED: Construct graph with FULLY deterministic connections\n",
        "    \"\"\"\n",
        "    # CRITICAL FIX: Set seeds at the beginning\n",
        "    np.random.seed(random_state)\n",
        "    torch.manual_seed(random_state)\n",
        "    G = nx.Graph()\n",
        "    # Add nodes with features in deterministic order\n",
        "    for idx in range(len(df)):\n",
        "        G.add_node(idx, features=X_net_std[idx])\n",
        "    if method == 'similarity':\n",
        "        # Similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            # CRITICAL FIX: Use argsort for deterministic ordering\n",
        "            similarities = similarity_matrix[i]\n",
        "            # Get indices sorted by similarity (excluding self)\n",
        "            similar_indices = np.argsort(similarities)[::-1]\n",
        "            similar_indices = similar_indices[similar_indices != i]  # Remove self\n",
        "            similar_indices = similar_indices[:max_connections]  # Limit connections\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold:\n",
        "                    G.add_edge(i, j, weight=float(similarities[j]))\n",
        "    elif method == 'account':\n",
        "        # Account-based edges (limited and deterministic)\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id in sorted(account_groups.keys()):  # CRITICAL FIX: Sort account IDs\n",
        "            indices = sorted(list(account_groups[account_id]))  # Sort indices\n",
        "            if len(indices) > 1:\n",
        "                # Connect only recent posts (limit connections)\n",
        "                for i in range(min(len(indices), max_connections)):\n",
        "                    for j in range(i + 1, min(len(indices), max_connections)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "    elif method == 'hybrid':\n",
        "        # CRITICAL FIX: Combination of both methods with deterministic order\n",
        "        # First add account-based edges\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id in sorted(account_groups.keys()):  # Sort account IDs\n",
        "            indices = sorted(list(account_groups[account_id]))  # Sort indices\n",
        "            if len(indices) > 1:\n",
        "                for i in range(min(len(indices), 3)):  # Limit account connections\n",
        "                    for j in range(i + 1, min(len(indices), 3)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "        # Then add similarity-based edges deterministically\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            similarities = similarity_matrix[i]\n",
        "            # CRITICAL FIX: Use argsort for deterministic similar node selection\n",
        "            similar_indices = np.argsort(similarities)[::-1]\n",
        "            similar_indices = similar_indices[similar_indices != i]  # Remove self\n",
        "            similar_indices = similar_indices[:3]  # Top 3 similar\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold and not G.has_edge(i, j):\n",
        "                    G.add_edge(i, j, weight=float(similarities[j]))\n",
        "    return G\n",
        "\n",
        "# Construct graph deterministically\n",
        "print(f\"\\n🔄 Constructing graph with hybrid method...\")\n",
        "set_all_seeds(42)  # Reset seeds before graph construction\n",
        "final_graph = construct_meaningful_graph(df, X_net_std, method='hybrid', random_state=42)\n",
        "print(f\"✅ Final graph selected: {final_graph.number_of_nodes()} nodes, {final_graph.number_of_edges()} edges\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: CONVERT GRAPH FOR PYG & DEFINE IMPROVED GCN MODEL\n",
        "# ==========================================\n",
        "# Convert NetworkX graph to PyTorch Geometric format with consistent ordering\n",
        "edges = sorted(list(final_graph.edges))  # Sort edges for consistency\n",
        "if len(edges) == 0:\n",
        "    # Create self-loops for all nodes if graph is empty\n",
        "    edge_index = torch.arange(len(df), dtype=torch.long).repeat(2, 1)\n",
        "else:\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    # Ensure edge_index is of shape [2, num_edges]\n",
        "    assert edge_index.shape[0] == 2, f\"Edge index must have shape [2, num_edges], got {edge_index.shape}\"\n",
        "\n",
        "# CRITICAL: Validate edge indices\n",
        "max_node_idx = len(df) - 1\n",
        "if edge_index.numel() > 0:  # Check if edge_index is not empty\n",
        "    if edge_index.max().item() > max_node_idx or edge_index.min().item() < 0:\n",
        "        raise ValueError(f\"Invalid edge indices! Max: {edge_index.max().item()}, Min: {edge_index.min().item()}, Expected range: [0, {max_node_idx}]\")\n",
        "\n",
        "x = torch.tensor(X_net_std, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "# Add self-loops deterministically\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "print(f\"PyTorch Geometric Data: {data}\")\n",
        "\n",
        "# CRITICAL FIX: Define improved GCN model with DETERMINISTIC initialization\n",
        "class ImprovedGCN(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, out_channels=128, dropout=0.3, seed=42):\n",
        "        super(ImprovedGCN, self).__init__()\n",
        "        self.seed = seed\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.init_weights()  # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights DETERMINISTICALLY with fixed seed\"\"\"\n",
        "        torch.manual_seed(self.seed)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x shape: [num_nodes, in_channels]\n",
        "        # edge_index shape: [2, num_edges]\n",
        "        # Debug inputs\n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            raise ValueError(\"Input features contain NaN or Inf values!\")\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        # x shape: [num_nodes, out_channels]\n",
        "        return x\n",
        "\n",
        "# Initialize and get GCN embeddings (Pre-compute GCN embeddings)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move to CPU first for debugging\n",
        "print(\"🔍 Running GCN on CPU first for debugging...\")\n",
        "set_all_seeds(42)  # CRITICAL: Reset seeds before model creation\n",
        "gcn_model = ImprovedGCN(seed=42)\n",
        "data_cpu = data.to('cpu')\n",
        "\n",
        "# Get GCN embeddings (from untrained model - used as pre-computed features)\n",
        "gcn_model.eval()\n",
        "with torch.no_grad():\n",
        "    try:\n",
        "        gcn_embeddings = gcn_model(data_cpu.x, data_cpu.edge_index)\n",
        "        print(\"✅ GCN forward pass successful on CPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GCN forward pass failed on CPU: {e}\")\n",
        "        raise\n",
        "\n",
        "# Now move to GPU if available\n",
        "if device.type == 'cuda':\n",
        "    print(\"🔄 Moving to GPU...\")\n",
        "    try:\n",
        "        gcn_model = gcn_model.to(device)\n",
        "        data = data.to(device)\n",
        "        gcn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            gcn_embeddings = gcn_model(data.x, data.edge_index)\n",
        "            print(\"✅ GCN forward pass successful on GPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GCN forward pass failed on GPU, using CPU results: {e}\")\n",
        "        gcn_model = gcn_model.to('cpu')\n",
        "        data = data.to('cpu')\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "print(f\"GCN embeddings shape: {gcn_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(gcn_embeddings).any().item()}\")\n",
        "\n",
        "# Save GCN model and embeddings (can keep saving)\n",
        "try:\n",
        "    torch.save(gcn_model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/improved_gcn_model.pth')\n",
        "    torch.save(gcn_embeddings.cpu(), '/content/drive/MyDrive/Projects/Hayat/gcn_embeddings.pt')\n",
        "    print(\"✅ GCN model and embeddings saved\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not save GCN results: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7: BERT MODEL AND TEXT BRANCH\n",
        "# ==========================================\n",
        "# CRITICAL FIX: Define improved attention mechanism with DETERMINISTIC initialization\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads=8, seed=42):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.seed = seed\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.init_weights()  # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights DETERMINISTICALLY\"\"\"\n",
        "        torch.manual_seed(self.seed)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, seq_len, hidden_dim = embeddings.shape\n",
        "        # Generate Q, K, V\n",
        "        Q = self.query(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.key(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = self.value(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for attention computation\n",
        "        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Reshape and output\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
        "        output = self.output(context)\n",
        "\n",
        "        # Global average pooling\n",
        "        output = output.mean(dim=1)  # (batch_size, hidden_dim)\n",
        "        return output, attention_weights\n",
        "\n",
        "# CRITICAL FIX: Force BERT reprocessing by clearing/disabling cache\n",
        "bert_cache_path = '/content/drive/MyDrive/Projects/Hayat/bert_embeddings_cache.pt'\n",
        "# Clear cache file if it exists and we want to force reprocessing\n",
        "if not USE_CACHE and os.path.exists(bert_cache_path):\n",
        "     try:\n",
        "         os.remove(bert_cache_path)\n",
        "         print(\"🗑️ Cleared BERT cache file.\")\n",
        "     except Exception as e:\n",
        "         print(f\"⚠️ Could not remove BERT cache: {e}\")\n",
        "\n",
        "if USE_CACHE:\n",
        "    try:\n",
        "        # Try to load cached BERT embeddings\n",
        "        bert_embeddings = torch.load(bert_cache_path)\n",
        "        print(\"✅ Loaded cached BERT embeddings\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ BERT cache load failed ({e}), running BERT...\")\n",
        "        # ... (rest of BERT processing code if cache fails) ...\n",
        "        # Initialize BERT and attention with deterministic settings\n",
        "        set_all_seeds(42)\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "        bert_model.eval()\n",
        "        # Freeze BERT weights (common practice for feature extraction)\n",
        "        for param in bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        attention_layer = MultiHeadAttention(hidden_dim=768, num_heads=8, seed=42).to(device)\n",
        "\n",
        "        # Process texts with BERT deterministically\n",
        "        batch_size = 16 # Keep batch size small to manage memory\n",
        "        bert_embeddings = []\n",
        "        texts = df['Context Post'].fillna(\"\").tolist()\n",
        "        print(f\"Processing {len(texts)} texts for BERT embeddings...\")\n",
        "        # CRITICAL FIX: Calculate optimal max_length DETERMINISTICALLY\n",
        "        set_all_seeds(42)\n",
        "        sample_texts = texts[:100] if len(texts) > 100 else texts  # Consistent sample\n",
        "        text_lengths = [len(tokenizer.tokenize(text)) for text in sample_texts]\n",
        "        optimal_max_length = int(np.percentile(text_lengths, 95))\n",
        "        optimal_max_length = min(optimal_max_length, 512)  # Cap at BERT max\n",
        "        print(f\"Optimal max_length: {optimal_max_length}\")\n",
        "\n",
        "        # Pre-compute BERT embeddings deterministically\n",
        "        try:\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                # CRITICAL FIX: Deterministic tokenization\n",
        "                inputs = tokenizer(\n",
        "                    batch_texts,\n",
        "                    return_tensors='pt',\n",
        "                    padding='max_length',  # FIXED: Use max_length padding\n",
        "                    truncation=True,\n",
        "                    max_length=optimal_max_length\n",
        "                )\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    outputs = bert_model(**inputs)\n",
        "                    token_embeddings = outputs.last_hidden_state\n",
        "                    context_vector, _ = attention_layer(token_embeddings)\n",
        "                    bert_embeddings.append(context_vector.cpu())\n",
        "            bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "            # Cache the BERT embeddings\n",
        "            try:\n",
        "                torch.save(bert_embeddings, bert_cache_path)\n",
        "                print(\"✅ BERT embeddings cached for future runs\")\n",
        "            except Exception as e2:\n",
        "                print(f\"⚠️ Could not cache BERT embeddings: {e2}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ BERT processing failed: {e}\")\n",
        "            print(\"Using CPU for BERT processing...\")\n",
        "            # Fallback to CPU with same deterministic approach\n",
        "            bert_model = bert_model.to('cpu')\n",
        "            attention_layer = attention_layer.to('cpu')\n",
        "            device_bert = torch.device('cpu')\n",
        "            bert_embeddings = []\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                inputs = tokenizer(\n",
        "                    batch_texts,\n",
        "                    return_tensors='pt',\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    max_length=optimal_max_length\n",
        "                )\n",
        "                inputs = {k: v.to(device_bert) for k, v in inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    outputs = bert_model(**inputs)\n",
        "                    token_embeddings = outputs.last_hidden_state\n",
        "                    context_vector, _ = attention_layer(token_embeddings)\n",
        "                    bert_embeddings.append(context_vector.cpu())\n",
        "            bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "            # Cache the BERT embeddings\n",
        "            try:\n",
        "                torch.save(bert_embeddings, bert_cache_path)\n",
        "                print(\"✅ BERT embeddings cached (CPU) for future runs\")\n",
        "            except Exception as e2:\n",
        "                print(f\"⚠️ Could not cache BERT embeddings (CPU): {e2}\")\n",
        "else:\n",
        "    print(\"🔄 Running BERT embedding generation (cache disabled)...\")\n",
        "    # Initialize BERT and attention with deterministic settings\n",
        "    set_all_seeds(42)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "    bert_model.eval()\n",
        "    # Freeze BERT weights (common practice for feature extraction)\n",
        "    for param in bert_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    attention_layer = MultiHeadAttention(hidden_dim=768, num_heads=8, seed=42).to(device)\n",
        "\n",
        "    # Process texts with BERT deterministically\n",
        "    # Reduce batch size slightly to ensure longer runtime if needed\n",
        "    batch_size = 8 # <--- Reduced batch size to increase processing time per step\n",
        "    bert_embeddings = []\n",
        "    texts = df['Context Post'].fillna(\"\").tolist()\n",
        "    print(f\"Processing {len(texts)} texts for BERT embeddings (Batch Size: {batch_size})...\")\n",
        "    # CRITICAL FIX: Calculate optimal max_length DETERMINISTICALLY\n",
        "    set_all_seeds(42)\n",
        "    sample_texts = texts[:100] if len(texts) > 100 else texts  # Consistent sample\n",
        "    text_lengths = [len(tokenizer.tokenize(text)) for text in sample_texts]\n",
        "    optimal_max_length = int(np.percentile(text_lengths, 95))\n",
        "    optimal_max_length = min(optimal_max_length, 512)  # Cap at BERT max\n",
        "    print(f\"Optimal max_length: {optimal_max_length}\")\n",
        "\n",
        "    # Pre-compute BERT embeddings deterministically\n",
        "    # Add a loop counter to monitor progress\n",
        "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_idx = i // batch_size + 1\n",
        "        print(f\"  Processing BERT batch {batch_idx}/{total_batches}...\")\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        # CRITICAL FIX: Deterministic tokenization\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',  # FIXED: Use max_length padding\n",
        "            truncation=True,\n",
        "            max_length=optimal_max_length\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "            context_vector, _ = attention_layer(token_embeddings)\n",
        "            bert_embeddings.append(context_vector.cpu())\n",
        "        # Optional: Add a tiny delay to ensure processing takes time (not usually necessary)\n",
        "        # import time; time.sleep(0.01)\n",
        "    bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "    # Cache the BERT embeddings (can still save for next time)\n",
        "    try:\n",
        "        torch.save(bert_embeddings, bert_cache_path)\n",
        "        print(\"✅ BERT embeddings cached for future runs\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not cache BERT embeddings: {e}\")\n",
        "\n",
        "print(f\"Pre-computed BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(bert_embeddings).any().item()}\")\n",
        "\n",
        "# Save BERT embeddings\n",
        "try:\n",
        "    torch.save(bert_embeddings, '/content/drive/MyDrive/Projects/Hayat/bert_embeddings_improved.pt')\n",
        "    print(\"✅ BERT embeddings saved (backup)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not save BERT embeddings (backup): {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 8: TWO-BRANCH ARCHITECTURE IMPLEMENTATION\n",
        "# ==========================================\n",
        "class TwoBranchClassifier(nn.Module):\n",
        "    def __init__(self, gcn_dim=128, bert_dim=768, hidden_dim=256, num_classes=2, dropout=0.4, seed=42):\n",
        "        super(TwoBranchClassifier, self).__init__()\n",
        "        self.seed = seed\n",
        "        # --- Social Branch (uses pre-computed GCN features) ---\n",
        "        self.social_branch = nn.Sequential(\n",
        "            nn.Linear(gcn_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        # --- Text Branch (uses pre-computed BERT features) ---\n",
        "        self.text_branch = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        # --- Fusion layer with attention ---\n",
        "        self.fusion_attention = nn.Sequential(\n",
        "            nn.Linear(num_classes * 2, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 4, 2),  # Attention weights for social and text\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        # Optional final classifier layer\n",
        "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
        "        self.init_weights()  # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights DETERMINISTICALLY\"\"\"\n",
        "        torch.manual_seed(self.seed)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, gcn_features, bert_features):\n",
        "        # Debug inputs\n",
        "        if torch.isnan(gcn_features).any() or torch.isinf(gcn_features).any():\n",
        "            raise ValueError(\"GCN features contain NaN or Inf values!\")\n",
        "        if torch.isnan(bert_features).any() or torch.isinf(bert_features).any():\n",
        "            raise ValueError(\"BERT features contain NaN or Inf values!\")\n",
        "\n",
        "        # Get predictions from both branches\n",
        "        social_out = self.social_branch(gcn_features)  # [batch_size, num_classes]\n",
        "        text_out = self.text_branch(bert_features)   # [batch_size, num_classes]\n",
        "\n",
        "        # Concatenate for attention computation\n",
        "        combined = torch.cat([social_out, text_out], dim=1)  # [batch_size, num_classes*2]\n",
        "        attention_weights = self.fusion_attention(combined)  # [batch_size, 2]\n",
        "\n",
        "        weighted_social = social_out * attention_weights[:, 0:1]\n",
        "        weighted_text = text_out * attention_weights[:, 1:2]\n",
        "        final_features = weighted_social + weighted_text\n",
        "\n",
        "        final_out = self.final_classifier(final_features)  # [batch_size, num_classes]\n",
        "        return final_out, social_out, text_out, attention_weights\n",
        "\n",
        "# Initialize the model with deterministic weights\n",
        "set_all_seeds(42)\n",
        "two_branch_model = TwoBranchClassifier(seed=42).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in two_branch_model.parameters()):,}\")\n",
        "print(\"✅ Two-Branch Classifier (using pre-computed embeddings) defined\")\n",
        "\n",
        "# ==========================================\n",
        "# STEP 9: DATA PREPARATION AND SMOTE\n",
        "# ==========================================\n",
        "# CRITICAL FIX: Force SMOTE reprocessing by clearing/disabling cache\n",
        "smote_cache_path = '/content/drive/MyDrive/Projects/Hayat/smote_cache.pkl'\n",
        "# Clear cache file if it exists and we want to force reprocessing\n",
        "if not USE_CACHE and os.path.exists(smote_cache_path):\n",
        "     try:\n",
        "         os.remove(smote_cache_path)\n",
        "         print(\"🗑️ Cleared SMOTE cache file.\")\n",
        "     except Exception as e:\n",
        "         print(f\"⚠️ Could not remove SMOTE cache: {e}\")\n",
        "\n",
        "if USE_CACHE:\n",
        "    try:\n",
        "        # Try to load cached SMOTE results\n",
        "        import pickle\n",
        "        with open(smote_cache_path, 'rb') as f:\n",
        "            cached_data = pickle.load(f)\n",
        "        X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test = cached_data\n",
        "        print(\"✅ Loaded cached SMOTE and train/test split results\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ SMOTE cache load failed ({e}), running SMOTE...\")\n",
        "        # Address class imbalance with SMOTE (deterministic)\n",
        "        # Use pre-computed GCN and BERT embeddings for SMOTE\n",
        "        print(f\"\\nOriginal class distribution: {np.bincount(y)}\")\n",
        "        # CRITICAL: Validate labels again before SMOTE\n",
        "        if not validate_labels(y, num_classes=2):\n",
        "            raise ValueError(\"Labels contain invalid values before SMOTE!\")\n",
        "        # Prepare data for SMOTE: Use pre-computed GCN and BERT embeddings\n",
        "        X_for_smote = np.concatenate([gcn_embeddings.cpu().numpy(), bert_embeddings.numpy()], axis=1)\n",
        "        print(f\"Combined embeddings for SMOTE shape: {X_for_smote.shape}\")\n",
        "        # CRITICAL FIX: Use SMOTE with explicit random state\n",
        "        set_all_seeds(42)\n",
        "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X_for_smote, y)\n",
        "        # CRITICAL: Validate resampled labels\n",
        "        print(f\"After SMOTE - class distribution: {np.bincount(y_resampled)}\")\n",
        "        if not validate_labels(y_resampled, num_classes=2):\n",
        "            raise ValueError(\"Resampled labels contain invalid values!\")\n",
        "        # Split resampled data back into components\n",
        "        X_gcn_resampled = X_resampled[:, :gcn_embeddings.shape[1]]  # First part: GCN embeddings\n",
        "        X_bert_resampled = X_resampled[:, gcn_embeddings.shape[1]:]  # Second part: BERT embeddings\n",
        "        print(f\"Resampled GCN embeddings shape: {X_gcn_resampled.shape}\")\n",
        "        print(f\"Resampled BERT embeddings shape: {X_bert_resampled.shape}\")\n",
        "        # CRITICAL FIX: Train-test split with explicit random state\n",
        "        set_all_seeds(42)\n",
        "        X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test = train_test_split(\n",
        "            X_gcn_resampled, X_bert_resampled, y_resampled,\n",
        "            test_size=0.2, random_state=42, stratify=y_resampled\n",
        "        )\n",
        "        # Cache the results\n",
        "        cached_data = (X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test)\n",
        "        try:\n",
        "            with open(smote_cache_path, 'wb') as f:\n",
        "                pickle.dump(cached_data, f)\n",
        "            print(\"✅ SMOTE and train/test split results cached for future runs\")\n",
        "        except Exception as e2:\n",
        "             print(f\"⚠️ Could not cache SMOTE results: {e2}\")\n",
        "else:\n",
        "    print(\"🔄 Running SMOTE and train/test split (cache disabled)...\")\n",
        "    # Address class imbalance with SMOTE (deterministic)\n",
        "    # Use pre-computed GCN and BERT embeddings for SMOTE\n",
        "    print(f\"\\nOriginal class distribution: {np.bincount(y)}\")\n",
        "    # CRITICAL: Validate labels again before SMOTE\n",
        "    if not validate_labels(y, num_classes=2):\n",
        "        raise ValueError(\"Labels contain invalid values before SMOTE!\")\n",
        "    # Prepare data for SMOTE: Use pre-computed GCN and BERT embeddings\n",
        "    X_for_smote = np.concatenate([gcn_embeddings.cpu().numpy(), bert_embeddings.numpy()], axis=1)\n",
        "    print(f\"Combined embeddings for SMOTE shape: {X_for_smote.shape}\")\n",
        "    # CRITICAL FIX: Use SMOTE with explicit random state\n",
        "    set_all_seeds(42)\n",
        "    # Slightly increase SMOTE complexity by reducing k_neighbors (makes it work harder with less data)\n",
        "    smote = SMOTE(random_state=42, k_neighbors=3) # <--- Reduced k_neighbors\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_for_smote, y)\n",
        "    # CRITICAL: Validate resampled labels\n",
        "    print(f\"After SMOTE - class distribution: {np.bincount(y_resampled)}\")\n",
        "    if not validate_labels(y_resampled, num_classes=2):\n",
        "        raise ValueError(\"Resampled labels contain invalid values!\")\n",
        "    # Split resampled data back into components\n",
        "    X_gcn_resampled = X_resampled[:, :gcn_embeddings.shape[1]]  # First part: GCN embeddings\n",
        "    X_bert_resampled = X_resampled[:, gcn_embeddings.shape[1]:]  # Second part: BERT embeddings\n",
        "    print(f\"Resampled GCN embeddings shape: {X_gcn_resampled.shape}\")\n",
        "    print(f\"Resampled BERT embeddings shape: {X_bert_resampled.shape}\")\n",
        "    # CRITICAL FIX: Train-test split with explicit random state\n",
        "    set_all_seeds(42)\n",
        "    X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test = train_test_split(\n",
        "        X_gcn_resampled, X_bert_resampled, y_resampled,\n",
        "        test_size=0.2, random_state=42, stratify=y_resampled\n",
        "    )\n",
        "    # Cache the results (can still save for next time)\n",
        "    cached_data = (X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test)\n",
        "    try:\n",
        "        with open(smote_cache_path, 'wb') as f:\n",
        "            pickle.dump(cached_data, f)\n",
        "        print(\"✅ SMOTE and train/test split results cached for future runs\")\n",
        "    except Exception as e:\n",
        "         print(f\"⚠️ Could not cache SMOTE results: {e}\")\n",
        "\n",
        "# CRITICAL: Validate split labels\n",
        "print(f\"Training set size: {len(X_gcn_train)}\")\n",
        "print(f\"Test set size: {len(X_gcn_test)}\")\n",
        "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "if not validate_labels(y_train, num_classes=2) or not validate_labels(y_test, num_classes=2):\n",
        "    raise ValueError(\"Train/test labels contain invalid values!\")\n",
        "\n",
        "# CRITICAL FIX: Create DataLoaders with deterministic settings\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Initialize worker with deterministic seed\"\"\"\n",
        "    np.random.seed(42 + worker_id)\n",
        "    torch.manual_seed(42 + worker_id)\n",
        "\n",
        "# Create deterministic generators\n",
        "train_generator = torch.Generator().manual_seed(42)\n",
        "test_generator = torch.Generator().manual_seed(42)\n",
        "train_dataset = TensorDataset(torch.tensor(X_gcn_train, dtype=torch.float),\n",
        "                              torch.tensor(X_bert_train, dtype=torch.float),\n",
        "                              torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_gcn_test, dtype=torch.float),\n",
        "                             torch.tensor(X_bert_test, dtype=torch.float),\n",
        "                             torch.tensor(y_test, dtype=torch.long))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          generator=train_generator,\n",
        "                          worker_init_fn=worker_init_fn,\n",
        "                          num_workers=0)  # CRITICAL: Use single thread for determinism\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                         worker_init_fn=worker_init_fn,\n",
        "                         num_workers=0)  # CRITICAL: Use single thread for determinism\n",
        "print(\"✅ DataLoaders created using pre-computed embeddings\")\n",
        "\n",
        "# Debug first batch\n",
        "print(\"\\n🔍 Debugging first batch...\")\n",
        "for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "    debug_tensor(batch_gcn, \"batch_gcn\")\n",
        "    debug_tensor(batch_bert, \"batch_bert\")\n",
        "    debug_tensor(batch_y, \"batch_y\")\n",
        "    break\n",
        "\n",
        "# ==========================================\n",
        "# STEP 10: MODEL TRAINING WITH COMPREHENSIVE MONITORING\n",
        "# ==========================================\n",
        "# Ensure training uses the model defined with the deterministic seed\n",
        "def train_model(model, train_loader, test_loader, num_epochs=50, learning_rate=0.001, seed=42):\n",
        "    \"\"\"FIXED: Enhanced training function with FULL determinism\"\"\"\n",
        "    # CRITICAL FIX: Set seeds at the start of training\n",
        "    set_all_seeds(seed)\n",
        "    # Loss and optimizer with deterministic settings\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'attention_weights': [],\n",
        "        'social_acc': [],\n",
        "        'text_acc': [],\n",
        "        'learning_rate': []\n",
        "    }\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        # CRITICAL FIX: Set seeds at the start of each epoch\n",
        "        set_all_seeds(seed + epoch)\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        social_correct = 0\n",
        "        text_correct = 0\n",
        "        epoch_attention_weights = []\n",
        "        try:\n",
        "            for batch_idx, (batch_gcn, batch_bert, batch_y) in enumerate(train_loader):\n",
        "                # CRITICAL: Move to device and validate\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                # Debug batch on first iteration\n",
        "                if epoch == 0 and batch_idx == 0:\n",
        "                    print(f\"🔍 First batch debug:\")\n",
        "                    debug_tensor(batch_gcn, \"batch_gcn\")\n",
        "                    debug_tensor(batch_bert, \"batch_bert\")\n",
        "                    debug_tensor(batch_y, \"batch_y\")\n",
        "                # CRITICAL: Validate labels are in correct range\n",
        "                if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                    raise ValueError(f\"Invalid labels in batch: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "                optimizer.zero_grad()\n",
        "                # Forward pass: Pass pre-computed embeddings\n",
        "                try:\n",
        "                    final_out, social_out, text_out, attention_weights = model(\n",
        "                        batch_gcn, batch_bert\n",
        "                    )\n",
        "                    # Debug outputs on first iteration\n",
        "                    if epoch == 0 and batch_idx == 0:\n",
        "                        debug_tensor(final_out, \"final_out\")\n",
        "                        debug_tensor(social_out, \"social_out\")\n",
        "                        debug_tensor(text_out, \"text_out\")\n",
        "                        debug_tensor(attention_weights, \"attention_weights\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Forward pass failed at batch {batch_idx}: {e}\")\n",
        "                    print(f\"Batch shapes: GCN={batch_gcn.shape}, BERT={batch_bert.shape}, Y={batch_y.shape}\")\n",
        "                    raise\n",
        "                # Calculate loss with error checking\n",
        "                try:\n",
        "                    loss = criterion(final_out, batch_y)\n",
        "                    # Check for invalid loss\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        raise ValueError(f\"Invalid loss detected: {loss.item()}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Loss calculation failed: {e}\")\n",
        "                    print(f\"final_out shape: {final_out.shape}, batch_y shape: {batch_y.shape}\")\n",
        "                    print(f\"final_out min/max: {final_out.min().item()}/{final_out.max().item()}\")\n",
        "                    print(f\"batch_y unique: {torch.unique(batch_y).cpu().numpy()}\")\n",
        "                    raise\n",
        "                # Backward pass with gradient clipping\n",
        "                try:\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Backward pass failed: {e}\")\n",
        "                    raise\n",
        "                # Statistics\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                _, social_pred = torch.max(social_out.data, 1)\n",
        "                _, text_pred = torch.max(text_out.data, 1)\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "                social_correct += (social_pred == batch_y).sum().item()\n",
        "                text_correct += (text_pred == batch_y).sum().item()\n",
        "                epoch_attention_weights.append(attention_weights.cpu().detach().numpy())\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Training failed at epoch {epoch}: {e}\")\n",
        "            # Save current state for debugging\n",
        "            try:\n",
        "                torch.save(model.state_dict(), f'/content/drive/MyDrive/Projects/Hayat/debug_model_epoch_{epoch}.pth')\n",
        "            except:\n",
        "                pass\n",
        "            raise\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                    batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                    # Validate labels\n",
        "                    if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                        raise ValueError(f\"Invalid validation labels: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "                    final_out, _, _, _ = model(batch_gcn, batch_bert)\n",
        "                    loss = criterion(final_out, batch_y)\n",
        "                    # Check for invalid loss\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        raise ValueError(f\"Invalid validation loss: {loss.item()}\")\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(final_out.data, 1)\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Validation failed at epoch {epoch}: {e}\")\n",
        "            raise\n",
        "        # Calculate metrics\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        val_loss_avg = val_loss / len(test_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        social_acc = 100 * social_correct / train_total\n",
        "        text_acc = 100 * text_correct / train_total\n",
        "        # Concatenate attention weights for the epoch\n",
        "        if epoch_attention_weights:\n",
        "             concatenated_attention_weights = np.concatenate(epoch_attention_weights, axis=0)\n",
        "        else:\n",
        "             concatenated_attention_weights = np.array([])\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss_avg)\n",
        "        history['val_loss'].append(val_loss_avg)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['social_acc'].append(social_acc)\n",
        "        history['text_acc'].append(text_acc)\n",
        "        if concatenated_attention_weights.size > 0:\n",
        "            history['attention_weights'].append(concatenated_attention_weights)\n",
        "        else:\n",
        "            history['attention_weights'].append(np.array([[0.5, 0.5]] * len(train_loader)))\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss_avg)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        history['learning_rate'].append(current_lr)\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            try:\n",
        "                torch.save(model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/best_two_branch_model_fixed.pth')\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not save best model: {e}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "            print(f'  Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}')\n",
        "            print(f'  Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "            print(f'  Social Acc (Train): {social_acc:.2f}%, Text Acc (Train): {text_acc:.2f}%')\n",
        "            print(f'  LR: {current_lr:.6f}')\n",
        "            if concatenated_attention_weights.size > 0:\n",
        "                avg_social_attn = np.mean(concatenated_attention_weights[:, 0])\n",
        "                avg_text_attn = np.mean(concatenated_attention_weights[:, 1])\n",
        "                print(f'  Avg Attention - Social: {avg_social_attn:.3f}, Text: {avg_text_attn:.3f}')\n",
        "        # Early stopping check\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "            break\n",
        "    return history\n",
        "\n",
        "# Test model forward pass before training\n",
        "print(\"\\n🔍 Testing model forward pass before training...\")\n",
        "try:\n",
        "    set_all_seeds(42)\n",
        "    model_test = TwoBranchClassifier(seed=42).to(device)\n",
        "    # Get a small batch for testing\n",
        "    for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "        batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "        # Test forward pass\n",
        "        with torch.no_grad():\n",
        "            final_out, social_out, text_out, attention_weights = model_test(batch_gcn, batch_bert)\n",
        "            print(f\"✅ Forward pass test successful!\")\n",
        "            print(f\"Output shapes: final={final_out.shape}, social={social_out.shape}, text={text_out.shape}, attention={attention_weights.shape}\")\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"❌ Forward pass test failed: {e}\")\n",
        "    print(\"Switching to CPU for training...\")\n",
        "    device = torch.device('cpu')\n",
        "    two_branch_model = two_branch_model.to(device)\n",
        "\n",
        "# Train the model (using pre-computed GCN embeddings)\n",
        "# Increase epochs slightly to add more computation time if needed\n",
        "print(\"\\n🚀 Starting model training using PRE-COMPUTED GCN embeddings...\")\n",
        "try:\n",
        "    set_all_seeds(42)  # CRITICAL: Reset seeds before training\n",
        "    # Slightly increase epochs to ensure longer runtime\n",
        "    history = train_model(two_branch_model, train_loader, test_loader, num_epochs=75, learning_rate=0.001, seed=42) # <--- Increased epochs\n",
        "    print(\"✅ Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Training failed: {e}\")\n",
        "    print(\"Attempting training with reduced complexity...\")\n",
        "    # Try with fewer epochs and simpler settings\n",
        "    try:\n",
        "        set_all_seeds(42)\n",
        "        history = train_model(two_branch_model, train_loader, test_loader, num_epochs=10, learning_rate=0.01, seed=42)\n",
        "        print(\"✅ Reduced training completed!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"❌ Reduced training also failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "# ==========================================\n",
        "# STEP 11: COMPREHENSIVE EVALUATION AND ANALYSIS\n",
        "# ==========================================\n",
        "def evaluate_model(model, test_loader, device, seed=42):\n",
        "    \"\"\"FIXED: Comprehensive model evaluation with deterministic settings\"\"\"\n",
        "    set_all_seeds(seed)  # Ensure deterministic evaluation\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_social_preds = []\n",
        "    all_text_preds = []\n",
        "    all_attention_weights = []\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                # Validate labels\n",
        "                if batch_y.min().item() < 0 or batch_y.max().item() >= 2:\n",
        "                    raise ValueError(f\"Invalid evaluation labels: min={batch_y.min().item()}, max={batch_y.max().item()}\")\n",
        "                final_out, social_out, text_out, attention_weights = model(\n",
        "                    batch_gcn, batch_bert\n",
        "                )\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                _, social_pred = torch.max(social_out.data, 1)\n",
        "                _, text_pred = torch.max(text_out.data, 1)\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(batch_y.cpu().numpy())\n",
        "                all_social_preds.extend(social_pred.cpu().numpy())\n",
        "                all_text_preds.extend(text_pred.cpu().numpy())\n",
        "                all_attention_weights.extend(attention_weights.cpu().numpy())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Evaluation failed: {e}\")\n",
        "        raise\n",
        "    return (np.array(all_predictions), np.array(all_labels),\n",
        "            np.array(all_social_preds), np.array(all_text_preds),\n",
        "            np.array(all_attention_weights))\n",
        "\n",
        "# Load best model and evaluate\n",
        "print(\"\\n🔄 Loading best model for evaluation...\")\n",
        "try:\n",
        "    two_branch_model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/best_two_branch_model_fixed.pth'))\n",
        "    set_all_seeds(42)  # Reset seeds before evaluation\n",
        "    predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device, seed=42)\n",
        "    print(\"✅ Evaluation completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Evaluation failed: {e}\")\n",
        "    print(\"Using current model state for evaluation...\")\n",
        "    set_all_seeds(42)\n",
        "    predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device, seed=42)\n",
        "\n",
        "# Final validation of results\n",
        "if not validate_labels(predictions, num_classes=2) or not validate_labels(labels, num_classes=2):\n",
        "    raise ValueError(\"Final predictions or labels contain invalid values!\")\n",
        "\n",
        "# Calculate metrics\n",
        "final_acc = accuracy_score(labels, predictions)\n",
        "social_acc = accuracy_score(labels, social_preds)\n",
        "text_acc = accuracy_score(labels, text_preds)\n",
        "final_f1 = f1_score(labels, predictions, average='weighted')\n",
        "social_f1 = f1_score(labels, social_preds, average='weighted')\n",
        "text_f1 = f1_score(labels, text_preds, average='weighted')\n",
        "print(f\"\\n🎯 Final Results:\")\n",
        "print(f\"Combined Model Accuracy: {final_acc:.4f}, F1 Score: {final_f1:.4f}\")\n",
        "print(f\"Social Branch Accuracy (Test): {social_acc:.4f}, F1 Score: {social_f1:.4f}\")\n",
        "print(f\"Text Branch Accuracy (Test): {text_acc:.4f}, F1 Score: {text_f1:.4f}\")\n",
        "print(f\"Improvement over Social: {final_acc - social_acc:.4f}\")\n",
        "print(f\"Improvement over Text: {final_acc - text_acc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\n📊 Detailed Classification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=['Mostly True', 'Others']))\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n============================================================\")\n",
        "print(f\"🎉 FINAL SUMMARY AND INSIGHTS (FULLY DETERMINISTIC VERSION)\")\n",
        "print(f\"============================================================\")\n",
        "print(f\"📈 Performance Metrics:\")\n",
        "print(f\"  • Combined Model Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
        "print(f\"  • Social Branch Accuracy (Test): {social_acc:.4f} ({social_acc*100:.2f}%)\")\n",
        "print(f\"  • Text Branch Accuracy (Test): {text_acc:.4f} ({text_acc*100:.2f}%)\")\n",
        "if attention_weights.size > 0:\n",
        "    avg_social_attn = np.mean(attention_weights[:, 0])\n",
        "    avg_text_attn = np.mean(attention_weights[:, 1])\n",
        "    print(f\"🔍 Model Analysis:\")\n",
        "    print(f\"  • Average Social Attention: {avg_social_attn:.3f}\")\n",
        "    print(f\"  • Average Text Attention: {avg_text_attn:.3f}\")\n",
        "    print(f\"  • Attention Correlation: {np.corrcoef(attention_weights[:, 0], attention_weights[:, 1])[0, 1]:.3f}\")\n",
        "    print(f\"💡 Key Insights:\")\n",
        "    print(f\"  ✅ The combined model outperforms individual branches\")\n",
        "    if avg_social_attn > avg_text_attn:\n",
        "        print(f\"  📊 Social features are more important on average\")\n",
        "    else:\n",
        "        print(f\"  📝 Text features are more important on average\")\n",
        "else:\n",
        "    print(\"🔍 Model Analysis: Attention weights not available.\")\n",
        "    print(\"💡 Key Insights:\")\n",
        "    print(\"  ✅ The combined model architecture is working.\")\n",
        "\n",
        "print(\"\\n⏱️ Script execution completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opjpcm_BouJE",
        "outputId": "b8a3c8a6-7ff6-4ca9-db2f-8b7b49ecbf79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries imported and seeds set for reproducibility!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "🔄 Loading dataset from CSV (cache disabled)...\n",
            "✅ Dataset loaded from CSV\n",
            "Dataset shape: (2282, 13)\n",
            "Columns: ['account_id', 'post_id', 'Category', 'Page', 'Post URL', 'Date Published', 'Post Type', 'Rating', 'Debate', 'share_count', 'reaction_count', 'comment_count', 'Context Post']\n",
            "\n",
            "Dataset Info:\n",
            "Number of samples: 2282\n",
            "Number of features: 13\n",
            "Missing values per column:\n",
            "account_id           0\n",
            "post_id              0\n",
            "Category             0\n",
            "Page                 0\n",
            "Post URL             0\n",
            "Date Published       0\n",
            "Post Type            0\n",
            "Rating               0\n",
            "Debate            1984\n",
            "share_count         70\n",
            "reaction_count       2\n",
            "comment_count        2\n",
            "Context Post         0\n",
            "dtype: int64\n",
            "✅ Network features preprocessed\n",
            "Raw features shape: (2282, 3)\n",
            "Processed features shape: (2282, 3)\n",
            "Standardized features shape: (2282, 3)\n",
            "NaN check: False\n",
            "\n",
            "🔍 Analyzing labels...\n",
            "Unique ratings: Rating\n",
            "mixture of true and false     245\n",
            "mostly false                  104\n",
            "mostly true                  1669\n",
            "no factual content            264\n",
            "Name: count, dtype: int64\n",
            "Label distribution: [1669  613]\n",
            "Class 0 (mostly true): 1669 (73.1%)\n",
            "Class 1 (others): 613 (26.9%)\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "✅ Labels validated successfully\n",
            "\n",
            "🔄 Constructing graph with hybrid method...\n",
            "✅ Final graph selected: 2282 nodes, 4304 edges\n",
            "PyTorch Geometric Data: Data(x=[2282, 3], edge_index=[2, 6586])\n",
            "Using device: cpu\n",
            "🔍 Running GCN on CPU first for debugging...\n",
            "✅ GCN forward pass successful on CPU\n",
            "GCN embeddings shape: torch.Size([2282, 128])\n",
            "NaN check: False\n",
            "✅ GCN model and embeddings saved\n",
            "🗑️ Cleared BERT cache file.\n",
            "🔄 Running BERT embedding generation (cache disabled)...\n",
            "Processing 2282 texts for BERT embeddings (Batch Size: 8)...\n",
            "Optimal max_length: 45\n",
            "  Processing BERT batch 1/286...\n",
            "  Processing BERT batch 2/286...\n",
            "  Processing BERT batch 3/286...\n",
            "  Processing BERT batch 4/286...\n",
            "  Processing BERT batch 5/286...\n",
            "  Processing BERT batch 6/286...\n",
            "  Processing BERT batch 7/286...\n",
            "  Processing BERT batch 8/286...\n",
            "  Processing BERT batch 9/286...\n",
            "  Processing BERT batch 10/286...\n",
            "  Processing BERT batch 11/286...\n",
            "  Processing BERT batch 12/286...\n",
            "  Processing BERT batch 13/286...\n",
            "  Processing BERT batch 14/286...\n",
            "  Processing BERT batch 15/286...\n",
            "  Processing BERT batch 16/286...\n",
            "  Processing BERT batch 17/286...\n",
            "  Processing BERT batch 18/286...\n",
            "  Processing BERT batch 19/286...\n",
            "  Processing BERT batch 20/286...\n",
            "  Processing BERT batch 21/286...\n",
            "  Processing BERT batch 22/286...\n",
            "  Processing BERT batch 23/286...\n",
            "  Processing BERT batch 24/286...\n",
            "  Processing BERT batch 25/286...\n",
            "  Processing BERT batch 26/286...\n",
            "  Processing BERT batch 27/286...\n",
            "  Processing BERT batch 28/286...\n",
            "  Processing BERT batch 29/286...\n",
            "  Processing BERT batch 30/286...\n",
            "  Processing BERT batch 31/286...\n",
            "  Processing BERT batch 32/286...\n",
            "  Processing BERT batch 33/286...\n",
            "  Processing BERT batch 34/286...\n",
            "  Processing BERT batch 35/286...\n",
            "  Processing BERT batch 36/286...\n",
            "  Processing BERT batch 37/286...\n",
            "  Processing BERT batch 38/286...\n",
            "  Processing BERT batch 39/286...\n",
            "  Processing BERT batch 40/286...\n",
            "  Processing BERT batch 41/286...\n",
            "  Processing BERT batch 42/286...\n",
            "  Processing BERT batch 43/286...\n",
            "  Processing BERT batch 44/286...\n",
            "  Processing BERT batch 45/286...\n",
            "  Processing BERT batch 46/286...\n",
            "  Processing BERT batch 47/286...\n",
            "  Processing BERT batch 48/286...\n",
            "  Processing BERT batch 49/286...\n",
            "  Processing BERT batch 50/286...\n",
            "  Processing BERT batch 51/286...\n",
            "  Processing BERT batch 52/286...\n",
            "  Processing BERT batch 53/286...\n",
            "  Processing BERT batch 54/286...\n",
            "  Processing BERT batch 55/286...\n",
            "  Processing BERT batch 56/286...\n",
            "  Processing BERT batch 57/286...\n",
            "  Processing BERT batch 58/286...\n",
            "  Processing BERT batch 59/286...\n",
            "  Processing BERT batch 60/286...\n",
            "  Processing BERT batch 61/286...\n",
            "  Processing BERT batch 62/286...\n",
            "  Processing BERT batch 63/286...\n",
            "  Processing BERT batch 64/286...\n",
            "  Processing BERT batch 65/286...\n",
            "  Processing BERT batch 66/286...\n",
            "  Processing BERT batch 67/286...\n",
            "  Processing BERT batch 68/286...\n",
            "  Processing BERT batch 69/286...\n",
            "  Processing BERT batch 70/286...\n",
            "  Processing BERT batch 71/286...\n",
            "  Processing BERT batch 72/286...\n",
            "  Processing BERT batch 73/286...\n",
            "  Processing BERT batch 74/286...\n",
            "  Processing BERT batch 75/286...\n",
            "  Processing BERT batch 76/286...\n",
            "  Processing BERT batch 77/286...\n",
            "  Processing BERT batch 78/286...\n",
            "  Processing BERT batch 79/286...\n",
            "  Processing BERT batch 80/286...\n",
            "  Processing BERT batch 81/286...\n",
            "  Processing BERT batch 82/286...\n",
            "  Processing BERT batch 83/286...\n",
            "  Processing BERT batch 84/286...\n",
            "  Processing BERT batch 85/286...\n",
            "  Processing BERT batch 86/286...\n",
            "  Processing BERT batch 87/286...\n",
            "  Processing BERT batch 88/286...\n",
            "  Processing BERT batch 89/286...\n",
            "  Processing BERT batch 90/286...\n",
            "  Processing BERT batch 91/286...\n",
            "  Processing BERT batch 92/286...\n",
            "  Processing BERT batch 93/286...\n",
            "  Processing BERT batch 94/286...\n",
            "  Processing BERT batch 95/286...\n",
            "  Processing BERT batch 96/286...\n",
            "  Processing BERT batch 97/286...\n",
            "  Processing BERT batch 98/286...\n",
            "  Processing BERT batch 99/286...\n",
            "  Processing BERT batch 100/286...\n",
            "  Processing BERT batch 101/286...\n",
            "  Processing BERT batch 102/286...\n",
            "  Processing BERT batch 103/286...\n",
            "  Processing BERT batch 104/286...\n",
            "  Processing BERT batch 105/286...\n",
            "  Processing BERT batch 106/286...\n",
            "  Processing BERT batch 107/286...\n",
            "  Processing BERT batch 108/286...\n",
            "  Processing BERT batch 109/286...\n",
            "  Processing BERT batch 110/286...\n",
            "  Processing BERT batch 111/286...\n",
            "  Processing BERT batch 112/286...\n",
            "  Processing BERT batch 113/286...\n",
            "  Processing BERT batch 114/286...\n",
            "  Processing BERT batch 115/286...\n",
            "  Processing BERT batch 116/286...\n",
            "  Processing BERT batch 117/286...\n",
            "  Processing BERT batch 118/286...\n",
            "  Processing BERT batch 119/286...\n",
            "  Processing BERT batch 120/286...\n",
            "  Processing BERT batch 121/286...\n",
            "  Processing BERT batch 122/286...\n",
            "  Processing BERT batch 123/286...\n",
            "  Processing BERT batch 124/286...\n",
            "  Processing BERT batch 125/286...\n",
            "  Processing BERT batch 126/286...\n",
            "  Processing BERT batch 127/286...\n",
            "  Processing BERT batch 128/286...\n",
            "  Processing BERT batch 129/286...\n",
            "  Processing BERT batch 130/286...\n",
            "  Processing BERT batch 131/286...\n",
            "  Processing BERT batch 132/286...\n",
            "  Processing BERT batch 133/286...\n",
            "  Processing BERT batch 134/286...\n",
            "  Processing BERT batch 135/286...\n",
            "  Processing BERT batch 136/286...\n",
            "  Processing BERT batch 137/286...\n",
            "  Processing BERT batch 138/286...\n",
            "  Processing BERT batch 139/286...\n",
            "  Processing BERT batch 140/286...\n",
            "  Processing BERT batch 141/286...\n",
            "  Processing BERT batch 142/286...\n",
            "  Processing BERT batch 143/286...\n",
            "  Processing BERT batch 144/286...\n",
            "  Processing BERT batch 145/286...\n",
            "  Processing BERT batch 146/286...\n",
            "  Processing BERT batch 147/286...\n",
            "  Processing BERT batch 148/286...\n",
            "  Processing BERT batch 149/286...\n",
            "  Processing BERT batch 150/286...\n",
            "  Processing BERT batch 151/286...\n",
            "  Processing BERT batch 152/286...\n",
            "  Processing BERT batch 153/286...\n",
            "  Processing BERT batch 154/286...\n",
            "  Processing BERT batch 155/286...\n",
            "  Processing BERT batch 156/286...\n",
            "  Processing BERT batch 157/286...\n",
            "  Processing BERT batch 158/286...\n",
            "  Processing BERT batch 159/286...\n",
            "  Processing BERT batch 160/286...\n",
            "  Processing BERT batch 161/286...\n",
            "  Processing BERT batch 162/286...\n",
            "  Processing BERT batch 163/286...\n",
            "  Processing BERT batch 164/286...\n",
            "  Processing BERT batch 165/286...\n",
            "  Processing BERT batch 166/286...\n",
            "  Processing BERT batch 167/286...\n",
            "  Processing BERT batch 168/286...\n",
            "  Processing BERT batch 169/286...\n",
            "  Processing BERT batch 170/286...\n",
            "  Processing BERT batch 171/286...\n",
            "  Processing BERT batch 172/286...\n",
            "  Processing BERT batch 173/286...\n",
            "  Processing BERT batch 174/286...\n",
            "  Processing BERT batch 175/286...\n",
            "  Processing BERT batch 176/286...\n",
            "  Processing BERT batch 177/286...\n",
            "  Processing BERT batch 178/286...\n",
            "  Processing BERT batch 179/286...\n",
            "  Processing BERT batch 180/286...\n",
            "  Processing BERT batch 181/286...\n",
            "  Processing BERT batch 182/286...\n",
            "  Processing BERT batch 183/286...\n",
            "  Processing BERT batch 184/286...\n",
            "  Processing BERT batch 185/286...\n",
            "  Processing BERT batch 186/286...\n",
            "  Processing BERT batch 187/286...\n",
            "  Processing BERT batch 188/286...\n",
            "  Processing BERT batch 189/286...\n",
            "  Processing BERT batch 190/286...\n",
            "  Processing BERT batch 191/286...\n",
            "  Processing BERT batch 192/286...\n",
            "  Processing BERT batch 193/286...\n",
            "  Processing BERT batch 194/286...\n",
            "  Processing BERT batch 195/286...\n",
            "  Processing BERT batch 196/286...\n",
            "  Processing BERT batch 197/286...\n",
            "  Processing BERT batch 198/286...\n",
            "  Processing BERT batch 199/286...\n",
            "  Processing BERT batch 200/286...\n",
            "  Processing BERT batch 201/286...\n",
            "  Processing BERT batch 202/286...\n",
            "  Processing BERT batch 203/286...\n",
            "  Processing BERT batch 204/286...\n",
            "  Processing BERT batch 205/286...\n",
            "  Processing BERT batch 206/286...\n",
            "  Processing BERT batch 207/286...\n",
            "  Processing BERT batch 208/286...\n",
            "  Processing BERT batch 209/286...\n",
            "  Processing BERT batch 210/286...\n",
            "  Processing BERT batch 211/286...\n",
            "  Processing BERT batch 212/286...\n",
            "  Processing BERT batch 213/286...\n",
            "  Processing BERT batch 214/286...\n",
            "  Processing BERT batch 215/286...\n",
            "  Processing BERT batch 216/286...\n",
            "  Processing BERT batch 217/286...\n",
            "  Processing BERT batch 218/286...\n",
            "  Processing BERT batch 219/286...\n",
            "  Processing BERT batch 220/286...\n",
            "  Processing BERT batch 221/286...\n",
            "  Processing BERT batch 222/286...\n",
            "  Processing BERT batch 223/286...\n",
            "  Processing BERT batch 224/286...\n",
            "  Processing BERT batch 225/286...\n",
            "  Processing BERT batch 226/286...\n",
            "  Processing BERT batch 227/286...\n",
            "  Processing BERT batch 228/286...\n",
            "  Processing BERT batch 229/286...\n",
            "  Processing BERT batch 230/286...\n",
            "  Processing BERT batch 231/286...\n",
            "  Processing BERT batch 232/286...\n",
            "  Processing BERT batch 233/286...\n",
            "  Processing BERT batch 234/286...\n",
            "  Processing BERT batch 235/286...\n",
            "  Processing BERT batch 236/286...\n",
            "  Processing BERT batch 237/286...\n",
            "  Processing BERT batch 238/286...\n",
            "  Processing BERT batch 239/286...\n",
            "  Processing BERT batch 240/286...\n",
            "  Processing BERT batch 241/286...\n",
            "  Processing BERT batch 242/286...\n",
            "  Processing BERT batch 243/286...\n",
            "  Processing BERT batch 244/286...\n",
            "  Processing BERT batch 245/286...\n",
            "  Processing BERT batch 246/286...\n",
            "  Processing BERT batch 247/286...\n",
            "  Processing BERT batch 248/286...\n",
            "  Processing BERT batch 249/286...\n",
            "  Processing BERT batch 250/286...\n",
            "  Processing BERT batch 251/286...\n",
            "  Processing BERT batch 252/286...\n",
            "  Processing BERT batch 253/286...\n",
            "  Processing BERT batch 254/286...\n",
            "  Processing BERT batch 255/286...\n",
            "  Processing BERT batch 256/286...\n",
            "  Processing BERT batch 257/286...\n",
            "  Processing BERT batch 258/286...\n",
            "  Processing BERT batch 259/286...\n",
            "  Processing BERT batch 260/286...\n",
            "  Processing BERT batch 261/286...\n",
            "  Processing BERT batch 262/286...\n",
            "  Processing BERT batch 263/286...\n",
            "  Processing BERT batch 264/286...\n",
            "  Processing BERT batch 265/286...\n",
            "  Processing BERT batch 266/286...\n",
            "  Processing BERT batch 267/286...\n",
            "  Processing BERT batch 268/286...\n",
            "  Processing BERT batch 269/286...\n",
            "  Processing BERT batch 270/286...\n",
            "  Processing BERT batch 271/286...\n",
            "  Processing BERT batch 272/286...\n",
            "  Processing BERT batch 273/286...\n",
            "  Processing BERT batch 274/286...\n",
            "  Processing BERT batch 275/286...\n",
            "  Processing BERT batch 276/286...\n",
            "  Processing BERT batch 277/286...\n",
            "  Processing BERT batch 278/286...\n",
            "  Processing BERT batch 279/286...\n",
            "  Processing BERT batch 280/286...\n",
            "  Processing BERT batch 281/286...\n",
            "  Processing BERT batch 282/286...\n",
            "  Processing BERT batch 283/286...\n",
            "  Processing BERT batch 284/286...\n",
            "  Processing BERT batch 285/286...\n",
            "  Processing BERT batch 286/286...\n",
            "✅ BERT embeddings cached for future runs\n",
            "Pre-computed BERT embeddings shape: torch.Size([2282, 768])\n",
            "NaN check: False\n",
            "✅ BERT embeddings saved (backup)\n",
            "Model parameters: 298,188\n",
            "✅ Two-Branch Classifier (using pre-computed embeddings) defined\n",
            "🗑️ Cleared SMOTE cache file.\n",
            "🔄 Running SMOTE and train/test split (cache disabled)...\n",
            "\n",
            "Original class distribution: [1669  613]\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Combined embeddings for SMOTE shape: (2282, 896)\n",
            "After SMOTE - class distribution: [1669 1669]\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Resampled GCN embeddings shape: (3338, 128)\n",
            "Resampled BERT embeddings shape: (3338, 768)\n",
            "✅ SMOTE and train/test split results cached for future runs\n",
            "Training set size: 2670\n",
            "Test set size: 668\n",
            "Train labels distribution: [1335 1335]\n",
            "Test labels distribution: [334 334]\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "✅ DataLoaders created using pre-computed embeddings\n",
            "\n",
            "🔍 Debugging first batch...\n",
            "Debug batch_gcn:\n",
            "  Shape: torch.Size([32, 128])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -1.2428635358810425\n",
            "  Max: 1.314090609550476\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_bert:\n",
            "  Shape: torch.Size([32, 768])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -1.2782071828842163\n",
            "  Max: 1.1876485347747803\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_y:\n",
            "  Shape: torch.Size([32])\n",
            "  Dtype: torch.int64\n",
            "  Device: cpu\n",
            "  Min: 0\n",
            "  Max: 1\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "  Unique values: [0 1]\n",
            "\n",
            "🔍 Testing model forward pass before training...\n",
            "✅ Forward pass test successful!\n",
            "Output shapes: final=torch.Size([32, 2]), social=torch.Size([32, 2]), text=torch.Size([32, 2]), attention=torch.Size([32, 2])\n",
            "\n",
            "🚀 Starting model training using PRE-COMPUTED GCN embeddings...\n",
            "🔍 First batch debug:\n",
            "Debug batch_gcn:\n",
            "  Shape: torch.Size([32, 128])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -1.2001279592514038\n",
            "  Max: 0.7866923809051514\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_bert:\n",
            "  Shape: torch.Size([32, 768])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -1.2782071828842163\n",
            "  Max: 1.065016508102417\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug batch_y:\n",
            "  Shape: torch.Size([32])\n",
            "  Dtype: torch.int64\n",
            "  Device: cpu\n",
            "  Min: 0\n",
            "  Max: 1\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "  Unique values: [0 1]\n",
            "Debug final_out:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -2.056279182434082\n",
            "  Max: 1.9944208860397339\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug social_out:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -4.214670658111572\n",
            "  Max: 5.020394325256348\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug text_out:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: -3.583387613296509\n",
            "  Max: 4.182421684265137\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Debug attention_weights:\n",
            "  Shape: torch.Size([32, 2])\n",
            "  Dtype: torch.float32\n",
            "  Device: cpu\n",
            "  Min: 0.2371724396944046\n",
            "  Max: 0.7628275752067566\n",
            "  Has NaN: False\n",
            "  Has Inf: False\n",
            "Epoch [5/75]\n",
            "  Train Loss: 0.4177, Val Loss: 0.4595\n",
            "  Train Acc: 80.82%, Val Acc: 77.25%\n",
            "  Social Acc (Train): 35.24%, Text Acc (Train): 53.41%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.509, Text: 0.491\n",
            "Epoch [10/75]\n",
            "  Train Loss: 0.3819, Val Loss: 0.4396\n",
            "  Train Acc: 82.43%, Val Acc: 80.69%\n",
            "  Social Acc (Train): 35.73%, Text Acc (Train): 67.83%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.468, Text: 0.532\n",
            "Epoch [15/75]\n",
            "  Train Loss: 0.3512, Val Loss: 0.4450\n",
            "  Train Acc: 84.27%, Val Acc: 81.29%\n",
            "  Social Acc (Train): 37.60%, Text Acc (Train): 62.36%\n",
            "  LR: 0.001000\n",
            "  Avg Attention - Social: 0.480, Text: 0.520\n",
            "Epoch [20/75]\n",
            "  Train Loss: 0.3395, Val Loss: 0.4707\n",
            "  Train Acc: 84.49%, Val Acc: 80.54%\n",
            "  Social Acc (Train): 39.55%, Text Acc (Train): 62.02%\n",
            "  LR: 0.000500\n",
            "  Avg Attention - Social: 0.477, Text: 0.523\n",
            "Epoch [25/75]\n",
            "  Train Loss: 0.3042, Val Loss: 0.4309\n",
            "  Train Acc: 86.37%, Val Acc: 82.19%\n",
            "  Social Acc (Train): 40.82%, Text Acc (Train): 61.05%\n",
            "  LR: 0.000500\n",
            "  Avg Attention - Social: 0.491, Text: 0.509\n",
            "Epoch [30/75]\n",
            "  Train Loss: 0.2785, Val Loss: 0.4112\n",
            "  Train Acc: 87.83%, Val Acc: 82.93%\n",
            "  Social Acc (Train): 39.78%, Text Acc (Train): 60.45%\n",
            "  LR: 0.000250\n",
            "  Avg Attention - Social: 0.501, Text: 0.499\n",
            "Epoch [35/75]\n",
            "  Train Loss: 0.2790, Val Loss: 0.4393\n",
            "  Train Acc: 87.72%, Val Acc: 82.19%\n",
            "  Social Acc (Train): 40.97%, Text Acc (Train): 58.16%\n",
            "  LR: 0.000250\n",
            "  Avg Attention - Social: 0.493, Text: 0.507\n",
            "Epoch [40/75]\n",
            "  Train Loss: 0.2551, Val Loss: 0.4691\n",
            "  Train Acc: 89.29%, Val Acc: 83.08%\n",
            "  Social Acc (Train): 42.21%, Text Acc (Train): 58.54%\n",
            "  LR: 0.000125\n",
            "  Avg Attention - Social: 0.492, Text: 0.508\n",
            "Early stopping triggered at epoch 42\n",
            "✅ Training completed successfully!\n",
            "\n",
            "🔄 Loading best model for evaluation...\n",
            "✅ Evaluation completed successfully!\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "Label validation:\n",
            "  Unique labels: [0 1]\n",
            "  Expected range: [0, 1]\n",
            "  All labels valid: True\n",
            "\n",
            "🎯 Final Results:\n",
            "Combined Model Accuracy: 0.8308, F1 Score: 0.8288\n",
            "Social Branch Accuracy (Test): 0.4192, F1 Score: 0.4079\n",
            "Text Branch Accuracy (Test): 0.5614, F1 Score: 0.5558\n",
            "Improvement over Social: 0.4117\n",
            "Improvement over Text: 0.2695\n",
            "\n",
            "📊 Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Mostly True       0.92      0.72      0.81       334\n",
            "      Others       0.77      0.94      0.85       334\n",
            "\n",
            "    accuracy                           0.83       668\n",
            "   macro avg       0.85      0.83      0.83       668\n",
            "weighted avg       0.85      0.83      0.83       668\n",
            "\n",
            "\n",
            "============================================================\n",
            "🎉 FINAL SUMMARY AND INSIGHTS (FULLY DETERMINISTIC VERSION)\n",
            "============================================================\n",
            "📈 Performance Metrics:\n",
            "  • Combined Model Accuracy: 0.8308 (83.08%)\n",
            "  • Social Branch Accuracy (Test): 0.4192 (41.92%)\n",
            "  • Text Branch Accuracy (Test): 0.5614 (56.14%)\n",
            "🔍 Model Analysis:\n",
            "  • Average Social Attention: 0.502\n",
            "  • Average Text Attention: 0.498\n",
            "  • Attention Correlation: -1.000\n",
            "💡 Key Insights:\n",
            "  ✅ The combined model outperforms individual branches\n",
            "  📊 Social features are more important on average\n",
            "\n",
            "⏱️ Script execution completed. The forced recomputation (especially BERT) should have taken significantly longer.\n"
          ]
        }
      ]
    }
  ]
}