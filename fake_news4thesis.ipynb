{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMonTHPY48I9+BglC0tTVFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/fake_news4thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUMf1ad_b_D7"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# STEP 1: SETUP AND IMPORTS & DETERMINISTIC SETUP\n",
        "# ==========================================\n",
        "import os # For deterministic setup\n",
        "# Install dependencies (run once)\n",
        "# !pip install torch-geometric imbalanced-learn -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "from scipy import io as sio\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "sns.set_palette(\"dark\")\n",
        "\n",
        "# ==========================================\n",
        "# COMPREHENSIVE REPRODUCIBILITY SETUP\n",
        "# ==========================================\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "    \"\"\"Set all possible seeds for reproducibility\"\"\"\n",
        "    # Python random\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # PyTorch backends\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Environment variables for additional determinism\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "    # Additional PyTorch settings\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "\n",
        "# Apply comprehensive seed setting\n",
        "set_all_seeds(42)\n",
        "\n",
        "print(\"âœ… All libraries imported and seeds set for reproducibility!\")\n",
        "\n",
        "# Reproducibility (Legacy - covered by set_all_seeds)\n",
        "# torch.manual_seed(42)\n",
        "# np.random.seed(42)\n",
        "# random.seed(42)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed_all(42)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: DATA LOADING AND INITIAL EXPLORATION\n",
        "# ==========================================\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Projects/Hayat/facebook-fact-check.csv', encoding='latin-1')\n",
        "# IMPORTANT: Sort dataframe to ensure consistent ordering (from Part 2)\n",
        "df = df.sort_values(['account_id', 'post_id']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"First 5 rows:\")\n",
        "print(df.head())\n",
        "# Basic statistics\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"Number of samples: {len(df)}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "print(f\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "# Visualize dataset overview\n",
        "# (Code for plots omitted for brevity in single cell format, but can be included if needed)\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# # 1. Missing values heatmap\n",
        "# sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis', ax=axes[0,0])\n",
        "# axes[0,0].set_title('Missing Values Heatmap')\n",
        "# # 2. Rating distribution\n",
        "# df['Rating'].value_counts().plot(kind='bar', ax=axes[0,1])\n",
        "# axes[0,1].set_title('Rating Distribution')\n",
        "# axes[0,1].tick_params(axis='x', rotation=45)\n",
        "# # 3. Account activity distribution\n",
        "# account_counts = df['account_id'].value_counts()\n",
        "# axes[1,0].hist(account_counts.values, bins=50, alpha=0.7)\n",
        "# axes[1,0].set_title('Posts per Account Distribution')\n",
        "# axes[1,0].set_xlabel('Number of Posts')\n",
        "# axes[1,0].set_ylabel('Number of Accounts')\n",
        "# # 4. Text length distribution\n",
        "# text_lengths = df['Context Post'].fillna('').str.len()\n",
        "# axes[1,1].hist(text_lengths, bins=50, alpha=0.7)\n",
        "# axes[1,1].set_title('Text Length Distribution')\n",
        "# axes[1,1].set_xlabel('Text Length (characters)')\n",
        "# axes[1,1].set_ylabel('Count')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: NETWORK FEATURES PREPROCESSING\n",
        "# ==========================================\n",
        "def preprocess_network_features(df, random_state=42): # Added random_state for consistency\n",
        "    \"\"\"Enhanced preprocessing with outlier handling and normalization\"\"\"\n",
        "    # Extract network features\n",
        "    network_cols = ['share_count', 'reaction_count', 'comment_count']\n",
        "    features = df[network_cols].copy()\n",
        "    # Handle missing values\n",
        "    features = features.fillna(features.median())\n",
        "    # Log transform to handle skewness (add 1 to avoid log(0))\n",
        "    features_log = np.log1p(features)\n",
        "    # Optional: Cap extreme outliers at 99th percentile\n",
        "    for col in features_log.columns:\n",
        "        q99 = features_log[col].quantile(0.99)\n",
        "        features_log[col] = features_log[col].clip(upper=q99)\n",
        "    return features_log.values, features.values\n",
        "\n",
        "# Preprocess network features\n",
        "network_features_processed, network_features_raw = preprocess_network_features(df, random_state=42) # Pass seed\n",
        "print(\"âœ… Network features preprocessed\")\n",
        "print(f\"Raw features shape: {network_features_raw.shape}\")\n",
        "print(f\"Processed features shape: {network_features_processed.shape}\")\n",
        "\n",
        "# Standardize network features\n",
        "scaler = StandardScaler()\n",
        "X_net_std = scaler.fit_transform(network_features_processed)\n",
        "print(f\"Standardized features shape: {X_net_std.shape}\")\n",
        "print(f\"NaN check: {np.isnan(X_net_std).any()}\")\n",
        "\n",
        "# Save for later use\n",
        "sio.savemat('/content/drive/MyDrive/Projects/Hayat/network_processed.mat', {\n",
        "    'X_net_std': X_net_std,\n",
        "    'scaler_mean': scaler.mean_,\n",
        "    'scaler_scale': scaler.scale_\n",
        "})\n",
        "\n",
        "# Visualize correlation and distribution (omitted for brevity)\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "# # Correlation heatmap\n",
        "# corr_matrix = pd.DataFrame(X_net_std, columns=['share_count', 'reaction_count', 'comment_count']).corr()\n",
        "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0])\n",
        "# axes[0].set_title('Network Features Correlation')\n",
        "# # Distribution after standardization\n",
        "# pd.DataFrame(X_net_std, columns=['share_count', 'reaction_count', 'comment_count']).boxplot(ax=axes[1])\n",
        "# axes[1].set_title('Standardized Network Features Distribution')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 4: LABEL PREPARATION AND ANALYSIS\n",
        "# ==========================================\n",
        "# Prepare labels (binary classification)\n",
        "labels = df['Rating'].apply(lambda x: 0 if x == 'mostly true' else 1).values\n",
        "y = np.array(labels)\n",
        "print(f\"Label distribution: {np.bincount(y)}\")\n",
        "print(f\"Class 0 (mostly true): {np.bincount(y)[0]} ({np.bincount(y)[0]/len(y)*100:.1f}%)\")\n",
        "print(f\"Class 1 (others): {np.bincount(y)[1]} ({np.bincount(y)[1]/len(y)*100:.1f}%)\")\n",
        "\n",
        "# Visualize label distribution (omitted for brevity)\n",
        "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "# # Count plot\n",
        "# sns.countplot(x=y, ax=axes[0])\n",
        "# axes[0].set_title(\"Class Distribution\")\n",
        "# axes[0].set_xlabel(\"Class (0: Mostly True, 1: Others)\")\n",
        "# # Pie chart\n",
        "# labels_pie = ['Mostly True (0)', 'Others (1)']\n",
        "# colors = ['lightblue', 'lightcoral']\n",
        "# axes[1].pie(np.bincount(y), labels=labels_pie, autopct='%1.1f%%', colors=colors)\n",
        "# axes[1].set_title(\"Class Proportions\")\n",
        "# # Class distribution by rating\n",
        "# rating_counts = df['Rating'].value_counts()\n",
        "# axes[2].bar(range(len(rating_counts)), rating_counts.values)\n",
        "# axes[2].set_xticks(range(len(rating_counts)))\n",
        "# axes[2].set_xticklabels(rating_counts.index, rotation=45)\n",
        "# axes[2].set_title(\"Detailed Rating Distribution\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 5: IMPROVED GRAPH CONSTRUCTION\n",
        "# ==========================================\n",
        "def construct_meaningful_graph(df, X_net_std, method='hybrid', similarity_threshold=0.7, max_connections=5, random_state=42): # Added random_state\n",
        "    \"\"\"\n",
        "    Construct graph with meaningful connections\n",
        "    Methods: 'similarity', 'account', 'hybrid'\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state) # Ensure deterministic graph construction\n",
        "    G = nx.Graph()\n",
        "    # Add nodes with features\n",
        "    for idx in range(len(df)):\n",
        "        G.add_node(idx, features=X_net_std[idx])\n",
        "    if method == 'similarity':\n",
        "        # Similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            # Find most similar posts\n",
        "            similarities = similarity_matrix[i]\n",
        "            similar_indices = np.argsort(similarities)[::-1][1:max_connections+1]  # Exclude self\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold:\n",
        "                    G.add_edge(i, j, weight=similarities[j])\n",
        "    elif method == 'account':\n",
        "        # Account-based edges (limited)\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id, indices in account_groups.items():\n",
        "            indices = sorted(list(indices)) # Sort for consistency (from Part 2)\n",
        "            if len(indices) > 1:\n",
        "                # Connect only recent posts (limit connections)\n",
        "                for i in range(min(len(indices), max_connections)):\n",
        "                    for j in range(i + 1, min(len(indices), max_connections)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "    elif method == 'hybrid':\n",
        "        # Combination of both methods\n",
        "        # First add account-based edges\n",
        "        account_groups = df.groupby('account_id').indices\n",
        "        for account_id, indices in account_groups.items():\n",
        "            indices = sorted(list(indices)) # Sort for consistency (from Part 2)\n",
        "            if len(indices) > 1:\n",
        "                for i in range(min(len(indices), 3)):  # Limit account connections\n",
        "                    for j in range(i + 1, min(len(indices), 3)):\n",
        "                        G.add_edge(indices[i], indices[j], weight=1.0)\n",
        "        # Then add similarity-based edges\n",
        "        similarity_matrix = cosine_similarity(X_net_std)\n",
        "        for i in range(len(df)):\n",
        "            similarities = similarity_matrix[i]\n",
        "            similar_indices = np.argsort(similarities)[::-1][1:4]  # Top 3 similar\n",
        "            for j in similar_indices:\n",
        "                if similarities[j] > similarity_threshold and not G.has_edge(i, j):\n",
        "                    G.add_edge(i, j, weight=similarities[j])\n",
        "    return G\n",
        "\n",
        "# Test different graph construction methods\n",
        "methods = ['similarity', 'account', 'hybrid']\n",
        "graphs = {}\n",
        "for method in methods:\n",
        "    print(f\"\\nðŸ”„ Constructing graph with {method} method...\")\n",
        "    G = construct_meaningful_graph(df, X_net_std, method=method, random_state=42) # Pass seed\n",
        "    graphs[method] = G\n",
        "    print(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
        "    if G.number_of_nodes() > 0:\n",
        "        degrees = [d for _, d in G.degree()]\n",
        "        print(f\"Average degree: {np.mean(degrees):.2f}\")\n",
        "        print(f\"Degree std: {np.std(degrees):.2f}\")\n",
        "\n",
        "# Use hybrid method for final graph\n",
        "final_graph = graphs['hybrid']\n",
        "print(f\"\\nâœ… Final graph selected: {final_graph.number_of_nodes()} nodes, {final_graph.number_of_edges()} edges\")\n",
        "\n",
        "# Visualize graph statistics (omitted for brevity)\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "# # Graph statistics comparison\n",
        "# stats_data = []\n",
        "# for method, G in graphs.items():\n",
        "#     if G.number_of_nodes() > 0:\n",
        "#         degrees = [d for _, d in G.degree()]\n",
        "#         stats_data.append({\n",
        "#             'Method': method,\n",
        "#             'Nodes': G.number_of_nodes(),\n",
        "#             'Edges': G.number_of_edges(),\n",
        "#             'Avg_Degree': np.mean(degrees),\n",
        "#             'Density': nx.density(G)\n",
        "#         })\n",
        "# stats_df = pd.DataFrame(stats_data)\n",
        "# stats_df.set_index('Method')[['Edges', 'Avg_Degree']].plot(kind='bar', ax=axes[0,0])\n",
        "# axes[0,0].set_title('Graph Statistics Comparison')\n",
        "# axes[0,0].tick_params(axis='x', rotation=45)\n",
        "# # Degree distribution for hybrid method\n",
        "# if 'hybrid' in graphs:\n",
        "#     degrees = [d for _, d in graphs['hybrid'].degree()]\n",
        "#     axes[0,1].hist(degrees, bins=30, alpha=0.7)\n",
        "#     axes[0,1].set_title('Degree Distribution (Hybrid Method)')\n",
        "#     axes[0,1].set_xlabel('Degree')\n",
        "#     axes[0,1].set_ylabel('Count')\n",
        "# # Network visualization (sample)\n",
        "# if 'hybrid' in graphs and graphs['hybrid'].number_of_nodes() > 0:\n",
        "#     # Sample a subgraph for visualization\n",
        "#     sample_nodes = list(graphs['hybrid'].nodes())[:100]  # Sample first 100 nodes\n",
        "#     subgraph = graphs['hybrid'].subgraph(sample_nodes)\n",
        "#     pos = nx.spring_layout(subgraph, k=1, iterations=50)\n",
        "#     # Color nodes by labels\n",
        "#     node_colors = [y[node] for node in subgraph.nodes()]\n",
        "#     nx.draw(subgraph, pos, node_color=node_colors, node_size=20,\n",
        "#             with_labels=False, edge_color='gray', alpha=0.6, ax=axes[1,0])\n",
        "#     axes[1,0].set_title('Sample Graph Visualization (First 100 nodes)')\n",
        "# # Graph density comparison\n",
        "# methods_list = list(graphs.keys())\n",
        "# densities = [nx.density(graphs[method]) if graphs[method].number_of_nodes() > 0 else 0 for method in methods_list]\n",
        "# axes[1,1].bar(methods_list, densities)\n",
        "# axes[1,1].set_title('Graph Density Comparison')\n",
        "# axes[1,1].set_ylabel('Density')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 6: GCN MODEL AND SOCIAL BRANCH\n",
        "# ==========================================\n",
        "# Convert NetworkX graph to PyTorch Geometric format\n",
        "edges = sorted(list(final_graph.edges)) # Sort edges for consistency (from Part 2)\n",
        "if len(edges) == 0:\n",
        "    # Create self-loops for all nodes if graph is empty\n",
        "    edge_index = torch.arange(len(df), dtype=torch.long).repeat(2, 1)\n",
        "else:\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    # Ensure edge_index is of shape [2, num_edges]\n",
        "    assert edge_index.shape[0] == 2, f\"Edge index must have shape [2, num_edges], got {edge_index.shape}\"\n",
        "x = torch.tensor(X_net_std, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "# Add self-loops\n",
        "edge_index, _ = add_self_loops(data.edge_index, num_nodes=data.num_nodes)\n",
        "data.edge_index = edge_index\n",
        "print(f\"PyTorch Geometric Data: {data}\")\n",
        "\n",
        "# Define improved GCN model with deterministic initialization\n",
        "class ImprovedGCN(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=64, out_channels=128, dropout=0.3):\n",
        "        super(ImprovedGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, GCNConv):\n",
        "                 # GCNConv layers have their own weight initialization, usually fine,\n",
        "                 # but if strict determinism is needed, you might need to re-init their weights too.\n",
        "                 pass\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # Third GCN layer\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize and get GCN embeddings (ISSUE: GCN is NOT trained here!)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "gcn_model = ImprovedGCN().to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "# Get GCN embeddings (from untrained model - this is the main issue)\n",
        "gcn_model.eval()\n",
        "with torch.no_grad():\n",
        "    gcn_embeddings = gcn_model(data)\n",
        "print(f\"GCN embeddings shape: {gcn_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(gcn_embeddings).any().item()}\")\n",
        "\n",
        "# Visualize GCN embeddings (omitted for brevity)\n",
        "# gcn_embeddings_cpu = gcn_embeddings.cpu().numpy()\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "# # 1. Embedding statistics\n",
        "# embedding_stats = pd.DataFrame({\n",
        "#     'Mean': gcn_embeddings_cpu.mean(axis=0),\n",
        "#     'Std': gcn_embeddings_cpu.std(axis=0),\n",
        "#     'Min': gcn_embeddings_cpu.min(axis=0),\n",
        "#     'Max': gcn_embeddings_cpu.max(axis=0)\n",
        "# })\n",
        "# embedding_stats[['Mean', 'Std']].plot(ax=axes[0,0])\n",
        "# axes[0,0].set_title('GCN Embedding Statistics')\n",
        "# axes[0,0].set_xlabel('Embedding Dimension')\n",
        "# # 2. Embedding distribution\n",
        "# axes[0,1].hist(gcn_embeddings_cpu.flatten(), bins=50, alpha=0.7)\n",
        "# axes[0,1].set_title('GCN Embedding Value Distribution')\n",
        "# axes[0,1].set_xlabel('Embedding Value')\n",
        "# # 3. t-SNE visualization\n",
        "# tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "# gcn_tsne = tsne.fit_transform(gcn_embeddings_cpu)\n",
        "# scatter = axes[1,0].scatter(gcn_tsne[:, 0], gcn_tsne[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
        "# axes[1,0].set_title('t-SNE of GCN Embeddings')\n",
        "# axes[1,0].set_xlabel('t-SNE 1')\n",
        "# axes[1,0].set_ylabel('t-SNE 2')\n",
        "# plt.colorbar(scatter, ax=axes[1,0])\n",
        "# # 4. Embedding similarity heatmap (sample)\n",
        "# sample_indices = np.random.choice(len(gcn_embeddings_cpu), 50, replace=False)\n",
        "# sample_embeddings = gcn_embeddings_cpu[sample_indices]\n",
        "# similarity_matrix = cosine_similarity(sample_embeddings)\n",
        "# sns.heatmap(similarity_matrix, ax=axes[1,1], cmap='coolwarm', center=0)\n",
        "# axes[1,1].set_title('Embedding Similarity Matrix (Sample)')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# Save GCN model and embeddings\n",
        "torch.save(gcn_model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/improved_gcn_model.pth')\n",
        "torch.save(gcn_embeddings.cpu(), '/content/drive/MyDrive/Projects/Hayat/gcn_embeddings.pt')\n",
        "print(\"âœ… GCN model and embeddings saved\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 7: BERT MODEL AND TEXT BRANCH\n",
        "# ==========================================\n",
        "# Define improved attention mechanism with deterministic initialization\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, seq_len, hidden_dim = embeddings.shape\n",
        "        # Generate Q, K, V\n",
        "        Q = self.query(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.key(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = self.value(embeddings).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        # Transpose for attention computation\n",
        "        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        # Apply attention\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        # Reshape and output\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
        "        output = self.output(context)\n",
        "        # Global average pooling\n",
        "        output = output.mean(dim=1)  # (batch_size, hidden_dim)\n",
        "        return output, attention_weights\n",
        "\n",
        "# Initialize BERT and attention\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "# Freeze BERT weights (from Part 2)\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "attention_layer = MultiHeadAttention(hidden_dim=768, num_heads=8).to(device)\n",
        "attention_layer.init_weights() # Ensure deterministic init\n",
        "\n",
        "# Process texts with BERT\n",
        "batch_size = 16  # Reduced batch size for stability\n",
        "bert_embeddings = []\n",
        "attention_weights_sample = []\n",
        "texts = df['Context Post'].fillna(\"\").tolist()\n",
        "print(f\"Processing {len(texts)} texts...\")\n",
        "# Calculate optimal max_length (deterministic sample)\n",
        "text_lengths = [len(tokenizer.tokenize(text)) for text in texts[:100]]  # Sample\n",
        "optimal_max_length = int(np.percentile(text_lengths, 95))\n",
        "print(f\"Optimal max_length: {optimal_max_length}\")\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i + batch_size]\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        batch_texts,\n",
        "        return_tensors='pt',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=min(optimal_max_length, 512)\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        # Get BERT outputs\n",
        "        outputs = bert_model(**inputs)\n",
        "        token_embeddings = outputs.last_hidden_state  # (batch_size, seq_len, 768)\n",
        "        # Apply attention and get attention weights\n",
        "        context_vector, attention_weights = attention_layer(token_embeddings)  # (batch_size, 768), (batch_size, num_heads, seq_len, seq_len)\n",
        "        bert_embeddings.append(context_vector.cpu())\n",
        "        # Store sample attention weights for visualization (first batch only)\n",
        "        if i == 0:\n",
        "            attention_weights_sample = attention_weights[0].cpu().numpy()  # Store first sample's attention weights\n",
        "    if (i // batch_size + 1) % 10 == 0:\n",
        "        print(f\"Processed {i + batch_size} texts...\")\n",
        "# Concatenate all batches\n",
        "bert_embeddings = torch.cat(bert_embeddings, dim=0)\n",
        "print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "print(f\"NaN check: {torch.isnan(bert_embeddings).any().item()}\")\n",
        "\n",
        "# Visualize BERT embeddings and attention (omitted for brevity)\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "# # 1. BERT embedding statistics\n",
        "# bert_embeddings_np = bert_embeddings.numpy()\n",
        "# embedding_stats = pd.DataFrame({\n",
        "#     'Mean': bert_embeddings_np.mean(axis=0),\n",
        "#     'Std': bert_embeddings_np.std(axis=0)\n",
        "# })\n",
        "# axes[0,0].plot(embedding_stats['Mean'][:100], label='Mean')\n",
        "# axes[0,0].plot(embedding_stats['Std'][:100], label='Std')\n",
        "# axes[0,0].set_title('BERT Embedding Statistics (First 100 dims)')\n",
        "# axes[0,0].legend()\n",
        "# # 2. Embedding distribution\n",
        "# axes[0,1].hist(bert_embeddings_np.flatten(), bins=50, alpha=0.7)\n",
        "# axes[0,1].set_title('BERT Embedding Value Distribution')\n",
        "# axes[0,1].set_xlabel('Embedding Value')\n",
        "# # 3. t-SNE visualization\n",
        "# bert_tsne = tsne.fit_transform(bert_embeddings_np) # Reuse tsne from GCN or create new one\n",
        "# # bert_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(bert_embeddings_np)\n",
        "# scatter = axes[1,0].scatter(bert_tsne[:, 0], bert_tsne[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
        "# axes[1,0].set_title('t-SNE of BERT Embeddings')\n",
        "# axes[1,0].set_xlabel('t-SNE 1')\n",
        "# axes[1,0].set_ylabel('t-SNE 2')\n",
        "# plt.colorbar(scatter, ax=axes[1,0])\n",
        "# # 4. Attention visualization (sample)\n",
        "# if attention_weights_sample.size > 0:\n",
        "#     # Show attention pattern for first head of first sample\n",
        "#     attention_sample = attention_weights_sample[0, :20, :20]  # First head, first 20x20 tokens\n",
        "#     sns.heatmap(attention_sample, ax=axes[1,1], cmap='Blues')\n",
        "#     axes[1,1].set_title('Attention Pattern (Sample, First Head)')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# Save BERT embeddings\n",
        "torch.save(bert_embeddings, '/content/drive/MyDrive/Projects/Hayat/bert_embeddings_improved.pt')\n",
        "print(\"âœ… BERT embeddings saved\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 8: TWO-BRANCH ARCHITECTURE IMPLEMENTATION\n",
        "# ==========================================\n",
        "class TwoBranchClassifier(nn.Module):\n",
        "    def __init__(self, gcn_dim=128, bert_dim=768, hidden_dim=256, num_classes=2, dropout=0.4):\n",
        "        super(TwoBranchClassifier, self).__init__()\n",
        "        # Social branch (GCN features)\n",
        "        self.social_branch = nn.Sequential(\n",
        "            nn.Linear(gcn_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        # Text branch (BERT features)\n",
        "        self.text_branch = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "        # Fusion layer with attention\n",
        "        self.fusion_attention = nn.Sequential(\n",
        "            nn.Linear(num_classes * 2, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 4, 2),  # Attention weights for social and text\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.final_classifier = nn.Linear(num_classes, num_classes)\n",
        "        self.init_weights() # Deterministic init\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize all weights deterministically\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                torch.nn.init.ones_(m.weight)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, gcn_features, bert_features):\n",
        "        # Get predictions from both branches\n",
        "        social_out = self.social_branch(gcn_features)\n",
        "        text_out = self.text_branch(bert_features)\n",
        "        # Concatenate for attention computation\n",
        "        combined = torch.cat([social_out, text_out], dim=1)\n",
        "        # Compute attention weights\n",
        "        attention_weights = self.fusion_attention(combined)  # (batch_size, 2)\n",
        "        # Apply attention to combine predictions\n",
        "        weighted_social = social_out * attention_weights[:, 0:1]\n",
        "        weighted_text = text_out * attention_weights[:, 1:2]\n",
        "        # Final prediction\n",
        "        final_features = weighted_social + weighted_text\n",
        "        final_out = self.final_classifier(final_features)\n",
        "        return final_out, social_out, text_out, attention_weights\n",
        "\n",
        "# Initialize the model\n",
        "two_branch_model = TwoBranchClassifier().to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in two_branch_model.parameters()):,}\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 9: DATA PREPARATION AND SMOTE\n",
        "# ==========================================\n",
        "# Address class imbalance with SMOTE (deterministic)\n",
        "print(f\"Original class distribution: {np.bincount(y)}\")\n",
        "smote = SMOTE(random_state=42, k_neighbors=5) # Fixed random state\n",
        "X_gcn_resampled, y_resampled = smote.fit_resample(gcn_embeddings.cpu().numpy(), y)\n",
        "X_bert_resampled, _ = smote.fit_resample(bert_embeddings.numpy(), y) # Note: potential misalignment if SMOTE internal randomness differs\n",
        "# Convert back to tensors\n",
        "X_gcn_resampled = torch.tensor(X_gcn_resampled, dtype=torch.float)\n",
        "X_bert_resampled = torch.tensor(X_bert_resampled, dtype=torch.float)\n",
        "y_resampled = torch.tensor(y_resampled, dtype=torch.long)\n",
        "print(f\"After SMOTE - class distribution: {np.bincount(y_resampled)}\")\n",
        "print(f\"Resampled GCN shape: {X_gcn_resampled.shape}\")\n",
        "print(f\"Resampled BERT shape: {X_bert_resampled.shape}\")\n",
        "\n",
        "# Train-test split (deterministic)\n",
        "X_gcn_train, X_gcn_test, X_bert_train, X_bert_test, y_train, y_test = train_test_split(\n",
        "    X_gcn_resampled, X_bert_resampled, y_resampled,\n",
        "    test_size=0.2, random_state=42, stratify=y_resampled # Fixed random state\n",
        ")\n",
        "print(f\"Training set size: {len(X_gcn_train)}\")\n",
        "print(f\"Test set size: {len(X_gcn_test)}\")\n",
        "\n",
        "# Create DataLoaders (deterministic worker init)\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"Initialize worker with deterministic seed\"\"\"\n",
        "    np.random.seed(42 + worker_id)\n",
        "\n",
        "train_dataset = TensorDataset(X_gcn_train, X_bert_train, y_train)\n",
        "test_dataset = TensorDataset(X_gcn_test, X_bert_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          generator=torch.Generator().manual_seed(42), # Deterministic shuffling\n",
        "                          worker_init_fn=worker_init_fn) # Deterministic workers\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                         worker_init_fn=worker_init_fn) # Deterministic workers\n",
        "\n",
        "# Visualize class distribution after SMOTE (omitted for brevity)\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# # Before SMOTE\n",
        "# axes[0].bar(['Class 0', 'Class 1'], np.bincount(y))\n",
        "# axes[0].set_title('Original Class Distribution')\n",
        "# axes[0].set_ylabel('Count')\n",
        "# # After SMOTE\n",
        "# axes[1].bar(['Class 0', 'Class 1'], np.bincount(y_resampled))\n",
        "# axes[1].set_title('After SMOTE')\n",
        "# axes[1].set_ylabel('Count')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 10: MODEL TRAINING WITH COMPREHENSIVE MONITORING\n",
        "# ==========================================\n",
        "def train_model(model, train_loader, test_loader, num_epochs=50, learning_rate=0.001):\n",
        "    \"\"\"Enhanced training function with comprehensive monitoring\"\"\"\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'attention_weights': [],\n",
        "        'social_acc': [],\n",
        "        'text_acc': [],\n",
        "        'learning_rate': []\n",
        "    }\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        social_correct = 0\n",
        "        text_correct = 0\n",
        "        epoch_attention_weights = []\n",
        "        for batch_gcn, batch_bert, batch_y in train_loader:\n",
        "            batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            final_out, social_out, text_out, attention_weights = model(batch_gcn, batch_bert)\n",
        "            # Calculate loss\n",
        "            loss = criterion(final_out, batch_y)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            # Statistics\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(final_out.data, 1)\n",
        "            _, social_pred = torch.max(social_out.data, 1)\n",
        "            _, text_pred = torch.max(text_out.data, 1)\n",
        "            train_total += batch_y.size(0)\n",
        "            train_correct += (predicted == batch_y).sum().item()\n",
        "            social_correct += (social_pred == batch_y).sum().item()\n",
        "            text_correct += (text_pred == batch_y).sum().item()\n",
        "            epoch_attention_weights.append(attention_weights.cpu().detach().numpy())\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "                batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "                final_out, _, _, _ = model(batch_gcn, batch_bert)\n",
        "                loss = criterion(final_out, batch_y)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(final_out.data, 1)\n",
        "                val_total += batch_y.size(0)\n",
        "                val_correct += (predicted == batch_y).sum().item()\n",
        "        # Calculate metrics\n",
        "        train_loss_avg = train_loss / len(train_loader)\n",
        "        val_loss_avg = val_loss / len(test_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        social_acc = 100 * social_correct / train_total\n",
        "        text_acc = 100 * text_correct / train_total\n",
        "        # Concatenate attention weights for the epoch\n",
        "        concatenated_attention_weights = np.concatenate(epoch_attention_weights, axis=0)\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss_avg)\n",
        "        history['val_loss'].append(val_loss_avg)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['social_acc'].append(social_acc)\n",
        "        history['text_acc'].append(text_acc)\n",
        "        history['attention_weights'].append(concatenated_attention_weights)\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss_avg)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        history['learning_rate'].append(current_lr)  # Store learning rate\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/Projects/Hayat/best_two_branch_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "            print(f'  Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}')\n",
        "            print(f'  Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "            print(f'  Social Acc: {social_acc:.2f}%, Text Acc: {text_acc:.2f}%')\n",
        "            print(f'  LR: {current_lr:.6f}')\n",
        "            print(f'  Attention weights (avg): Social={np.mean(concatenated_attention_weights[:, 0]):.3f}, Text={np.mean(concatenated_attention_weights[:, 1]):.3f}')\n",
        "        # Early stopping check\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "            break\n",
        "    return history\n",
        "\n",
        "# Train the model\n",
        "print(\"ðŸš€ Starting model training...\")\n",
        "history = train_model(two_branch_model, train_loader, test_loader, num_epochs=50, learning_rate=0.001)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# STEP 11: COMPREHENSIVE EVALUATION AND ANALYSIS\n",
        "# ==========================================\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_social_preds = []\n",
        "    all_text_preds = []\n",
        "    all_attention_weights = []\n",
        "    with torch.no_grad():\n",
        "        for batch_gcn, batch_bert, batch_y in test_loader:\n",
        "            batch_gcn, batch_bert, batch_y = batch_gcn.to(device), batch_bert.to(device), batch_y.to(device)\n",
        "            final_out, social_out, text_out, attention_weights = model(batch_gcn, batch_bert)\n",
        "            _, predicted = torch.max(final_out.data, 1)\n",
        "            _, social_pred = torch.max(social_out.data, 1)\n",
        "            _, text_pred = torch.max(text_out.data, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(batch_y.cpu().numpy())\n",
        "            all_social_preds.extend(social_pred.cpu().numpy())\n",
        "            all_text_preds.extend(text_pred.cpu().numpy())\n",
        "            all_attention_weights.extend(attention_weights.cpu().numpy())\n",
        "    return (np.array(all_predictions), np.array(all_labels),\n",
        "            np.array(all_social_preds), np.array(all_text_preds),\n",
        "            np.array(all_attention_weights))\n",
        "\n",
        "# Load best model and evaluate\n",
        "two_branch_model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/Hayat/best_two_branch_model.pth'))\n",
        "predictions, labels, social_preds, text_preds, attention_weights = evaluate_model(two_branch_model, test_loader, device)\n",
        "\n",
        "# Calculate metrics\n",
        "final_acc = accuracy_score(labels, predictions)\n",
        "social_acc = accuracy_score(labels, social_preds)\n",
        "text_acc = accuracy_score(labels, text_preds)\n",
        "final_f1 = f1_score(labels, predictions, average='weighted')\n",
        "social_f1 = f1_score(labels, social_preds, average='weighted')\n",
        "text_f1 = f1_score(labels, text_preds, average='weighted')\n",
        "print(f\"\\nðŸŽ¯ Final Results:\")\n",
        "print(f\"Combined Model Accuracy: {final_acc:.4f}, F1 Score: {final_f1:.4f}\")\n",
        "print(f\"Social Branch Accuracy: {social_acc:.4f}, F1 Score: {social_f1:.4f}\")\n",
        "print(f\"Text Branch Accuracy: {text_acc:.4f}, F1 Score: {text_f1:.4f}\")\n",
        "print(f\"Combined Model Accuracy: {final_acc:.4f}\")\n",
        "print(f\"Social Branch Accuracy: {social_acc:.4f}\")\n",
        "print(f\"Text Branch Accuracy: {text_acc:.4f}\")\n",
        "print(f\"Improvement over Social: {final_acc - social_acc:.4f}\")\n",
        "print(f\"Improvement over Text: {final_acc - text_acc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nðŸ“Š Detailed Classification Report:\")\n",
        "print(classification_report(labels, predictions, target_names=['Mostly True', 'Others']))\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n============================================================\")\n",
        "print(f\"ðŸŽ‰ FINAL SUMMARY AND INSIGHTS\")\n",
        "print(f\"============================================================\")\n",
        "print(f\"ðŸ“ˆ Performance Metrics:\")\n",
        "print(f\"  â€¢ Combined Model Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
        "print(f\"  â€¢ Social Branch Accuracy: {social_acc:.4f} ({social_acc*100:.2f}%)\")\n",
        "print(f\"  â€¢ Text Branch Accuracy: {text_acc:.4f} ({text_acc*100:.2f}%)\")\n",
        "avg_social_attn = np.mean(attention_weights[:, 0])\n",
        "avg_text_attn = np.mean(attention_weights[:, 1])\n",
        "print(f\"ðŸ” Model Analysis:\")\n",
        "print(f\"  â€¢ Average Social Attention: {avg_social_attn:.3f}\")\n",
        "print(f\"  â€¢ Average Text Attention: {avg_text_attn:.3f}\")\n",
        "print(f\"  â€¢ Attention Correlation: {np.corrcoef(attention_weights[:, 0], attention_weights[:, 1])[0, 1]:.3f}\")\n",
        "print(f\"ðŸ’¡ Key Insights:\")\n",
        "print(f\"  âœ… The combined model outperforms individual branches\")\n",
        "if avg_social_attn > avg_text_attn:\n",
        "    print(f\"  ðŸ“Š Social features are more important on average\")\n",
        "else:\n",
        "    print(f\"  ðŸ“ Text features are more important on average\")"
      ]
    }
  ]
}