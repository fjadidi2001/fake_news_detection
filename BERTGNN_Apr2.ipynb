{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY7imCcaB9duDsVLYnyvQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/fake_news_detection/blob/main/BERTGNN_Apr2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Definition\n",
        "> combining Graph Neural Networks (GNNs) for social network analysis and BERT for text processing, with the facebook-fact-check.csv dataset and the embedding/modeling scripts. This dataset includes social network features (share_count, reaction_count, comment_count) and text (Context Post), making it a great fit for this hybrid approach. **The goal is to classify posts (e.g., binary classification: \"mostly true\" vs. others) by integrating graph-based social interactions and text semantics.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Gt_Y6jxn3chL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Overview\n",
        "\n"
      ],
      "metadata": {
        "id": "MQyRkyk44ocK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Objective: Classify Facebook posts’ veracity using social network structure (via GNN) and text content (via BERT).\n",
        "\n",
        "- Dataset: facebook-fact-check.csv (2282 rows, with account_id, post_id, network features, and Context Post).\n",
        "\n",
        "- Output: Binary classification (0: \"mostly true\", 1: others).\n",
        "\n"
      ],
      "metadata": {
        "id": "NaxtPdSp4qOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step Development Process\n",
        "\n"
      ],
      "metadata": {
        "id": "JcL1Vpy447rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Preprocessing and Exploration\n",
        "\n",
        "> Goal: Prepare the dataset for GNN and BERT, ensuring compatibility with both models.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Load and Inspect Data: Use the existing embedding script’s loading logic.\n",
        "\n",
        "2. Labels: Map Rating to binary labels (0 vs. 1).\n",
        "\n",
        "3. Network Features: Extract share_count, reaction_count, comment_count and standardize them.\n",
        "\n",
        "4. Graph Construction: Create a graph where nodes are posts (post_id), edges are based on shared account_id or interactions (e.g., co-occurring in the dataset), and node features are network metrics.\n",
        "\n",
        "5. Text Data: Keep Context Post raw for BERT input (no tokenization yet; BERT handles it internally).\n",
        "\n"
      ],
      "metadata": {
        "id": "KdmY0ymL5KhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5xAj_Do2OG9"
      },
      "outputs": [],
      "source": []
    }
  ]
}